In the next two chapters, we will start looking 
at the case where the MDP models are large. 
In This chapter, we will look at approximating the
value function, and in the next chapter, we will consider
directly learning and optimizing a policy.

When we talk about a large MDP, it can be for a few different
reasons. The most common is having a large state space. For example,
Backgammon has $10^{20}$ states, Go has $10^{170}$, and robot control
has continuous state space. Another dimension is the action space,
which can even be continuous in many applications (say, robots).
Finally, we might have complex dynamics which are hard to describe
succinctly. The main challenge is that we need our algorithm to
scale up to these enormous spaces.

 Previously, we had a look-up table for the value
function $\Value^\policy(\state)$ or $Q^\policy(\state,\action)$,
which we updated every time we encountered the state $\state$.
%
Here, we will consider function approximation for large MDPs, mainly
large state spaces.
%
In a large state space, this will be infeasible. Not only that, we do
not have the memory, but even more importantly, we are unlikely to
observe states re-occurring. We will need to make (implicit)
assumptions about the MDP, which can be viewed similarly to our
assumption in learning a classifier in supervised learning.

Specifically, we will have the value functions parameterized by {\em
weights} $\weight$, namely, $\widehat{\Value}(\state;\weight)\approx
\Value^\policy(\state)$ and
$\widehat{Q}(\state,\action;\weight)\approx
Q^\policy(\state,\action)$. We would like to guarantee
generalization from the observed states and trajectories to
unobserved states and trajectories. In the process, we will update
the weights $\weight$, using some learning procedure, most notably
using MC or TD learning.

When considering a value function, there are a few interpretations
of what exactly we mean. It can either be: (1) mapping from a state
$\state$ to its expected return, i.e.,
$\widehat{\Value}^\policy(\state;\weight)$. (2) mapping from state-action
pairs $(\state,\action)$ to their expected return, i.e.,
$\widehat{Q}^\policy(\state,\action;\weight)$. (3) mapping from
states $\state$ to the expected return of each action, i.e.,
$\langle\widehat{Q}^\policy(\state,\action_i;\weight):\action_i\in
\Actions\rangle$. All the interpretations are valid, and our
discussion will not distinguish between them. (Actually, for
$\langle\widehat{Q}^\policy(\state,\action_i;\weight):\action_i\in
\Actions\rangle$ we implicitly assume that the number of actions is
small.)

We now need to discuss how we will build the approximating function.
For this, we can turn to the rich literature on Machine Learning and
consider popular hypothesis classes. For example: (1) Linear
functions, (2) Neural networks, (3) Decision trees, (4) Nearest
neighbors, (5) Fourier or wavelet basis, etc. We will concentrate
here on linear functions and neural networks, mainly since gradient-based
methods naturally apply to them.

We will consider in this chapter (mainly) the discounted return with
a discount parameter $\gamma\in(0,1)$. The results extend very
naturally to the finite horizon and episodic settings.

% \textbf{YM: Maybe better to switch to finite horizon, and avoid the
% need for mixing time and steady state distribution?}

\section{Basic Approximation}

Before discussing the learning methods, we will make a
small detour. We will discuss the effect of having an error in the
value function we learn and its effect on the outcome. Assume we
have a value function $\Value$ such that $\|\Value-\Value^*\|_\infty\leq
\varepsilon$. Let $\policy$ be the greedy policy with respect to
$\Value$, namely,
\[
\policy(\state)=\arg\max_\action \left\{\reward(\state,\action)+\gamma
\E_{\state'\sim p(\cdot|\state,\action)}[\Value(\state')]\right\}
\]

\begin{theorem}
Let $\Value$ such that $\|V-\Value^*\|_\infty\leq \varepsilon$ and
$\policy$ be the greedy policy with respect to $\Value$. Then,
\[
\|\Value^\policy-\Value^*\|_\infty\leq\frac{2\gamma\varepsilon}{1-\gamma}
\]
\end{theorem}

\begin{proof}
Consider two operators $\operator^\policy$ and $\operator^*$ (see
Chapter \ref{ss:DP_op}). The first $\operator^\policy$ is
\[
(\operator^\policy v)(\state)=\reward(\state,\policy(\state))+\gamma
\E_{\state'\sim p(\cdot|\state,\policy(\state))}[v(\state')]
\]
which converges to $\Value^\policy$ (see Theorem~\ref{thm:DP_op}).
The second $T^*$ is
\[
(\operator^* v)(\state)=\max_\action\left\{\reward(\state,\action)+\gamma
\E_{\state'\sim p(\cdot|\state,\action)}[v(\state')] \right\}
\]
which converges to $\Value^*$ (see Theorem~\ref{thm:DP_op}).
In addition, recall that we have shown that both
$\operator^\policy$ and $\operator^*$ are $\gamma$-contracting (see
Theorem~\ref{thm:DP_op}).\\
Since $\policy$ is greedy with respect to $\Value$ we have
$\operator^\policy \Value=\operator^* \Value$ (but this does not hold for
other value functions $\Value'$).\\
Then,
\begin{align*}
\|\Value^\policy-\Value^*\|_\infty &= \|T^\policy \Value^\policy - \Value^*\|_\infty\\
&\leq \|T^\policy \Value^\policy -T^\policy \Value\|_\infty +\|T^\policy \Value - \Value^*\|_\infty\\
&\leq \gamma\| \Value^\policy - \Value\|_\infty +\|T^* \Value - T^*\Value^*\|_\infty\\
&\leq \gamma\| \Value^\policy - \Value\|_\infty +\gamma\| V - \Value^*\|_\infty\\
&\leq \gamma(\| \Value^\policy - \Value^*\|_\infty+ \|\Value^*-\Value\|_\infty ) +\gamma\| \Value - \Value^*\|_\infty\;,
\end{align*}
where in the second inequality, we used the fact that since
$\policy$ is greedy with respect to $\Value$ then $T^\policy \Value=T^* \Value$.

Reorganizing the inequality and recalling that
$\|\Value^*-\Value\|_\infty\leq \varepsilon$, we have
\[
(1-\gamma)\|\Value^\policy-\Value^*\|_\infty \leq 2\varepsilon\gamma
\]
and the theorem follows.
\end{proof}

The above theorem states that if we have a small error in
$L_\infty$ norm, the effect of the errors on the expected return is
bounded. However, in most cases, we will not be able to guarantee an
approximation in norm $L_\infty$. This is because it is infeasible even
to compute the $L_\infty$ norm of two given value functions since
this requires considering {\em all} states. In the large state space
setting, such operations are infeasible.

\section{From RL to ML}

We would like to reduce our reinforcement learning problem to a
supervised learning problem. This will enable us to use any of the
many techniques of machine learning to address the problem. Let us
consider the basic ingredients of supervised learning. The most
important ingredient is having a labeled sample set,
which is sampled i.i.d.

Let us start by considering an idealized setting. Fix a policy
$\policy$ that we like to learn its value function. We like to
generate a sample
\[
\{(\state_1, \Value^\policy(\state_1)),\ldots,
(\state_m,\Value^\policy(\state_m))\}
\]
We first need to discuss how to sample the states $\state_i$ in an
i.i.d. way. We can generate a trajectory, but we need to be careful
since adjacent states are dependent! One solution is to
space the sampling from the trajectory using the mixing time of
$\policy$.\footnote{See Chapter \ref{chapter:MC} for definition.}
This will give us samples $\state_i$ which are (almost) from the
stationary distribution of $\policy$ and are (almost) independent.
%For the action, we clearly will use $\policy$, no problem there.

The hardest and most confusing ingredient is the labels
$\Value^\policy(\state_i)$. In machine learning, we assume someone gives us the labels to build a classifier.
% (or alternatively, in unsupervised learning, where there is no clear ground truth).
If someone is able to give us
$\Value^\policy(\state)$, it seems like assuming the problem away
since this is what we would like to learn.

We need to define three things:
\begin{enumerate}
    \item Sampling states and actions $\{(\state,\action)\}$, which we will do using $\policy$
    \item A loss function, which will give the tradeoffs between different approximation errors
    \item Labels, which we are not given explicitly
\end{enumerate}
For the state and actions, we will define a threshold $\tau$, which
will depend on the mixing time. We will run the policy $\policy$
for $\tau$ steps and then output the state $\state$ (and action
$\action$, if needed).

For the loss function, we will define a differential loss function
$J(\weight)$. This will enable us to compute the gradient of the
loss function $\nabla_\weight
J(\weight)=\langle\frac{\partial}{\partial \weight_1}J(\weight),
\ldots , \frac{\partial}{\partial \weight_d}J(\weight)\rangle$, and
update the weights in the negative direction of the gradient,
namely, $\Delta \weight=-\alpha\nabla_\weight J(\weight)$, where
$\alpha$ is the step size or the learning rate. In supervised
learning we are guaranteed that the process will converge to a local
minimum (or a saddle point).

Let us start by defining the loss function,
\[
J(\weight)=\frac{1}{2}\sum_s
\mu(\state)(\Value^\policy(\state)-\widehat{\Value}^\policy(\state;\weight))^2
\]
where $\mu(\state)$ is the steady state distribution of
$\policy$.\footnote{See Chapter \ref{chapter:MC} for definition.} Given the loss function $J(\weight)$, we can optimize it by Gradient Descent, i.e., compute the gradient and update the weights. The Stochastic
Gradient Descent (SGD) is simply sampling a single state $\state$
and using it to update, i.e.,
\[
\Delta \weight_\ttime = \alpha (\Value^\policy
(\state)-\widehat{\Value}^\policy(\state;\weight))\nabla
\widehat{\Value}^\policy (\state;\weight)
\]

We now need to build the sample, namely, how do we set the labels to
replace $\Value^\policy (\state)$. The basic idea is to find an
unbiased estimator $U_\ttime$ such that $E[U_\ttime
|\state_\ttime]=\Value^\policy(\state_\ttime)$. We will now present
a few different approaches to derive such unbiased estimators.

The simplest approach is to use Monte-Carlo (MC)
sampling.\footnote{See Chapter \ref{sec:MC}} Recall that in First
Visit Monte-Carlo we have $R_\ttime(\state)=\sum_{\tau=1}^T
\reward_\tau$, starting at the first visit of $\state$ in episode
$\ttime$. Clearly, we have
$\E[R_\ttime(\state)]=\Value^\policy(\state)$, since samples are
independent, so we can set $U_\ttime(\state)=R_\ttime(\state)$. The
update becomes,
\[
\Delta \weight_\ttime = \alpha
(R_\ttime(\state)-\widehat{\Value}^\policy(\state;\weight_\ttime))\nabla_{w}
\widehat{\Value}^\policy (\state;\weight)
\]

We can try to use the same idea for $TD(0)$.\footnote{See Chapter
\ref{sec:TD}.} The estimate of $TD(0)$ for $\state_\ttime$ is
$R_\ttime(\state_\ttime)=\reward_\ttime+\gamma
\widehat{\Value}^\policy(\state_{\ttime+1};\weight)$. The first problem
is that this is a biased estimator since
\[
\Value^\policy(\state_\ttime)=\E[\reward_\ttime+\gamma
\Value^\policy(\state_{\ttime+1})]\neq \E[\reward_\ttime+\gamma
\widehat{\Value}(\state_{\ttime+1};\weight)]\;.
\]
We have an additional problem with the gradient. Since
$\weight$ also influence the target $R_\ttime(\state_\ttime)$
through $\widehat{\Value}^\policy(\state_{\ttime+1};\weight)$, something
we do not have in MC (or generally in ML). For this reason
$\nabla_\weight \widehat{\Value}(\state_\ttime;\weight)$ is called a
semi-gradient. The update becomes,
\[
\Delta \weight (\state)= \alpha[\reward_\ttime +\gamma
\widehat{\Value}(\state_{\ttime+1};\weight)-\widehat{\Value}(\state_\ttime;\weight)]\nabla_\weight
\widehat{\Value}(\state_\ttime;\weight)
\]

Finally we get to $TD(\lambda)$. Similar to $TD(0)$ we can now
define the forward update to be,
\[
\Delta \weight =  \alpha[R^\lambda_\ttime
-\widehat{\Value}(\state_\ttime;\weight)]\nabla_\weight
\widehat{\Value}(\state_\ttime;\weight)
\]
and the backward update,
\begin{align*}
e_\ttime &= \gamma\lambda e_{\ttime-1}+\nabla_\weight \widehat{\Value}(\state_\ttime;\weight)\\
\Delta_\ttime &= \reward_\ttime+\gamma \widehat{\Value}(\state_{\ttime+1};\weight)-\widehat{\Value}(\state_{t};\weight)\\
\Delta \weight &= \alpha \Delta_\ttime e_\ttime
\end{align*}


\section{Linear Functions}

We first start with state encoding. In general we assume that
each state $\state$ is encoded by a vector $x(\state)\in
\mathbb{R}^d$. For notation, let $x(\state)=(x_1(\state),\ldots ,
x_d(\state))$. This will be useful for any hypothesis, and
specifically linear function and neural networks.

A linear function is characterized by a vector of weights
$\weight\in \mathbb{R}^d$. The value of the function at state
$\state$ is
\[
\widehat{\Value}(\state;\weight)=\weight^\top x(\state)
\]
This implies that the SGD updates become
\[
\weight_{\ttime+1}=\weight_\ttime \alpha[U_\ttime
-\widehat{\Value}(\state_\ttime;\weight_\ttime)]x(\state_\ttime)
\]

The main benefit of the linear function is when $d\ll |\States|$.
When $d=|\States|$ we can simply encode a look-up table using
$\weight$. This is done by setting the encoding of each state to be
the unit vector $x_i(\state)=I(\state=\state_i)$, and the
approximation to be $\widehat{\Value}(\state;\weight)=\sum_{i=1}^d
\weight_i I(\state=\state_i)$, which implies that
$\widehat{\Value}(\state_i;\weight)=\weight_i$.

For a linear function the gradient is simply $x(\state)$, i.e.,
$\nabla_\weight \weight^\top x(\state)=x(\state)$. The Monte-Carlo
update will become
\[
\Delta \weight=
\alpha[R_\ttime-\widehat{\Value}(\state_\ttime;\weight_\ttime)]x(\state_\ttime)
\]
For $TD(0)$ we have
\[
\Delta \weight= \alpha[\reward_\ttime+\gamma
\widehat{\Value}(\state_{\ttime+1};\weight_\ttime)-\widehat{\Value}(\state_\ttime;\weight_\ttime)]x(\state_\ttime)
\]
For $TD(\lambda)$, for the forward view, we have
\[
\Delta \weight=
\alpha[R^\lambda_\ttime-\widehat{\Value}(\state_\ttime;\weight_\ttime)]x(\state_\ttime)
\]
For the backward view we have,
\begin{align*}
e_\ttime &= \gamma\lambda e_{\ttime-1}+x(\state_\ttime)\\
\Delta_\ttime &= \reward_\ttime+\gamma \widehat{\Value}(\state_{\ttime+1};\weight_\ttime)-\widehat{\Value}(\state_{t};\weight_\ttime)\\
\Delta \weight &= \alpha \Delta_\ttime e_\ttime
\end{align*}

%\paragraph{Monte-Carlo:}

We would like to discuss the convergence of the different
algorithms. Recall that the mean squared error loss function is
strongly convex, i.e.,
\[
J(\weight)=\frac{1}{2}\sum_{\state\in\States}
\mu(\state)(\Value^\policy(\state)-\widehat{\Value}^\policy(\state;\weight))^2\;.
\]
Therefore there is a unique $\weight_{min}$ which minimizes
$J(\weight)$. Monte-Carlo is simply running a Stochastic Gradient
Decent (SGD) algorithm, and therefore the convergence and its rate
follow from known results in convex optimization.

The convergence of $TD$ is more problematic since it uses a
semi-gradient and not the true gradient. The good news is that for
linear functions when using an on-policy, there is a proof of
convergence. However, the convergence is not to $\weight_{min}$ but
rather to a different point $\weight_{TD}$. The difference is due to
the fact that we are minimizing the expression
\[
\weight_{TD}=\arg\min_\weight\frac{1}{2}\sum_s
\mu(\state)\left(\reward(\state,\policy(\state))+\gamma
\E_{\state'}[\widehat{\Value}(\state';\weight_\ttime)]-\widehat{\Value}^\policy(\state;\weight)\right)^2
\]
The difference in the losses can be bounded as follows\footnote{ The
derivation appears in Section \ref{sec:FA-Advanced}.}
\[
J(\weight_{TD})\leq \frac{1}{1-\gamma}J(\weight_{min})
\]
Note that for $\gamma\approx 1$, which is a popular setting, we
might have a huge multiplicative factor, yet bounded. However, if we
are in the realizable case, $J(\weight_{min})=0$, then we have also
$J(\weight_{TD})=0$.




\section{Off-policy}
\label{sec:TD-FA}

We would like to see what effect the samples have when
following a different policy, namely, an off-policy setting. 
For Monte Carlo there is no issue, and the same logic would still be
valid. And for TD, we did not have any problem with the look-up model. We
would like to see what can go wrong when we have a function
approximation setting.


\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{centering}
  \includegraphics[width=0.5\textwidth]{figures/L8-2states.png}\\
  \caption{Two state snippet of an MDP }\label{fig:L8-2state}
  \end{centering}
\end{figure}

Consider the following part of an MDP (see Figure
\ref{fig:L8-2state}) consisting of two nodes, with a transition from
the first to the second, with reward $0$. The main issue is that the
linear approximation gives the first node a weight $\weight$ and the
second $2\weight$. Assume we start with some value $\weight_0>0$.
Each time we have an update for the two states, we have
\[
\weight_{\ttime+1}=\weight_\ttime +\alpha[0+\gamma
(2\weight_\ttime)-\weight_\ttime]=[1+\alpha(2\gamma-1)]\weight_\ttime=[1+\alpha(2\gamma-1)]^\ttime\weight_1
\]
For $\gamma>0.5$ we have $\alpha(2\gamma-1)>0$, and $\weight_\ttime$
diverges.

We are implicitly assuming that the setting is off-policy since in
an on-policy, we would continue from the second state, and
eventually, lower the weight.

To have a ``complete'' example, consider the three-state MDP in
Figure~\ref{fig:L8-3state}. All the rewards are zero, and the main
difference is that we have a new terminating state that we reach
with probability $p$.

\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{centering}
  \includegraphics[width=0.5\textwidth]{figures/L8-3state.png}\\
  \caption{The three-state MDP. All rewards are zero.}\label{fig:L8-3state}
  \end{centering}
\end{figure}




Again, assume that we start with some $\weight_0>0$ We have three
types of updates, one per possible transition. When we transition
from the initial state to the second state, we have
\[
\Delta \weight= \alpha[0+\gamma
(2\weight_\ttime)-\weight_\ttime]\cdot
1=\alpha(2\gamma-1)\weight_\ttime
\]
The transition from the second state back to itself has an update,
\[
\Delta \weight= \alpha[0+\gamma
(2\weight_\ttime)-(2\weight_\ttime)]\cdot 2
=-4\alpha(1-\gamma)\weight_\ttime
\]
The transition to the terminal state, we have
\[
\Delta \weight= \alpha[0+\gamma 0 -(2\weight_\ttime)]\cdot
2=-4\alpha \weight_\ttime
\]

When we use on-policy, we have all transitions. Assume that the
second transition happens $n\geq 0$ times. Then we have
\[
\frac{\weight_{\ttime+1}}{\weight_\ttime}=(1+\alpha(2\gamma-1))(1-4\alpha(1-\gamma))^n(1-4\alpha)<
1-\alpha
\]
This implies that $\weight_\ttime$ converges to zero, as desired.

Now consider an off-policy that truncates the episodes after $n$
transitions of the second state, where $n\ll 1/p$, and in addition
$\gamma> 1-1/(40n)$. This implies that in most updates, we do not
reach the terminal state, and we have
\[
\frac{\weight_{\ttime+1}}{\weight_\ttime}=(1+\alpha(2\gamma-1))(1-4\alpha(1-\gamma))^n>1
\]
and therefore, for the some setting of $n$ we have that the weight
$\weight_\ttime$ diverges.

We might hope that the divergence is due to the online nature of the
TD updates. We can consider an algorithm that in each iteration
minimizes the square error. Namely,
\[
\weight_{\ttime+1}=\arg\min_\weight\sum_s
[\widehat{\Value}(\state_\ttime;\weight)-\E^\policy[\reward_\ttime+\gamma\widehat{\Value}(\state_{\ttime+1};\weight_\ttime)]]^2
\]

For the MDP example of Figure~\ref{fig:L8-3state} we have that:
\begin{align*}
 \weight_{\ttime+1} &= \arg\min_\weight (\weight-\gamma(2\weight_\ttime))^2+(2\weight-(1-p)\gamma(2\weight_\ttime))^2
\end{align*}
Solving for $\weight_{\ttime+1}$ we have
\begin{align*}
0&=  2(\weight_{\ttime+1}-\gamma(2\weight_\ttime))+4(2\weight_{\ttime+1}-(1-p)\gamma(2\weight_\ttime)\\
  10 \weight_{\ttime+1}&=4\gamma \weight_\ttime + 8\gamma(1-p)\weight_\ttime\\
  \weight_{\ttime+1}&=\frac{6-4p}{5}\gamma \weight_\ttime
\end{align*}
So for $\gamma(6-4p)>5$ we have divergence. (Recall that
$\gamma\approx 1-\varepsilon$ and $p\approx \varepsilon$ is a very
important setting.)

Note that if we have taken in to account the influence of
$\weight_{\ttime+1}$ on $\Value$ and use
$\widehat{\Value}(\state_{\ttime+1};\weight_{\ttime+1})$ instead of
$\widehat{\Value}(\state_{\ttime+1};\weight_\ttime)$, this specific
problem would have disappeared since $\weight_{\ttime+1}=0$ would
be the minimizer.

\section{Summary}

What we have seen, as Rich Sutton says, is that there is a {\em deadly
triad}:
\begin{enumerate}
\item
Function Approximation
\item
Bootstrapping methods, such as TD.
\item
Off-policy training.
\end{enumerate}
When the three conditions hold, we often have counter-examples for the convergence.
Follows is a summary of the known convergence and divergence results in
the literature:
\begin{center}
  \begin{tabular}{ | l | c | c|c|c| }
    \hline
    & algorithm & look-up table&linear function&non-linear\\ \hline
    on-policy & MC & + & + & + \\ \hline
    on-policy & $TD(0),TD(\lambda)$ & + & + & -\\ \hline
 %   on-policy & $TD(\lambda)$ & + & + & - \\ \hline
    off-policy & MC & + & + & + \\ \hline
    off-policy & $TD(0),TD(\lambda)$ & + & - & -\\ \hline
 %   off-policy & $TD(\lambda)$ & + & - & - \\ \hline
  \end{tabular}
\end{center}

The results for the look-up table were derived in Chapter
\ref{chapter:learning-model-free}.
%
The fact that Monte-Carlo methods converge is due to the fact that
they are running an SGD algorithm. Linear functions with convex
loss, will converge to the global optimum, whereas non-linear
functions (for example, neural networks) will converge to a local optimum. 
The convergence to the TD appears in Chapter
\ref{sec:FA-Advanced}. The divergence of TD with linear functions in
an off-policy setting appears in Chapter \ref{sec:TD-FA}. The TD
divergence in the non-linear online setting appears in
\cite{TsitsiklisVR97}.


%\newpage
\begin{leftbar}
\section{Advanced topics}
\label{sec:FA-Advanced}

\subsection{Comparing $\weight_{min}$ and $\weight_{TD}$}

The derivation follows \cite{TsitsiklisVR97}, however, for
simplicity, we limit ourselves to $TD(0)$ rather than $TD(\lambda)$.

We will need to start with a few definitions. Given the steady state
distribution $\mu(\state)$ for $\state\in \States$ we define a
diagonal matrix $D$ of size $|\States|\times|\States|$ where
$D(\state,\state)=\mu(\state)$. We define an inner-product
$\langle\cdot,\cdot\rangle_D$ where $\langle
z_1,z_2\rangle_D=z_1^\top Dz_2$ and a norm $\|z\|_D=\sqrt{<z,z>_D}$.

We define $\operator^\policy(\Value)= \reward + \gamma P\Value$ and recall
that: (1) $\Value^\policy = \operator^\policy(\Value^\policy)$, (2)
$\|\operator^\policy(\Value_1)-\operator^\policy(\Value_2)\|_D\leq \gamma
\|\Value_1-\Value_2\|_D$ (actually we we need to prove that for
$\|\cdot\|_D$).

We will perform a linear function approximation where the attributes
of state $\state$ are $x_\state$. Define a matrix $X$ of size
$|\States|\times d$, where the $\state$ row is $x_\state$. Then
using a parameter $\weight$ our approximation is
$\widehat{\Value}(\cdot;\weight)=X\weight$. We now need to project
vectors to an approximation $X\weight$. Give a vector $\Value$, let
$\Pi(\Value)=\arg\min_\weight \|X\weight-\Value\|_D$. This projection
operation can be written explicitly as
\[
\Pi(\Value)=X(X^\top DX)^{-1}X^\top D \Value.
\]


We can write the expected update as:
\[
\Delta \weight= X^\top D(T^\policy(X\weight_\ttime)-X\weight)
\]
which is the gradient of
\[
\nabla_\weight \|T^\policy(X\weight_\ttime)-Xw\|_D^2
\]
note that the input to $T^\policy$ is fixed and we optimize over
$\weight$ only. The minimization is
\[
\weight_{\ttime+1}=\Pi(T^\policy(X\weight_\ttime))
\]

We will need the following technical claim:
\begin{claim}
\[
\|P\Value\|_D^2\leq \|\Value\|_D^2
\]
\end{claim}

\begin{proof}
\begin{align*}
\|P\Value\|_D^2 & =\Value^\top P^\top D P \Value \\
&= \sum_s \mu(\state) (\sum_{\state'} p(\state'|\state) \Value(\state'))^2\\
&\leq \sum_s \mu(\state) \sum_{\state'} p(\state'|\state) \Value^2(\state')\\
&=\sum_{\state'} \Value^2(\state') \sum_s \mu(\state)p(\state'|\state)\\
&=\sum_{\state'} \mu(\state')\Value^2(\state')=\|\Value\|_D
\end{align*}
\end{proof}

\begin{lemma}
Let $\weight_{TD}$ be the limit of
$\weight_{\ttime+1}=\Pi(T^\policy(X\weight_\ttime))$, i.e., using
$TD(0)$. Then,
\[
\|X\weight_{TD}-\Value^\policy\|_D\leq
\frac{1}{1-\gamma}\|X\weight_{min}-\Value^\policy\|_D
\]
\end{lemma}

\begin{proof}
By definition, $\weight_{min}$ is the projection of $\Value^\policy$
to $X$, i.e., $\Pi(\Value^\policy)$. Note that we have
$\Value^\policy=
\Pi(\Value^\policy)+(\Value^\policy-\Pi(\Value^\policy))$ and
$\langle\Pi(\Value^\policy),\Value^\policy-\Pi(\Value^\policy)\rangle_D=0$,
otherwise we can improve. This implies that
$\|\Value^\policy\|_D=\|\Pi(\Value^\policy)\|_D+\|\Value^\policy-\Pi(\Value^\policy)\|_D$.
\begin{align*}
\|X\weight_{TD}-\Value^\policy\| &\leq
\|X\weight_{TD}-\Pi(\Value^\policy\|_D+\|\Pi(\Value^\policy)-\Value^\policy\|_D\\
&= \|\Pi(T^\policy(X\weight_{TD}))-\Pi(\Value^\policy)\|_D+\|\Pi(\Value^\policy)-\Value^\policy\|_D\\
&\leq \|T^\policy(X\weight_{TD})-\Value^\policy\|_D+\|\Pi(\Value^\policy)-\Value^\policy\|_D\\
&\leq \gamma \|X\weight_{TD}-\Value^\policy\|_D+\|\Pi(\Value^\policy)-\Value^\policy\|_D\\
\end{align*}
and the lemma follows since $\Pi(\Value^\policy)=X\weight_{min}$ by
definition of $\weight_{min}$.
\end{proof}


\subsection{LSTD}


We consider least Squares Temporal Difference (LSTD). We will
analyze $LSTD(0)$.

Let $x_\ttime=x_{\state_\ttime}$. The weights:
\begin{align*}
\weight_{\ttime+1} &= \weight_\ttime +\alpha (\reward_\ttime+\gamma
\weight_\ttime^\top x_{\ttime+1}-\weight_\ttime^\top
x_\ttime)x_\ttime\\
\weight_{\ttime+1} &= \weight_\ttime +\alpha (\reward_\ttime
x_\ttime-x_\ttime(x_\ttime-\gamma x_{\ttime+1})^\top \weight_\ttime)
\end{align*}

We have that
\[
\E[\weight_{\ttime+1}|\weight_\ttime]=\weight_\ttime +\alpha
(b-A\weight_\ttime)
\]
where $b=\E[\reward_\ttime x_\ttime]\in \Reals^d$ and
$A=\E[x_\ttime(x_\ttime-\gamma x_{\ttime+1})^\top]\in\Reals^{d\times
d}$.

At convergence we have $\weight=A^{-1}b$.

\end{leftbar}


\section{Applications: games}

\subsection{Backgammon}

Tesauro developed the TD-gammon in the late 1980's and 1990's. The
system uses a neural network approximation, using a $TD(\lambda)$
updates. The initial system was able to win against other computer
programs, which were hand-designed, and later matched the
performance of the best human. For the training the system used
self-play, namely playing against itself.

The neural network has a single hidden layer (with $40/80/160$
nodes, in the three different generations of the project). There are
$198$ inputs. The hidden layer has sigmoidal nodes with
$\sigma(z)=\frac{1}{1+e^{-z}}$. The output $y$ can be viewed as the
probability of winning from a given position.

The $198$ inputs are composed as follows. For each of the $24$ slots
and for each of the two colors, we have four Boolean inputs. The
first indicates if there is a single chip, the second indicates two
or more chips, the third indicates exactly three chips and the
fourth indicates four or more chips. We have two Boolean variables
that indicate whose turn it is (one for white and one for black).
Finally, we have two aggregate inputs, per color: (1) number of
chips divided by $2$ and (2) number of chips removed divided by
$15$.

The TD-gammon uses the standard TD updates.
\[
\Delta \weight_\ttime =
\alpha(y_\ttime-y_{\ttime-1})\sum_{k=1}^{t}\lambda^{\ttime-k}\nabla_\weight
y_k
\]
When the game ends we replace $y_\ttime$ by $z$ the outcome of the
game.



The evaluation of the position is done using a max-min tree, whose
depth is $1/2/3$ (depending on the generation). The max-min tree has max layer where the value of
a node is the maximum of their children, and min layer, where the
value of a node is the minimum of their children. Given the tree (of
size at most $20/400/8000$ in different generations) we evaluate the
leaves using the neural network, and propagate the values in the
min-max tree to get the predicted value of a position.

The network is initialized with small random weights. Note that due
to the game nature, even with random policies the game terminates.
The training is done using self-play, where the number of games was
$300K/800K/1500K$ in different generations.

One very intriguing aspect  of the TD-gammon is that it {\em
improved the human performance}. For example, when the first role is
{\tt 4-1}, grandmasters did the $1$ move in location $6$, while the
TD-gammon moved location $24$. Following the TD-gammon, grandmasters
changed their view and started playing those positions in the same
way that TD-gammon did.

\subsection{Atari games}

The system was developed by DeepMind researchers during $2013-2015$.
The system learns to play from the raw video frames, a variety of
$49$ games. The learning uses a neural network and employs
$Q$-learning (and a Deep-Q-Network, DQN).

The neural network works as follows. There is a preprocessing stage
that maps frames of size $210\times 160$ with 128-colors, to
$84\times 84$ and 4-colors. This mapping is fixed and shared by all
games. The neural network has three convolutional layers followed by
one completely connected layer. The output layer has multiple
outputs. The nodes in the network use RelU gates.

The DQN innovations are: (1) Experience replay, reusing the same
example many times, (2) fixed Q-target, when setting the target
update, and (3) clipping the error. To better understand the
innovations, we need to consider the flow of the training.

In the training, we first select an action $\action_\ttime$, using
$\varepsilon$-greedy policy. We observe the reward and the next
state
$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$,
and store them in the replay memory. We sample a mini-batch from the
replay memory. For each sample we use the fixed-Q-target (more
latter). We use the SGD to minimize the MSE.

For the fixed-Q-target, we do the update as follows. We have two
sets of weights $\weight_\ttime$, the current weights, and
$\weight^-_\ttime$ as the fixed weights. We update $\weight_\ttime$
to $\weight^-_\ttime$ every (large) number of plays, setting
$\weight_\ttime=\weight^-_\ttime$. The update at time $\ttime$
becomes:
\[
\Delta \weight_\ttime = \alpha \Gamma_\ttime \nabla_{\weight_\ttime}
Q(\state_\ttime,\action_\ttime;\weight_\ttime)
\]
where
\[
\Gamma_{\ttime}=\reward_\ttime+\gamma \max_\action
Q(\state_{\ttime+1},\action;\weight^-_\ttime)-Q(\state_\ttime,\action_\ttime;\weight_\ttime)
\]
Note that $\weight^-_\ttime$ is used only to estimate the future
return from $\state_{\ttime+1}$. The clipping enforces
$\Gamma_\ttime\in[-1,+1]$.

The system is able to match human performance in $29$ out of $49$
Attari games.







\section{Bibliography Remarks}

The work of Gerald Tesauro on TD-gammon is described in
\cite{Tesauro95,Tesauro02}.

The DeepMind system for playing Attari games is described in
\cite{MnihKSRVBGRFOPB15}.


Part of the outline borrows from David Silver class notes and
the book of Sutton and Barto \cite{SuttonB98}.
