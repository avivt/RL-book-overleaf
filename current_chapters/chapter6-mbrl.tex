Until now, we looked at planning problems, where we are given a
complete model of the MDP, and the goal is to either evaluate a
given policy or compute the optimal policy. In this chapter, we will
start looking at learning problems, where we need to learn from
interactions. This chapter will concentrate on {\em model based}
learning, where the agent explicitly learns an MDP model
from its experience and uses it for planning decisions.
thus the main goal is to learn an accurate model of the
MDP. In the next chapters, we will look at {\em model
free} learning, where we learn a value function or a policy without
recovering the actual underlying model.

\section{Effective horizon of discounted return}

Before we start looking at the learning setting, we will show a
``reduction'' from discounted return to finite horizon return, 
by showing that the discounted return has an {\em
effective horizon} beyond
which the effect of rewards is negligible.

\begin{theorem}
\label{thm:disc-effective-horizon}
%
Given a discount factor $\discount$, the discounted
return in the first $\tHorizon=\frac{1}{1-\discount}\log
\left(\frac{\Rmax}{\varepsilon(1-\discount)} \right)$ time steps, is within
$\varepsilon$ of the total discounted return.
\end{theorem}

\begin{proof}
Recall that the rewards are $\reward_\ttime \in [0,\Rmax]$. Fix an
infinite sequence of rewards $(\reward_0, \ldots , \reward_\ttime,
\ldots)$. We would like to consider the following difference:
\begin{align*}
%\Delta=
\sum_{\ttime=0}^\infty \reward_\ttime \discount^\ttime -
\sum_{\ttime=0}^{\tHorizon-1} \reward_\ttime \discount^\ttime =
\sum_{\ttime=\tHorizon}^\infty \reward_\ttime \discount^\ttime \leq
\frac{\discount^\tHorizon}{1-\discount}\Rmax,
\end{align*}
We want this difference to be bounded by $\varepsilon$, hence
\[
\frac{\discount^\tHorizon}{1-\discount}\Rmax\leq \varepsilon\;.
\]
This is equivalent to,
\[
\tHorizon\log (1/\discount) \geq \log
\frac{\Rmax}{\varepsilon(1-\discount)}\;.
\]
Since $\log(1+x)\leq x$, we can bound $\log (1/\discount)= \log (
1+\frac{1-\discount}{\discount} )\leq
\frac{1-\discount}{\discount}$. Since $\discount< 1$, we have that
$\frac{\discount}{1-\discount} \leq \frac{1}{1-\discount}$ and hence
it is sufficient to have $\tHorizon\geq \frac{1}{1-\discount}\log\left(
\frac{\Rmax}{\varepsilon(1-\discount)}\right) $, and the theorem follows.
\end{proof}

\section{Off-Policy Model-Based Learning}
In the off-policy setting, we have access to previous executed trajectories decomposed to
quadruples $\left(s,a,r,s'\right)$, where $r\sim R\left(s,a\right)$
and $s'\sim p\left(\cdot|s,a\right)$. Intuitively, we have a simulator that allows us to initialize to any
initial state, and thus explore the MDP as we wish. Naturally, we will have to make some assumptions about these trajectories.
Intuitively, we will need to assume that they are sufficiently
exploratory.

Our goal is to approximate the MDP model $\left(\mathcal{S},\mathcal{A},\hat{r},\hat{p}\right)$
where $\mathcal{S}$ - set of states, $\mathcal{A}$ - set of actions,
$\hat{r}\left(s,a\right)$ - approximate expected reward of $R\left(s,a\right)\in\left[0,R_{max}\right]$,
and $\hat{p}\left(s'|s,a\right)$ - approximate probability of reaching
state $s'$ when we are in state $s$ and doing action $a$.

The main question in this chapter is, how many samples of each state-action pair are
required to learn a sufficiently effective policy?

%We consider the case that we are given as input a sequence of trajectories.
%Essentially, our input will be composed of quadruples:

\subsection{Mean estimation}

We start with a basic mean estimation problem. 
Suppose we want to approximate the mean $\mu=\E\left[R\right]$
of a random variable $R\in\left[0,1\right]$ given $m$ samples denoted
$R_{1},\dots,R_{m}$. We can compute the observed mean $\widehat{\mu}=\frac{1}{m}\sum_{i=1}^{m}R_{i}$,
and by the law of large numbers $\lim_{m\to\infty}\widehat{\mu}=\E\left[R\right]$.
We would like to have concrete finite convergence bounds, mainly to
derive the value of $m$ as a function of the desired accuracy $\varepsilon$.
For this, we use concentration bounds (known as Chernoff-Hoffding bounds)
which have both an additive form and a multiplicative form, given
as follows: 

\begin{lemma}[Chernoff-Hoffding]
\label{lemma:chernoff}
%
Let $R_1, \ldots, R_m$ be $m$ i..i.d. samples of a random variable
$\Rewards\in[0,1]$. Let $\mu=\E[\Rewards]$ and
$\widehat{\mu}=\frac{1}{m}\sum_{i=1}^m R_i$. For any
$\varepsilon\in(0,1)$ we have,
\[
\Pr[|\mu-\widehat{\mu}|\geq \varepsilon]\leq 2e^{-2\varepsilon^2 m}
\]
In addition, %for $\varepsilon\in(0,1)$,
\[
\Pr[\widehat{\mu}\leq (1-\varepsilon)\mu]\leq e^{-\varepsilon^2 m
/2}\qquad \mbox{and}\qquad
 \Pr[\widehat{\mu}\geq
(1+\varepsilon)\mu]\leq e^{-\varepsilon^2 m /3}
\]
\end{lemma}
We will refer to the first bound as {\em additive} and the second
set of bounds as {\em multiplicative}.

Using the additive bound of Lemma~\ref{lemma:chernoff}, we have
\begin{corollary}
\label{cor:chernoff}
 Let $R_1, \ldots, R_m$ be $m$
i..i.d. samples of a random variable $\Rewards\in[0,1]$. Let
$\mu=\E[\Rewards]$ and $\widehat{\mu}=\frac{1}{m}\sum_{i=1}^m R_i$.
Fix $\varepsilon,\delta>0$. Then, for $m\geq
\frac{1}{2\varepsilon^2}\log (\frac{2}{\delta})$, with probability
$1-\delta$, we have that $|\mu-\widehat{\mu}|\leq \varepsilon$.
\end{corollary}


We can now use the above concentration bound in order to estimate
the expected rewards. For each state-action $(\state,\action)$ let
$\widehat{\reward}(\state,\action)=\frac{1}{m}\sum_{i=1}^m
\Rewards_i(\state,\action)$ be the average of $m$ samples. We can
show the following:

\begin{claim}
\label{claim:sample}
Given $m\geq\frac{\Rmax^{2}}{2\varepsilon^{2}}\log\left(\frac{2\left|\States\right|\left|\Actions\right|}{\delta}\right)$
samples for each state-action pair $\left(\state,\action\right)$
\[
\Pr\left(\left|\reward\left(\state,\action\right)-\widehat{\reward}\left(\state,\action\right)\right|\leq\varepsilon\right)=1-\delta
\]
\end{claim}

\begin{proof}
First we scale the random variables to $\left[0,1\right]$ by dividing
them by $\Rmax$, i.e., $r'\left(\state,\action\right)=\frac{r\left(\state,\action\right)}{\Rmax},\widehat{\reward}'\left(\state,\action\right)=\frac{\widehat{\reward}\left(\state,\action\right)}{\Rmax}$,
and from the Chernoff-Hoffding bound (Corollary~\ref{cor:chernoff}), using $\varepsilon'=\frac{\varepsilon}{\Rmax}$
and $\delta'=\frac{\delta}{\left|\States\right|\left|\Actions\right|}$,
we have for each state-action pair $\left(\state,\action\right)$
\[
\Pr\left(\left|\frac{r\left(\state,\action\right)}{\Rmax}-\frac{\widehat{\reward}\left(\state,\action\right)}{\Rmax}\right|\leq\frac{\varepsilon}{\Rmax}\right)=1-\frac{\delta}{\left|\States\right|\left|\Actions\right|}
\]

Then, using a union bound, we bound the probability over all state-action
pairs
\begin{align*}
	\Pr\left(\exists\left(\state,\action\right):\left|\frac{r\left(\state,\action\right)}{\Rmax}-\frac{\widehat{\reward}\left(\state,\action\right)}{\Rmax}\right|>\frac{\varepsilon}{\Rmax}\right) & \leq\sum_{\left(\state,\action\right)}\Pr\left(\left|\frac{r\left(\state,\action\right)}{\Rmax}-\frac{\widehat{\reward}\left(\state,\action\right)}{\Rmax}\right|>\frac{\varepsilon}{\Rmax}\right)\\
	& =1-\sum_{\left(\state,\action\right)}\Pr\left(\left|\frac{r\left(\state,\action\right)}{\Rmax}-\frac{\widehat{\reward}\left(\state,\action\right)}{\Rmax}\right|\leq\frac{\varepsilon}{\Rmax}\right)\\
	& =\sum_{\left(\state,\action\right)}\frac{\delta}{\left|\States\right|\left|\Actions\right|}=\delta
\end{align*}

Therefore, for every state-action pair $\Pr\left(\left|r\left(\state,\action\right)-\widehat{\reward}\left(\state,\action\right)\right|\leq\varepsilon\right)=1-\delta$
\end{proof}

\subsection{Estimating the Value function $\widehat{\Value}^{\policy}$ and Optimal
	policy $\widehat{\policy}^{*}$}

We would like to quantify the influence of inaccurate rewards estimates. We start with the case of finite
horizon.

\subsubsection{Influence of reward estimation errors: Finite horizon}

Fix a stochastic Markov policy $\policy\in  {\Pi _{MS}}$. We want
to compare the return using $\reward_\ttime(\state,\action)$ versus
$\widehat{\reward}_\ttime(\state,\action)$ and $\reward_\tHorizon(\state)$
versus $\widehat{\reward}_\tHorizon(\state)$. We will assume that
for every $(\state,\action)$ and $\ttime$ we have
$|\reward_\ttime(\state,\action)-\widehat{\reward}_\ttime(\state,\action)|\leq
\varepsilon$ and
$|\reward_\tHorizon(\state)-\widehat{\reward}_\tHorizon(\state)|\leq
\varepsilon$. We will show that the difference in return is bounded
by $\varepsilon(\tHorizon+1)$, where $\tHorizon$ is the finite
horizon.

Define the expected return of a policy $\policy$ with the true rewards
\[
\Value^\policy_\tHorizon(\state_0)=\E^{\policy,\state_0}[\sum_{\ttime=0}^{\tHorizon-1}
\reward_\ttime(\state_\ttime,\action_\ttime)+\reward_\tHorizon(\state_\tHorizon)].
\]
and with the estimated rewards
\[
\widehat{\Value}^\policy_\tHorizon(\state_0)=\E^{\policy,\state_0}[\sum_{\ttime=0}^{\tHorizon-1}
\widehat{\reward}_\ttime(\state_\ttime,\action_\ttime)+\widehat{\reward}_\tHorizon(\state_\tHorizon)].
\]
We are interested in bounding the difference between the two
\[
error(\policy)=|\Value^\policy_\tHorizon(\state_0)-\widehat{\Value}^\policy_\tHorizon(\state_0)|.
\]
Note that in both cases we use the true transition probability. For
a given trajectory $\sigma=(\state_0,\action_0,\ldots,
\state_{\tHorizon-1},\action_{\tHorizon-1},\state_\tHorizon)$ we
define
\[
error(\policy,\sigma)= \left(\sum_{\ttime=0}^{\tHorizon-1}
\reward_\ttime(\state_\ttime,\action_\ttime)+\reward_\tHorizon(\state_\tHorizon)\right)-
\left(\sum_{\ttime=0}^{\tHorizon-1}
\widehat{\reward}_\ttime(\state_\ttime,\action_\ttime)+\widehat{\reward}_\tHorizon(\state_\tHorizon)\right).
\]
Taking the expectation over trajectories we define,
\[
error(\policy)=|\E^{\policy,\state_0} [error(\policy,\sigma)]|.
\]

\begin{lemma}
\label{lemma:approx-FH-error}
%
Assume that for every $(\state,\action)$ and $\ttime$ we have
$|\reward_\ttime(\state,\action)-\widehat{\reward}_\ttime(\state,\action)|\leq
\varepsilon$ and that for every $\state$, we have $\left|\reward_{\tHorizon}\left(s\right)-\widehat{\reward}_{\tHorizon}\left(s\right)\right|\leq\varepsilon$.
Then, for any stochastic Markov policy $\policy\in\Policy_{MS}$, we have 
\[
\left|\Value_{\tHorizon}^{\policy}\left(\state_{0}\right)-\widehat{\Value}_{\tHorizon}^{\policy}\left(s_{0}\right)\right|\leq\varepsilon\left(\tHorizon+1\right)
\]
\end{lemma}

\begin{proof}
Since $\policy\in  {\Pi _{MS}}$ it implies that $\policy$ depends
only on the time $\ttime$ and state $\state_\ttime$.
%
Therefore, the probability of each trajectory
$\sigma=(\state_0,\action_0,\ldots,
\state_{\tHorizon-1},\action_{\tHorizon-1},\state_\tHorizon)$ is the
same under the true rewards $\reward_\ttime(\state,\action)$ and the
estimated rewards $\widehat{\reward}_\ttime(\state,\action)$,

For each trajectory $\sigma=(\state_0,\action_0,\ldots,
\state_{\tHorizon-1},\action_{\tHorizon-1},\state_\tHorizon)$, , thus 

\begin{align*}
	\left|\Value_\tHorizon^{\policy}\left(\state_{0}\right)-\widehat{\Value}_\tHorizon^{\policy}\left(\state_{0}\right)\right| & =\left|\E^{\policy,\state_{0}}\left[\left(\sum_{t=0}^{\tHorizon-1}\reward_\tHorizon\left(\state_\tHorizon,\action_\tHorizon\right)+\reward_\tHorizon\left(\state_{\tHorizon}\right)\right)-\left(\sum_{\ttime=0}^{\tHorizon-1}\widehat{\reward}_\tHorizon\left(\state_\tHorizon,\action_\tHorizon\right)+\widehat{\reward}_\tHorizon\left(\state_\tHorizon\right)\right)\right]\right|\\
	& =\left|\E^{\policy,\state_{0}}\left[\sum_{t=0}^{\tHorizon-1}\left(\reward_\tHorizon\left(\state_\tHorizon,\action_\tHorizon\right)-\widehat{\reward}_\tHorizon\left(\state_\tHorizon,\action_\tHorizon\right)\right)+\left(\reward_\tHorizon\left(\state_\tHorizon\right)-\widehat{\reward}_\tHorizon\left(\state_\tHorizon\right)\right)\right]\right|\\
	& \leq\E^{\policy,\state_{0}}\left[\sum_{t=0}^{\tHorizon-1}\left|\reward_\tHorizon\left(\state_\tHorizon,\action_\tHorizon\right)-\widehat{\reward}_\tHorizon\left(\state_\tHorizon,\action_\tHorizon\right)\right|+\left|\reward_\tHorizon\left(\state_\tHorizon\right)-\widehat{\reward}_\tHorizon\left(\state_\tHorizon\right)\right|\right]\\
	& \leq\E^{\policy,\state_{0}}\left[\tHorizon\varepsilon+\varepsilon\right]=\varepsilon\left(\tHorizon+1\right)
\end{align*}
\end{proof}

\subsubsection{Computing approximate optimal policy: finite horizon}

We now describe how to compute a near-optimal policy for the finite
horizon case. We start with the sample requirement. We need a sample
of size $m\geq \frac{1}{2\varepsilon^2}\log\left(
\frac{2|\States|\;|\Actions|\; \tHorizon}{\delta}\right)$ for each random
variable $\Rewards_\ttime(\state,\action)$ and
$\Rewards_\tHorizon(\state)$. Given the sample, we compute the
rewards estimates $\widehat{\reward}_\ttime(\state,\action)$ and
$\widehat{\reward}_\tHorizon(\state)$. By Claim~\ref{claim:sample},
with probability $1-\delta$, for every $\state\in\States$ and action
$\action \in\Actions$, we have
$|\reward_\ttime(\state,\action)-\widehat{\reward}_\ttime(\state,\action)|\leq
\varepsilon$ and
$|\reward_\tHorizon(\state)-\widehat{\reward}_\tHorizon(\state)|\leq\varepsilon$.
Now we can compute the optimal policy $\widehat{\policy}^*$ for the
estimated rewards $\widehat{\reward}_\ttime(\state,\action)$ and
$\widehat{\reward}_\tHorizon(\state)$. The main goal is to show that
$\widehat{\policy}^*$ is a near-optimal policy.

\begin{theorem}
Assume that for every $(\state,\action)$ and $\ttime$ we have
$|\reward_\ttime(\state,\action)-\widehat{\reward}_\ttime(\state,\action)|\leq
\varepsilon$ and for every $\state$ we have
$|\reward_\tHorizon(\state)-\widehat{\reward}_\tHorizon(\state)|\leq\varepsilon$.
Then,
\[
\left|\Value^{\policy^*}_\tHorizon(\state_0) -
\Value^{\widehat{\policy}^*}_\tHorizon (\state_0)\right| \leq
2\varepsilon(\tHorizon+1)
\]
\end{theorem}

\begin{proof}
%From the definition of $error(\policy)$ and since for any $\policy\in  {\Pi _{MS}}$ we showed that
By Lemma~\ref{lemma:approx-FH-error}, for any policy $\policy$, we have $\left|\Value_\tHorizon^{\policy}\left(\state_{0}\right)-\widehat{\Value}_\tHorizon^{\policy}\left(\state_{0}\right)\right|\leq\varepsilon\left(T+1\right)$.
This implies that
\begin{align*}
	\left|\Value_\tHorizon^{\policy^{*}}\left(\state_{0}\right)-\widehat{\Value}_\tHorizon^{\policy^{*}}\left(\state_{0}\right)\right| & \leq\varepsilon\left(T+1\right)\\
	\left|\Value_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)-\widehat{\Value}_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right| & \leq\varepsilon\left(T+1\right)
\end{align*}

and since $\widehat{\policy}^{*}$ is optimal for the estimated rewards
$\widehat{\reward}_\tHorizon\left(\state,\action\right)$ and $\widehat{\reward}_\tHorizon\left(s\right)$, we
have $\widehat{\Value}_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)\geq\widehat{\Value}_\tHorizon^{\policy^{*}}\left(\state_{0}\right)$

\begin{align*}
2\varepsilon\left(\tHorizon+1\right) & \geq\left|\Value_\tHorizon^{\policy^{*}}\left(\state_{0}\right)-\widehat{\Value}_\tHorizon^{\policy^{*}}\left(\state_{0}\right)\right|+\left|\Value_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)-\widehat{\Value}_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|\\
& \geq\left|\Value_\tHorizon^{\policy^{*}}\left(\state_{0}\right)-\widehat{\Value}_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|+\left|-\Value_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)+\widehat{\Value}_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|\\
& \geq\left|\Value_\tHorizon^{\policy^{*}}\left(\state_{0}\right)-{\widehat{\Value}_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)}+{\widehat{\Value}_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)}-\Value_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|\\
& \geq\left|\Value_\tHorizon^{\policy^{*}}\left(\state_{0}\right)-\Value_\tHorizon^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|
\end{align*}

In the second inequality, we substituted the third inequality
and swapped the signs in the second absolute value. 
\end{proof}

\subsubsection{Influence of reward estimation errors: discounted return}

Fix a stationary stochastic policy $\policy\in {\Pi _{SS}}$.
Again, define the expected return of policy $\policy$ with the true
rewards
\[
\Value^\policy_\discount(\state_0)=\E^{\policy,\state_0}[\sum_{\ttime=0}^\infty
\reward(\state_\ttime,\action_\ttime)\discount^\ttime]
\]
and with the estimated rewards
\[
\widehat{\Value}^\policy_\discount(\state_0)=\E^{\policy,\state_0}[\sum_{\ttime=0}^\infty
\widehat{\reward}(\state_\ttime,\action_\ttime)\discount^\ttime ]
\]
We are interested in bounding the difference between the two
\[
error(\policy)=|\Value^\policy_\discount(\state_0)-\widehat{\Value}^\policy_\discount(\state_0)|
\]
%Note that as for the finite horizon, in both cases we use the true
%transition probability.
For a given trajectory
$\sigma=(\state_0,\action_0,\ldots$)
%\state_{\tHorizon-1},\action_{\tHorizon-1},\state_\tHorizon)$ 
we define
\[
error(\policy,\sigma)= \sum_{\ttime=0}^\infty \discount^\ttime
\reward_\ttime(\state_\ttime,\action_\ttime)- \sum_{\ttime=0}^\infty
\discount^\ttime \hat{\reward}_\ttime(\state_\ttime,\action_\ttime)
\]
Again, taking the expectation over trajectories we define,
\[
error(\policy)=|\E^{\policy,\state_0} [error(\policy,\sigma)]|.
\]

\begin{lemma}
\label{lemma:approx-disc-error}
%
Assume that for every $\left(\state,\action\right)$ we have $\left|r\left(\state,\action\right)-\widehat{\reward}\left(\state,\action\right)\right|\leq\varepsilon$.
Then, for any stationary stochastic policy $\policy\in\Policy_{SS}$ we have
\[
\left|\Value_{\discount}^{\policy}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\policy}\left(\state_{0}\right)\right|\leq\frac{\varepsilon}{1-\discount}
\]
\end{lemma}

\begin{proof}
Since the policy $\policy\in {\Pi _{SS}}$ is stationary, the
probability of each trajectory $\sigma=(\state_0,\action_0,\ldots,
\state_{\tHorizon-1},\action_{\tHorizon-1},\state_\tHorizon)$ is the
same under $\reward(\state,\action)$ and
$\widehat{\reward}(\state,\action)$. For each trajectory
$\sigma=(\state_0,\action_0,\ldots,
\state_{\tHorizon-1},\action_{\tHorizon-1},\state_\tHorizon)$, we
have,
\begin{align*}
	\left|\Value_{\discount}^{\policy}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\policy}\left(\state_{0}\right)\right| & =\left|\E^{\policy,\state_{0}}\left[\left(\sum_{\ttime=0}^{\infty}r\left(s_\tHorizon,\action_\tHorizon\right)\discount^\tHorizon\right)-\left(\sum_{\ttime=0}^{\infty}\widehat{\reward}\left(s_\tHorizon,\action_\tHorizon\right)\discount^\tHorizon\right)\right]\right|\\
	& =\left|\E^{\policy,\state_{0}}\left[\sum_{\ttime=0}^{\infty}\left(r\left(s_\tHorizon,\action_\tHorizon\right)-\widehat{\reward}\left(s_\tHorizon,\action_\tHorizon\right)\right)\discount^\tHorizon\right]\right|\\
	& \leq\E^{\policy,\state_{0}}\left[\sum_{\ttime=0}^{\infty}\left|r\left(s_\tHorizon,\action_\tHorizon\right)-\widehat{\reward}\left(s_\tHorizon,\action_\tHorizon\right)\right|\discount^\tHorizon\right]\\
	& \leq\E^{\policy,\state_{0}}\left[\sum_{\ttime=0}^{\infty}\varepsilon\discount^\tHorizon\right]=\varepsilon\sum_{\ttime=0}^{\infty}\discount^\tHorizon=\frac{\varepsilon}{1-\discount}
\end{align*}
\end{proof}

\subsubsection{Computing approximate optimal policy: discounted return}

We now describe how to compute a near-optimal policy for the
discounted return.
%we start with the sample requirement.
We need a sample of size $m\geq \frac{\Rmax^2}{2\varepsilon^2}\log\left(
\frac{2|\States|\;|\Actions|}{\delta}\right)$ for each random variable
$\Rewards(\state,\action)$. Given the sample, we compute
$\widehat{\reward}(\state,\action)$. As we saw in the finite horizon
case, with probability $1-\delta$, we have for every
$(\state,\action)$ that
$|\reward(\state,\action)-\widehat{\reward}(\state,\action)|\leq
\varepsilon$. Now we can compute the policy $\widehat{\policy}^*$
for the estimated rewards
$\widehat{\reward}_\ttime(\state,\action)$. Again, the main goal is
to show that $\widehat{\policy}^*$ is a near optimal policy.

\begin{theorem}
\label{thm:approx-model-disc}
%
Assume the for each state-action pair $\left(\state,\action\right)$ we have
a sample size of $m\geq\frac{\Rmax^{2}}{2\varepsilon^{2}}\log\left(\frac{2\left|\States\right|\left|\Actions\right|}{\delta}\right)$,
thus for every $\left(\state,\action\right)$ we have $\left|\reward\left(\state,\action\right)-\widehat{\reward}\left(\state,\action\right)\right|\leq\varepsilon$.
Then,
\[
\left|\Value_{\discount}^{\policy^{*}}\left(\state_{0}\right)-\Value_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|\leq\frac{2\varepsilon}{1-\discount}
\]
\end{theorem}

\begin{proof}
%From the definition of $error(\policy)$ and since
By Lemma \ref{lemma:approx-disc-error} for any stationary stochastic policy $\policy\in\Policy_{SS}$, we have
$\left|\Value_{\discont}^{\policy}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\policy}\left(\state_{0}\right)\right|\leq\frac{\varepsilon}{1-\discount}$.
This implies that
\begin{align*}
	\left|\Value_{\discount}^{\policy^{*}}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\policy^{*}}\left(\state_{0}\right)\right| & \leq\frac{\varepsilon}{1-\discount}\\
	\left|\Value_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right| & \leq\frac{\varepsilon}{1-\discount}
\end{align*}
and since $\widehat{\policy}^{*}$ is optimal for the estimated rewards
$\widehat{\reward}\left(\state,\action\right)$ , we have $\widehat{\Value}_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\geq\widehat{\Value}_{\discount}^{\policy^{*}}\left(\state_{0}\right)$
\begin{align*}
\frac{2\varepsilon}{1-\discount} & \geq\left|\Value_{\discount}^{\policy^{*}}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\policy^{*}}\left(\state_{0}\right)\right|+\left|\Value_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|\\
& \geq\left|\Value_{\discount}^{\policy^{*}}\left(\state_{0}\right)-\widehat{\Value}_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|+\left|-\Value_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)+\widehat{\Value}_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|\\
& \geq\left|\Value_{\discount}^{\policy^{*}}\left(\state_{0}\right)-{\widehat{\Value}_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)}+{\widehat{\Value}_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)}-\Value_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|\\
& \geq\left|\Value_{\discount}^{\policy^{*}}\left(\state_{0}\right)-\Value_{\discount}^{\widehat{\policy}^{*}}\left(\state_{0}\right)\right|
\end{align*}
in the second inequality, we've substituted the third inequality
and swapped the signs in the second absolute value. 
\end{proof}

\subsection{Estimating the transition probabilities $\widehat{\transitionprob}\left(\state'|\state,\action\right)$}

For a given state-action pair $\left(\state,\action\right)$, we consider $m$
i.i.d transition $\left(\state,\action,s'_{i}\right)$ where $i\in\left[1,m\right]$.We
define the observed transition distribution,
\[
\widehat{\transitionprob}(\state'|\state,\action)=\frac{|\{i:\state'_i=\state',\state_{i-1}=\state,\action_{i-1}=\action\}|}{m}
\]
we would like to evaluate the observed model $\widehat{M}$ as a function
of the sample size $m$.

%{\bf [[The description here differs from the one in the class and
%the slides. Hopefully it is simpler and easier tp follow.]]}

We start with a general well-known observation about distributions.

\begin{theorem}
\label{thm:dist-l1} Let $q_1$ and $q_2$ be two distributions over
$\States$. Let $f:\States \rightarrow [0,F_{max}]$. Then,
\[
|\E_{s\sim q_1}[f(\state)]- \E_{s\sim q_2}[f(\state)]|\leq F_{max}
\|q_1-q_2\|_1
\]
where $\|q_1-q_2\|_1=\sum_{\state \in \States}
|q_1(\state)-q_2(\state)|$.
\end{theorem}

\begin{proof} Consider the following derivation,
\begin{align*} |\E_{s\sim q_1}[f(\state)]- \E_{s\sim q_2}[f(\state)]|
&=|\sum_{\state \in \States} f(\state)q_1(\state) - \sum_{\state \in \States} f(\state)q_2(\state)| \\
&\leq \sum_{\state \in \States} f(\state)|q_1(\state) - q_2(\state)|\\
 &\leq F_{max} \|q_1-q_2\|_1,
\end{align*}
where the first identity is the explicit expectation, the second is by the triangle inequality, and the third is by bounding the values of $f$ by the maximum possible value.
\end{proof}

One way to measure the distance between two Markov chains $M_{1},M_{2}$
is to measure the difference between the next state distributions
of each $i$, namely 
$\sum_{j}\left|M_{1}\left[i,j\right]-M_{2}\left[i,j\right]\right|$
and take the worst case, i.e., $\max_{i}\sum_{j}\left|M_{1}\left[i,j\right]-M_{2}\left[i,j\right]\right|\triangleq\left\Vert M_{1}-M_{2}\right\Vert _{\infty}$.
In other words, if $\left\Vert M_{1}-M_{2}\right\Vert _{\infty}\leq\alpha$,
the next state distributions differ by at most by $\alpha$ in the
$L_{1}$ norm. Moreover, if $\alpha\approx0$, the distributions are
almost identical.
Next, we will proceed to obtain a quantitative bound on the difference,
that will allow us to derive an upper bound of the required sample
size $m$.

%Therefore, we would like to bound the $L_1$-norm between the state
%distributions generated by $M_1$ and $M_2$.

\begin{theorem}
\label{thm:l1-error}
%
Assume that $\|M_1-M_2\|_{\infty,1}\leq \alpha$.
%
Let $q_1^\ttime$ and $q_2^\ttime$ be the distribution over states
after trajectories  of length $\ttime$ of $M_1$ and $M_2$,
respectively. Then,
\[
\|q_1^\ttime-q_2^\ttime\|_1\leq \alpha  \ttime
\]
\end{theorem}

\begin{proof}
Let $p_0$ be the distribution of the start state. Then
$q_1^\ttime=p_0^\top M_1^\ttime$ and $q_2^\ttime=p_0^\top
M_2^\ttime$. The proof is by induction on $\ttime$. Clearly, for
$\ttime=0$ we have $q_1^0=q_2^0=p_0^\top$.

We start with a few basic facts about matrix norms. Recall that
$\|M\|_{\infty,1} = \max_i \sum_j |M[i,j]|$. Then,
\begin{equation}
\label{eq_norm_matrix_1}
%
\| z M\|_1 = \sum_j | \sum_i z[i] M[i,j]|\leq \sum_{i,j} |z[i]|\;
|M[i,j] | = \sum_i |z[i]| \sum_j  |M[i,j]| \leq \|z\|_1
\|M\|_{\infty,1}
\end{equation}

This implies the following two simple facts. First, let $q$ be a
distribution, i.e., $\|q\|_1=1$, and $M$ a matrix such that
%with all the entries at most $\alpha$, i.e., $|M[i,j]|\leq\alpha$ which implies
$\|M\|_{\infty,1} \leq \alpha $. Then,
\begin{equation}
\label{eq_norm_matrix_qM} \|qM\|_1 \leq \|q\|_1 \|M\|_{\infty,1}
 \leq \alpha
\end{equation}
Second, let $M$ be a row-stochastic matrix,  implies that
$\|M\|_{\infty,1}=1$. Then,
\begin{equation}
\label{eq_norm_matrix_zM} \|zM\|_1 \leq \|z\|_1 \|M\|_{\infty,1}
\leq \|z\|_1
\end{equation}

For the induction step, let $z^\ttime=q_1^\ttime-q_2^\ttime$, and assume that 
$\|z^{\ttime-1}\|_1 \leq \alpha (\ttime-1)$. We
have,
\begin{align*}
\|q_1^\ttime-q_2^\ttime\|_1 &= \|p_0^\top M_1^\ttime-p_0^\top M_2^\ttime\|_1\\
&= \|q_1^{\ttime-1}M_1 - (q_1^{\ttime-1}-z^{\ttime-1})M_2\|_1\\
&\leq \|q_1^{\ttime-1} (M_1-M_2)\|_1 + \|z^{\ttime-1}M_2\|_1\\
&\leq \alpha + \alpha (\ttime-1) = \alpha\ttime,
\end{align*}
where the last inequality is derived as follows. For the first
term we used Eq.~(\ref{eq_norm_matrix_qM}). For the second term we
used Eq.~(\ref{eq_norm_matrix_zM}) with the inductive claim.
\end{proof}

\subsubsection{Approximate model and simulation lemma}

We define an {\em $\alpha$-approximate model} as follows.
%
\begin{definition}
A model $\widehat{M}$ is an $\alpha$-approximate model of $M$ if for
every state-action pair $\left(\state,\action\right)$ we have: 
\begin{enumerate}
	\item $\left|\widehat{\reward}\left(\state,\action\right)-r\left(\state,\action\right)\right|\leq\alpha$
	\item $\left\Vert \widehat{\transitionprob}\left(\cdot|\state,\action\right)-\widehat{\transitionprob}\left(\cdot|\state,\action\right)\right\Vert _{1}\leq\alpha$
\end{enumerate}
\end{definition}

Since we have two different models, we define the following
\begin{definition}
$\Value^{\policy}\left(\state_{0};M\right)$ is the value function of
a policy $\policy$ in a model $M$ 
\end{definition}

The following simulation lemma, for the finite horizon case,
guarantees that approximate models have similar return.

\begin{lemma}
\label{lemma:approx-model-FH}
%
%Assume that $\|M_1-M_2\|_{\infty,1}\leq \alpha$.
%
Fix $\alpha\leq \frac{\varepsilon}{\Rmax \tHorizon^2}$, and
assume that model $\widehat{M}$ is an $\alpha$-approximate model of
$M$. For the finite horizon return, for any policy $\policy \in
 {\Pi _{MS}}$, we have
\[
|\Value^\policy_\tHorizon(\state_0;M)-\Value^\policy_\tHorizon(\state_0;\widehat{M})|\leq
\varepsilon
\]
\end{lemma}

\begin{proof}
By Theorem~\ref{thm:l1-error} the distance between the state
distributions of $M$ and $\widehat{M}$ at time $\ttime$ is bounded
by $\alpha\ttime$. Since the maximum reward  is $\Rmax$, by
Theorem~\ref{thm:dist-l1} the difference is bounded by
$\sum_{\ttime=0}^\tHorizon \alpha\ttime \Rmax\leq \alpha\tHorizon^2
\Rmax$. For $\alpha\leq \frac{\varepsilon}{\Rmax \tHorizon^2}$ it
implies that the difference is at most $\varepsilon$.
\end{proof}


We now present the simulation lemma for the discounted return
case, which also guarantees that approximate models have similar
return.

\begin{lemma}
\label{lemma:approx-model-dic}
%
Fix $\alpha\leq \frac{(1-\discount)^2\varepsilon}{\Rmax}$, and
assume that model $\widehat{M}$ is an $\alpha$-approximate model of $M$. For the discounted return, for any policy $\policy \in  {\Pi_{SS}}$, we have
\[
|\Value^\policy_\discount(\state_0;M)-\Value^\policy_\discount(\state_0;\widehat{M})|\leq
\varepsilon
\]
%$\alpha\leq \frac{ 0.5 \varepsilon(1-\discount)^2}{\Rmax\log^2(\Rmax/(\varepsilon(1-\discount)))}$.
%, for some constant $c>0$.
\end{lemma}

\begin{proof}
By Theorem~\ref{thm:l1-error} the distance between the state distributions of $M$ and $\widehat{M}$ at time $\ttime$ is bounded by $\alpha\ttime$. Since the maximum reward  is $\Rmax$, by Theorem~\ref{thm:dist-l1} the difference is bounded by
$\sum_{\ttime=0}^\infty \alpha\ttime \Rmax\discount^t$.
The sum 
$$\sum_{\ttime=0}^\infty \ttime \discount^t=\frac{\discount}{1-\discount}\sum_{\ttime=0}^\infty \ttime \discount^{t-1}(1-\discount)=\frac{\discount}{(1-\discount)^2}< \frac{1}{(1-\discount)^2}$$
where the last equality uses the expected value of a geometric distribution with parameter $\discount$.
%
%$\leq \alpha\tHorizon^2\Rmax$. For $\alpha\leq \frac{\varepsilon}{\Rmax \tHorizon^2}$ it
Using the bound for $\alpha$ implies that the difference is at most $\varepsilon$.
%
%We reduce the discounted setting to the finite horizon setting. By Theorem~\ref{thm:disc-effective-horizon}, a horizon $\tHorizon=\frac{1}{1-\discount}\log\frac{\Rmax}{\varepsilon(1-\discount)/2}$, guarantees that the error due to truncating at horizon $\tHorizon$ is at most $\varepsilon/2$. 
%By Lemma~\ref{lemma:approx-model-FH}, for a horizon $\tHorizon$ and $\alpha\leq \frac{\varepsilon/2}{\Rmax| \tHorizon^2}$ we get and error at most $\varepsilon/2$. By substituting the bound for $\tHorizon$ in the expression for $\alpha$ we derive the theorem.
\end{proof}

\subsubsection{Putting it all together}

We want with high probability ($1-\delta$) to have an
$\alpha$-approximate model. For this we need to bound the sample
size needed to approximate a distribution in the norm $L_1$. Here,
Bretagnolle Huber-Carol inequality comes handy.

\begin{lemma}[Bretagnolle Huber-Carol]
Let $X$ be a random variable taking values in $\{1, \ldots , k\}$,
where $\Pr[X=i]=p_i$. Assume we sample $X$ for $n$ times and
observe the value $i$ in $\hat{n}_i$ outcomes. Then,
\[
\Pr[\sum_{i=1}^k \left|\frac{\hat{n}_i}{n}-p_i\right|\geq
\lambda]\leq 2^{k+1} e^{-n\lambda^2/2}
\]
\end{lemma}

For completeness we give the proof. (The proof can also be found at
Proposition A6.6 of %van der Vaart and Wellner 
\cite{van1996weak})
\begin{proof}
Note that,
\[
\sum_{i=1}^k |\frac{\hat{n}_i}{n}-p_i| = 2\max_{S\subset [k]}
\sum_{i\in S} \frac{\hat{n}_i}{n}-p_i,
\]
which follows by taking $S=\{i:\frac{\hat{n}_i}{n}\geq p_i\}$.

We can now perform a concentration bound (Chernoff-Hoeffding, Lemma~\ref{lemma:chernoff}) for
each subset $S\subset [k]$, and get that the deviation is $\lambda$
with probability at most $e^{-n\lambda^2/2}$. Using a union bound
over all $2^k$ subsets $S$ we
 derive the lemma.
\end{proof}

The above lemma implies that to get, with probability $1-\delta$,
accuracy $\alpha$ for each $(\state,\action)$, it is sufficient to
sample $m=O(\frac{|\States| +
\log(|\States|\;|\Actions|/\delta)}{\alpha^2} )$ samples for each
state-action pair $(\state,\action)$. Plugging in the value of
$\alpha$, for the finite horizon, we have
\[
m=O(\frac{\Rmax^2}{\varepsilon^2} \tHorizon^4 (|\States|+\log
(|\States|\;|\Actions|/\delta))),
\]
and for the discounted return
\[
m=O(\frac{\Rmax^2}{\varepsilon^2}  \frac{1}{(1-\discount)^4}
(|\States|+\log (|\States|\;|\Actions|/\delta)).
%\log^4\frac{\Rmax}{\varepsilon(1-\discount)}))
\]


Assume we have a sample of $m$ for each $(\state,\action)$. Then
with probability $1-\delta$ we have an $\alpha$-approximate model
$\widehat{M}$.
%
We compute an optimal policy $\widehat{\policy}^*$ for
$\widehat{M}$.
%
This implies that $\widehat{\policy}^*$ is a $2\varepsilon$-optimal
policy. Namely,
\[
|\Value^*(\state_0)-\Value^{\widehat{\policy}^*}(\state_0)|\leq
2\varepsilon
\]


When considering the total sample size, we need to consider all
state-action pairs. For the finite horizon, the total sample size is
\[
m\tHorizon|\States|\;|\Actions|=O(\frac{\Rmax^2}{\varepsilon^2}
|\States|^2 |\Actions| \tHorizon^5 \log
(|\States|\;|\Actions|/\delta)),
\]
and for the discounted return
\[
m|\States|\;|\Actions|=O(\frac{\Rmax^2}{\varepsilon^2(1-\discount)^4} |\States|^2 |\Actions|  \log(|\States|\;|\Actions|/\delta)).
%\log^4\frac{\Rmax}{\varepsilon(1-\discount)})
\]

We can now look on the dependency of our sample complexity and its
dependence on the various parameters.
\begin{enumerate}
	\item The required sample size scales like $\frac{\Rmax^{2}}{\varepsilon^{2}}$
	which looks like the right bound, even for estimation of random variables
	expectations. 
	\item The dependency on the horizon is necessary, although it is probably
	not optimal. 
	\item The dependency on the number of states $\left|\States\right|$
	and actions$\left|\Actions\right|$, is due to the fact that we
	like a very high approximation of the next state distribution. We
	need to approximate $\left|\States\right|^{2}\left|\Actions\right|$
	parameters, so for this task the bound is reasonable. 
\end{enumerate}

\subsection{Improved sample bound: Approximate Value
Iteration (AVI)}

We will show that if we restrict the task to compute an approximate
optimal policy value $\widehat{\Value^{\policy^{*}}}\left(s\right)$, by using
approximate the Value Iteration algorithm we can reduce the sample
size $\left|\States\right|^{2}\left|\Actions\right|$ by a
factor of approximately $\left|\States\right|$.
Recall (seebChapter~\ref{sec:VI})., that the Value Iteration algorithm works as follows. We Initialize
$\Value_{0}\left(s\right)$ arbitrary and for $n=0,\dots$ compute
\[
\Value_{n+1}\left(\state\right)=\max_{a\in\Actions_{n}}\left\{ \reward\left(\state,\action\right)+\discount\sum_{\state'\in\States_{n+1}}\transitionprob\left(\state'|\state,\action\right)\Value_{n}\left(\state'\right)\right\} =\max_{a\in\Actions_{n}}\left\{\reward\left(\state,\action\right)+\discount\E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}\left[\Value_{n}\left(\state'\right)\right]\right\} 
\]
We showed that $\lim_{n\rightarrow\infty}\Value_{n}=\Value^{\policy^{*}}$,
and that the error rate is $\mathcal{O}(\frac{\discount^{n}}{1-\discount}\Rmax)$.
This implies that if we run for $N=\frac{1}{1-\discount}\log\frac{\Rmax}{\varepsilon(1-\discount)}$
iterations, we have an error of at most $\varepsilon$.
We would like to approximate the Value Iteration algorithm using a
sample. Namely, for each $(\state,\action)$ we have a sample of size $m$, i.e.,$\left\{ (\state,\action,\reward_{i},\state'_{i})\right\} _{i\in[1,m]}$
The Approximate Value Iteration (AVI) using the sample would be, 
\[
\widehat{\Value}_{n+1}(\state)=\max_{\state\in as}\left\{ \frac{1}{m}\sum_{i=1}^{m}\reward_{i}(\state,\action)+\discount\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})\right\} =\max_{\state\in as}\left\{ \widehat{\reward}\left(\state,\action\right)+\discount\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})\right\} 
\]
The intuition is that if we have a large enough sample, AVI will approximate
the Value Iteration. If we set $m=O\left(\frac{V_{max}^{2}}{\varepsilon'^{2}}\log\left(\frac{N\left|\States\right|\left|\Actions\right|}{\delta}\right)\right)$,
where $V_{max}$ bounds the maximum value, i.e., for finite horizon $V_{max}=T\Rmax$
and for discounted return $V_{max}=\frac{\Rmax}{1-\discount}$, with
probability $1-\delta$, for every $(\state,\action)$ and any iteration $n\in[1,N]$
we have: 
\begin{align*}
	\left|\E[\widehat{\Value}_{n}(\state')]-\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})\right| & \leq\varepsilon'\\
	\left|\widehat{\reward}(\state,\action)-r(\state,\action)\right| & \leq\varepsilon'
\end{align*}
Assume that for every state $\state\in\States$ we have 
\[
\left|\widehat{\Value}_{n}(\state)-\Value_{n}(\state)\right|\leq\lambda
\]
Then 
\begin{align*}
	\left|\widehat{\Value}_{n+1}(\state)-\Value_{n+1}(\state)\right|= & \left|\max_{\state}\left\{ \widehat{\reward}(\state,\action)+\discount\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})\right\} -\max_{\state}\left\{ r(\state,\action)+\discount E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\Value_{n}(\state')]\right\} \right|\\
	\leq & \max_{\state}\left|\widehat{\reward}(\state,\action)+\discount\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})-r(\state,\action)-\discount E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\Value_{n}(\state')]\right|\\
	\leq & \max_{\state}\left\{ \Bigg|\widehat{\reward}(\state,\action)-r(\state,\action)\Bigg|+\discount\left|\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})-E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\Value_{n}(\state')]\right|\right\} \\
	= & \max_{\state}\left\{ \varepsilon'+\discount\left|\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})-E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\Value_{n}(\state')]\pm E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\widehat{\Value}_{n}(\state')]\right|\right\} \\
	\leq & \max_{\state}\left\{ \varepsilon'+\discount\left|\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}(\state'_{i})-E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\widehat{\Value}_{n}(\state')]\right|\right.\\
	& \left.+\discount\left|E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\widehat{\Value}_{n}(\state')]-E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}[\Value_{n}(\state')]\right|\right\} \\
	\leq & \max_{\state}\left\{ \varepsilon'+\discount\varepsilon'+\discount\left|E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}\left[\widehat{\Value}_{n}(\state')-\Value_{n}(\state')\right]\right|\right\} \\
	\leq & \max_{\state}\left\{ \varepsilon'+\discount\varepsilon'+\discount\left|E_{\state'\sim \transitionprob\left(\cdot|\state,\action\right)}\left[\lambda\right]\right|\right\} \\
	\leq & \max_{\state}\left\{ \varepsilon'+\discount\varepsilon'+\discount\lambda\right\} =\left(1+\discount\right)\varepsilon'+\discount\lambda
\end{align*}
Since $\widehat{V}_{0}(\state)=V_{0}(\state)$, $\left|\widehat{V}_{0}(\state)-V_{0}(\state)\right|=0$,
the recurrence above gives: 
\begin{align*}
	\left|\widehat{\Value}_{n+1}(\state)-\Value_{n+1}(\state)\right| & \leq\left(1+\discount\right)\varepsilon'+\discount\left|\widehat{V}_{n}(\state)-\Value_{n}(\state)\right|\\
	& \leq\left(1+\discount\right)\varepsilon'+\discount\left(1+\discount\right)\varepsilon'+\discount^{2}\left|\widehat{V}_{n}(\state)-\Value_{n}(\state)\right|\\
	& \leq\left(1+\discount\right)\varepsilon'+\dots+\discount^{n-1}\left(1+\discount\right)\varepsilon+\discount^{n}\cdot0\\
	& \leq(1+\discount)\varepsilon'\cdot\sum_{i=0}^{n-1}\discount^{i}\leq\frac{(1+\discount)\varepsilon'}{1-\discount}\leq\frac{\varepsilon'}{1-\discount}
\end{align*}
This implies that the Approximate Value Iteration has error at most $\frac{\varepsilon'}{1-\discount}$. The main result is that we can run Approximate Value Iteration algorithm for $N$ iterations and approximate well the \textbf{optimal value} function and policy.\\

Algorithm outline for finding $\widehat{\Value}^{\policy^{*}}$:
\begin{enumerate}
	\item Initialize simulator to state $\state$
	\item Perform action $a$ and observe the next state $\state'$
	\item Repeat stages $1-2$ , $m$ times and calculate $\frac{1}{m}\sum_{i=1}^{m}\widehat{\Value}_{n}\left(s_{i}'\right)$
	\item Perform $VI$ step and calculate $\widehat{\Value}_{n+1}\left(\state\right)$
	\item Repeat stages $1-4$ $N$ times until $VI$ converges
\end{enumerate}

\begin{theorem}
Given for every state-action pair a sample of size
\[
m=O\left(\frac{R^2_{max}}{(1-\discount)^4\varepsilon^2}\log \frac{ |\States| \;|\Actions|\log \left(
\frac{\Rmax}{\varepsilon(1-\discount)} \right) }{(1-\discount)\delta}\right)
\]
Running the Approximate Value Iteration for
$N=\frac{1}{1-\discount}\log \frac{\Rmax}{\varepsilon(1-\discount)}$
iterations results in an $\varepsilon$-approximation of the optimal value
function.
\end{theorem}

The implicit drawback of the above theorem is that we are
approximating only the optimal policy, and cannot evaluate an
arbitrary policy.

\section{On-Policy Learning}

Unlike the off-policy setting, where we assume that the trajectories
are exploratory enough that we can learn the model and use it to approximate
the optimal policy, in the online setting, is the learner'\state responsibility
to perform the exploration, and intuitively, we cannot jump to any arbitrary state as we wish, 
thus we can only "play" the MDP and collect data observed along the way. We will consider two (similar) tasks
\begin{enumerate}
	\item Reconstruct the MDP to sufficient accuracy, such that we can compute
	the optimal policy for it and be guaranteed that it is a near optimal
	policy in the true MDP. 
	\item Reconstruct only the parts of the MDP which have a significant influence
	on the optimal policy. In this case we will be able to show that in
	most time steps we are playing a near optimal action. 
\end{enumerate}

\subsection{Learning a Deterministic Decision Process}

Recall that a Deterministic Decision Process (DDP) is modeled by a
directed graph, where the states are the vertices, and each action
is associated with an edge. For simplicity we will assume that the
graph is strongly connected, i.e., there is a directed path between
any two states. (See Chapter~\ref{chapter:DDP}.)

%For a directed graph $G(V,E)$ we define a traversal as a path (not
%necessarily simple) which traverses each edge at least once. Note
%that given a traversal of the directed graph a DDP, then we
%reconstruct all the dynamics (next states) and the rewards (which we
%observe when we traverse edges.
%
%For a strongly connected graph $G(V,E)$ we can build a traversal of
%length $O(|V|\cdot |E|)$. In building the traversal we assume that
%we are only given a start state $v_0$ and for any node we reach
%
%\begin{theorem}
%For any strongly connected DDP
%$(\States,\Action,f,\Rewards,\state_0)$, given the initial states
%$\state_0$ we can construct the DDP after a trajectory of length at
%most $O(|\States|\cdot |\Action|)$.
%\end{theorem}
%
%\begin{proof}
%We will have two sets: (1) the set $Visited$ will include all the
%state we already visited at least once. (2) The set $Explored$ will
%include quadruples $(\state,\action,\reward,\state')$ such that
%$\state \in Visited$ and st some time $\ttime$ we are in
%$\state_\ttime = \state$ and perform action $\action_\ttime=\action$
%and observe reward $\reward$ and the next state is
%$\state_{\ttime+1}=\state'$.
%
%We define the observed model given $Visited$ as follows. For
%$(\state_\ttime,\action_\ttime ,\reward_\ttime,
%\state_{\ttime+1})\in Explored$, we define an observed model
%$\widehat{M}$, where
%$\widehat{f}(\state_\ttime,\action_\ttime)=\state_{\ttime+1}$ and
%$\widehat{\reward}(\state,\action)=\reward_\ttime$.
%
%\end{proof}
%
%
%For reconstructing a DDP we essentially need a traversal, i.e.,

We will start by showing how to recover the DDP. The basic idea is
rather simple. We partition the state-action pairs to \texttt{known}
and \texttt{unknown}. Initially all states-action pairs are \texttt{unknown}.
Each \texttt{unknown} state-action that we execute is moved to \texttt{known}. Each
time we look for a path from the current state to some \texttt{unknown}
state-action pair. When all the state-action pairs are \texttt{known} we are
done. This implies that we have at most $|\States|\;|\Actions|$
iterations, and since the maximum length of such a path is at most
$|\States|$, the total number of time steps would be bounded by
$|\States|^2\;|\Actions|$.

To compute a path from the \texttt{known} state-action pairs to some \texttt{unknown}
state-action pair, we reduce this task to a planning task in DDP.
%
For each \texttt{known} state-action pair  we define the reward to be zero and the
next state to be the observed next state. For each \texttt{unknown}
state-action pair we define the reward to be $\Rmax$ and the next state
as the same state, i.e., we stay in the same state. We can now solve for the optimal
policy (infinite horizon average reward) of our model. As long as
there are unobserved state-action pairs, the optimal policy will
reach one of them.
%The we redefine the model, and compute a new
%optimal policy. This implies that we have $O(|\States|\cdot
%|\Actions|)$ iteration, and each iteration lasts at most
%$O(|\States|)$ time steps. This implies that the overall time steps
%is at most $O(|\States|^2 |\Actions|)$.

\begin{theorem}
For any strongly connected DDP there is a strategy $\rho$ which
recovers the DDP in at most $O(|\States|^2 |\Actions|)$
%$(\States,\Action,f,\Rewards,\state_0)$, given the initial states
%$\state_0$ we can construct the DDP after a trajectory of length at
%most $O(|\States|\cdot |\Action|)$.
\end{theorem}

\begin{proof}
We first define the explored model. Given an observation set
$\{(\state_\ttime,\action_\ttime ,\reward_\ttime,
\state_{\ttime+1})\}$, we define an explored model $\widetilde{M}$,
where $\tilde{f}(\state_\ttime,\action_\ttime)=\state_{\ttime+1}$
and $\tilde{\reward}(\state,\action)=0$.
%
For $(\state,\action)$ which do not appear in the observation set,
we define $\tilde{f}(\state,\action)=\state$ and
$\tilde{\reward}(\state,\action)=\Rmax$.


We can now present the on-policy exploration algorithm. Initially
set $\widetilde{M}_0$ to have $\tilde{f}(\state,\action)=\state$ and
$\tilde{\reward}(\state,\action)=\Rmax$ for every $(\state,\action)$. Initialize $\ttime=0$. At
time $\ttime$ do the following.
\begin{enumerate}
%\item
%At time $\tHorizon$ let
%$\{(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1}):
%0\leq \ttime\leq \tHorizon-1\}$ be the previous observations.
%\item
%Let $\ttime=0$.
%\item
%Build the explored model $\widetilde{M}_\tHorizon$.
\item
Compute $\tilde{\policy}^*_\ttime$, the optimal policy for
$\widetilde{M}_\ttime$, for the infinite horizon average reward return.
\item
If the return of $\tilde{\policy}^*_\ttime$ on $\widetilde{M}_\ttime$ is
zero, then terminate.
\item
Use $\action_\ttime=\tilde{\policy}^*_\ttime(\state_{\ttime})$.
\item
Observe the reward $\reward_\ttime$ and the next state
$\state_{\ttime+1}$ and add
$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$ to
the observation set.
\item
Modify $\widetilde{M}_\ttime$ to  $\widetilde{M}_{\ttime+1}$
%be identical to
%except (potentially)
by setting for state $\state_\ttime$ and action $\action_\ttime$ the
transition $\tilde{f}(\state_\ttime,\action_\ttime)=\state_{\ttime+1}$
and the reward $\tilde{\reward}(\state_\ttime,\action_\ttime)=0$.
(Note that this will have an effect only the first time we encounter
$(\state_\ttime,\action_\ttime)$.)
\end{enumerate}

We claim that at termination we have observed each state-action pair
at least once. Otherwise, there will be state-action pairs that
would have a reward of $\Rmax$ and at least one of those pairs would
be reachable from the current \texttt{known} states. So the optimal policy
would have a return of $\Rmax$ contradicting the fact that it had
return of zero.

The time to termination can be bounded by  $O(|\States|^2 |\Actions|)$, since we have  $|\States|\; |\Actions|$ state-action pairs and while we did not terminate, we reach an \texttt{unknown} state-action pair after at most  $|\States|$ steps.

After the algorithm terminates, define the following model. Given
the observations during the run of the algorithm
$\{(\state_\ttime,\action_\ttime ,\reward_\ttime,
\state_{\ttime+1})\}$, we define the observed model $\widehat{M}$,
where $\widehat{f}(\state_\ttime,\action_\ttime)=\state_{\ttime+1}$
and $\widehat{\reward}(\state,\action)=\reward_\ttime$. This model
is exactly the true DDP $M$ since it includes all state-action
pairs, and for each it has the correct reward and next state. (We
are using the fact that for a DDP multiple observations of the same
state and action result in identical observations.)
\end{proof}

The above algorithm reconstructs the model completely. We can be
slightly more refined. We can define an {\em optimistic} model, whose
return upper bounds that of the true model. We can then solve for
the optimal policy in the optimistic model, and if it does not reach a new state-action pair (after sufficiently long time) then it
has to be the true optimal policy.

We first define the optimistic observed model. Given an observation
set $\{(\state_\ttime,\action_\ttime ,\reward_\ttime,
\state_{\ttime+1})\}$, we define an optimistic observed model
$\widehat{M}$, where
$\widehat{f}(\state_\ttime,\action_\ttime)=\state_{\ttime+1}$ and
$\widehat{\reward}(\state,\action)=\reward_\ttime$.
%
For $(\state,\action)$ which do not appear in the observation set,
we define $\widehat{f}(\state,\action)=\state$ and
$\widehat{\reward}(\state,\action)=\Rmax$.

First, we claim that for any $\policy\in {\Pi _{SS}}$ the optimistic
observed model $\widehat{M}$ can only increase the value compared to
the true model $M$. Namely,
\[
\widehat{V}^\policy(\state;\widehat{M})\geq V^\policy(\state;M)
\]
The increase holds for any trajectory, and note that once $\policy$
reaches $(\state,\action)$ that was not observed, its reward will be
$\Rmax$ forever. (This is since $\policy\in {\Pi _{SS}}$.)

We can now present the on-policy learning algorithm.
%
Initially set $\widehat{M}_0$ to have for every $(\state,\action)$
the $\widehat{f}(\state,\action)=\state$ and
$\tilde{\reward}(\state,\action)=\Rmax$. Initialize $\ttime=0$. At
time $\ttime$ do the following.
\begin{enumerate}
%\item
%At time $\tHorizon$ let
%$\{(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1}):
%0\leq \ttime\leq \tHorizon-1\}$ be the previous observations.
%\item
%Build the observed model $\widehat{M}_\tHorizon$.
\item
Compute $\widehat{\policy}^*_\ttime$, the optimal policy for
$\widehat{M}_\ttime$ with the infinite horizon average reward.
\item
Use $\action_\ttime=\widehat{\policy}^*_\ttime(\state_{\ttime})$.
\item
Observe the reward $\reward_\ttime$ and the next state
$\state_{\ttime+1}$ and add
$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$ to
the observation set.
\item
Modify $\widetilde{M}_\ttime$ to  $\widetilde{M}_{\ttime+1}$
%be identical to
%except (potentially)
by setting for state $\state_\ttime$ and action $\action_\ttime$ the
transition $\tilde{f}(\state_\ttime,\action_\ttime)=\state_{\ttime+1}$
and the reward
$\tilde{\reward}(\state_\ttime,\action_\ttime)=\reward_\ttime$.
(Again, note that this will have an effect only the first time we
encounter $(\state_\ttime,\action_\ttime)$.)
%
% Modify $\widehat{M}_\ttime$ to
%$\widehat{M}_{\ttime+1}$
%%be identical to
%except (potentially) for $\state_\ttime$ and action $\action_\ttime$
%where $\widehat{f}(\state_\ttime,\action_\ttime)=\state_\ttime$ and
%$\widehat{\reward}(\state_\ttime,\action_\ttime)=\reward_\ttime$
\end{enumerate}





We can now state the convergence of the algorithm to the optimal
policy.

\begin{theorem}
After $\tau\leq|\States|^2|\Actions|$ time steps the policy
$\widehat{\policy}^*_\tau$ never changes and it is optimal for the true model $M$.
\end{theorem}

\begin{proof}
We first claim that the model $\widehat{M}_\ttime$ can change  at
most $|\States|\;|\Actions|$ times (i.e.,  $\widehat{M}_\ttime \neq
\widehat{M}_{\ttime+1}$). Each time we change the observed model
$\widehat{M}_\ttime$, we observe a new $(\state,\action)$ for the
first time. Since there are $|\States|\;|\Actions|$ such pairs, this
bounds the number of changes of  $\widehat{M}_\ttime$.

Next, we show that we either make a change in $\widehat{M}_\ttime$
during the next $|\States|$ steps or we never make any more changes.
The model $M$ is deterministic, if we do not change the policy in
the next $|\States|$ time steps, the policy
$\widehat{\policy}^*_\tau \in {\Pi _{SD}}$ reach a cycle and
continue on this cycle forever. Hence, the model will never change.

We showed that the number of changes is at most
$|\States|\;|\Actions|$, and the time between changes is at most
$|\States|$. This implies that after time $\tau\leq
|\States|^2|\Actions|$ we never change.

The return of $\widehat{\policy}^*_\tau$ after time $\tau$
is identical in $\widehat{M}_\tau$ and $M$, since all the edges
it traverses are \texttt{known}. Therefore, $V^{\widehat{\policy}^*_\tau}(s;M)=V^{\widehat{\policy}^*_\tau}(s;\widehat{M}_\tau)$. Since $\widehat{\policy}^*_\tau$ is the optimal policy in $\widehat{M}_\tau$ we have that $V^{\widehat{\policy}^*_\tau}(s;\widehat{M}_\tau)\geq V^{\policy^*}(s;\widehat{M}_\tau)$, where $\policy^*$ is the optimal policy in $M$. By the optimism we have $ V^{\policy^*}(s;\widehat{M}_\tau)\geq  V^{\policy^*}(s;M) $. We established that $ V^{\widehat{\policy}^*_\tau}(s;M) \geq V^{\policy^*}(s;M) $, but due to the optimality of $\policy^*$ we have $\policy^*=\widehat{\policy}^*_\tau$.
%
% Let $\widehat{V}^*$ be its return. Assume
% that the policy $\policy^*$ has a strictly higher return in $M$,
% i.e., $V^*>\widehat{V}^*$. This implies that the return of
% $\policy^*$ is at least $V^*>\widehat{V}^*$ in
% $\widehat{M}_\tHorizon$, since the rewards in
% $\widehat{M}_\tHorizon$ are always at least those in $M$. This
% contradicts the fact that $\widehat{\policy}^*_\tHorizon$ is optimal
% for $\widehat{M}_\tHorizon$.
\end{proof}

In this section we used the infinite horizon average reward, however
this is not critical. If we are interested in the finite horizon, or
the discounted return, we can use them to define the optimal policy,
and the claims would be almost identical.


%\newpage
\subsection{On-policy learning MDP: Explicit Explore or Exploit ($E^3$)}

We will now extend the techniques we developed for DDP to a general
MDP. We will move from infinite horizon average reward to finite
horizon, mainly for simplicity, however, the techniques presented
can be applied to a variety of return criteria.

The main difference between a DDP and MDP is that in a DDP it is sufficient to have a single state-action sample $(\state,\action)$ to know both the reward and the next state. In a general MDP we need to have a larger number of state-action samples of $(\state,\action)$ to approximate it well. (Recall that to have an $\alpha$-approximate model it is sufficient to have from each state-action pair $m=O(|\States|+\log(\tHorizon |\States|\;|\Actions|/\delta))$ samples.)
%from \texttt{unknown} to \texttt{known}.
Otherwise, the algorithms would be very similar.

We start with the $E^3$ (Explicit Explore or Exploit) algorithm of
\cite{KearnsS02}. The algorithm learns the MDP model by sampling
each state-action pair $m$ times. The main task would be to generate
those $m$ samples. (A technical point would be that some
states-action pairs might have very low probability under any
policy, such state-action pairs would be implicitly ignored.)

As in the DDP we will maintain an explored model. Given an
observation set $\{(\state_\ttime,\action_\ttime ,\reward_\ttime,
\state_{\ttime+1})\}$, we define a state-action $(\state,\action)$
pair \texttt{known} if we have $m$ times $\ttime_i$, $1\leq i \leq m$,
where $\state_{\ttime_i}=\state$ and $\action_{\ttime_i}=\action$,
otherwise it is \texttt{unknown}. We define the observed distribution
of a \texttt{known} $(\state,\action)$ to be
\[
\widehat{\transitionprob}(\state'|\state,\action)= \frac{|\{\ttime_i
:\state_{\ttime_i+1}=\state',\state_{\ttime_i}=\state,\action_{\ttime_i}=\action\}|}{m}
\]
and the observed reward to be,
\[
\widehat{\reward}(\state,\action)=\frac{1}{m}\sum_{i=1}^m
\reward_{\ttime_i}
\]


We define the explored model $\widetilde{M}$ as follows. We add a new
state $\state_1$. For each \texttt{known} $(\state,\action)$, we set the next
state distribution $\tilde{\transitionprob}(\cdot|\state,\action)$ to be the
observed distribution $\widehat{\transitionprob}(\cdot|\state,\action)$, and the
reward to be zero, i.e., $\tilde{\reward}(\state,\action)=0$.
%
For \texttt{unknown} $(\state,\action)$, we define
$\tilde{\transitionprob}(\state'=\state_1 | \state,\action)=1$ and
$\tilde{\reward}(\state,\action)=1$. For state $\state_1$ we
have $\tilde{\transitionprob}(\state'=\state_1 | \state_1,\action)=1$ and
$\tilde{\reward}(\state_1,\action)=0$ for any action
$\action\in\Actions$.
The terminal reward of any state $\state$ is zero, i.e., $\tilde{\reward}_\tHorizon(\state)=0$.
%
Note that the expected value of any policy $\policy$ in $\widetilde{M}$
is {\em exactly} the probability it will reach an \texttt{unknown}
state-action pair.

We can now specify the $E^3$ (Explicit Explore or Exploit)
algorithm. The algorithm has three parameters: (1) $m$, how many
samples we need to change a state-action from \texttt{unknown} to \texttt{known}, (2)
$\tHorizon$, the finite horizon parameter, and (3) $\varepsilon$, the
accuracy parameter.

Initially all state-action pairs are \texttt{unknown} and we set $\widetilde{M}$
accordingly. We initialize $\ttime=0$, and at time $\ttime$ do the
following.
\begin{enumerate}
\item
Compute $\tilde{\policy}^*_\ttime$, the optimal policy for
$\widetilde{M}$, for the finite horizon return with horizon $\tHorizon$.
\item
If the expected return of $\tilde{\policy}^*_\ttime$ on $\widetilde{M}$
is less than $\varepsilon/2$, then terminate.
\item
Run policy $\tilde{\policy}^*_\ttime(\state_{\ttime})$ and observe a trajectory $(\state_0,\action_0,\reward_0,\state_1, \ldots, \state_\tHorizon)$
%Use $\action_\ttime=\tilde{\policy}^*_\ttime(\state_{\ttime})$.
\item
%Observe the reward $\reward_\ttime$ and the next state
%$\state_{\ttime+1}$ and add
Add to the observations the quadruples
$(\state_i,\action_i,\reward_i,\state_{i+1})$
for $0\leq i\leq \tHorizon-1$.
\item
For each $(\state,\action)$ which became \texttt{known} for the first time,
update $\widetilde{M}$ entries for $(\state,\action)$.
\end{enumerate}

At termination we define $M'$ as follows. For each \texttt{known} state-action pair
$(\state,\action)$, we set the next state distribution to be the
observed distribution $\widehat{\transitionprob}(\cdot|\state,\action)$, and the
reward to be the observed reward, i.e.,
$\widehat{\reward}(\state,\action)$.
%
For \texttt{unknown} $(\state,\action)$, we can define the rewards and next
state distribution arbitrarily. For concreteness, we will use the
following: $\widehat{\transitionprob}(\state,\action)=\state$ and
$\widehat{\reward}(\state,\action)=\Rmax$.

\begin{theorem}
Let $m\geq
\frac{|\States|+\log(|\States|\;|\Actions|/\delta)}{\alpha^2}$
and $\alpha=\frac{\varepsilon/4}{\Rmax \tHorizon^2}$.
%
The $E^3$ (Explicit Explore or Exploit) algorithm recovers an MDP
$M'$, such that for any policy $\policy$ the expected return on $M'$
and $M$ differ by at most $\varepsilon(\tHorizon\Rmax+1)$, i.e.,
$$|\Value^\policy_{M'}(\state_0)-\Value^\policy_M(\state_0) |\leq
\varepsilon\tHorizon\Rmax+\varepsilon. $$
 In addition, the expected number of time
steps until termination is at most $O(m\tHorizon
|\States|\;|\Actions|/\varepsilon)$
\end{theorem}

\begin{proof}
We set the sample size $m$ such that with probability $1-\delta$ we
have that for every state $\state$ and action $\action$ we have that
both the observed and true next state distribution are $\alpha$
close and the difference between the observed and true reward is at
most $\alpha$. Namely,
$\|\transitionprob(|\state,\action)-\widehat{\transitionprob}(\cdot|\state,\action)\|_1\leq
\alpha$ and
$|\reward(\state,\action)-\widehat{\reward}(\state,\action)|\leq
\alpha$.
As we saw before, for this it is sufficient to have that $m\geq
\frac{|\States|+\log(|\States|\;|\Actions|/\delta)}{\alpha^2}$.

Let $\widetilde{M}_\ttime$ be the model at time $\ttime$. We define an intermediate model
$\widetilde{M'}_\ttime$ to be the model where we replace the observed
next-state distributions with the true next state distributions for the \texttt{known} state-action pairs.
Since the two models are $\alpha$-approximate, their expected return
differ by at most $\alpha\tHorizon^2\Rmax\leq \varepsilon/4$.

Note that the probability of reaching some \texttt{unknown} state in the true model $M$ and the intermediate model
$\widetilde{M}'_\ttime$ at time $\ttime$ is identical. This is since the
two models agree on the \texttt{known} states, and once an \texttt{unknown} state is
reached, we are done.

We will show that while the probability of reaching some \texttt{unknown} state in the true model is large (larger than $0.75\varepsilon$) we will not terminate. This will guarantee that when we terminate the probability of reaching any \texttt{unknown} state is negligible, and hence we can conceptually ignore such state and still be near optimal. The second part is to show that we do terminate and bound the expected time until termination. For this part we will show that once every policy has a low probability of reaching some \texttt{unknown} state in the true model  (less than $0.25\varepsilon$) then we will terminate. 
%This will imply that, in expectation, we have at most $O(1/\varepsilon)$ times between updates to the model, and overall bound the number of iterations by $O(|\States|\;|\Actions|/\varepsilon)$

Assume there is a policy $\policy$ that at time $\ttime$ in the true
model $M$ has a probability of at least $(3/4)\varepsilon$ to reach
an \texttt{unknown} state. (Note that the set of \texttt{known} and \texttt{unknown} states
change with $\ttime$.) Recall that this implies that $\policy$ has
the same probability in $\widetilde{M}'_\ttime$. Therefore, this policy $\policy$
has a probability of at least $(1/2)\varepsilon$ to reach an \texttt{unknown}
state in $\widetilde{M}_\ttime$ since $\widetilde{M}'_\ttime$ and
$\widetilde{M}_\ttime$ are $\alpha$-approximate. This implies that we will not terminate while there is such a policy $\pi$.

Similarly, once at time $\ttime$, every policy $\policy$ in the true
model $M$ has a probability of at most $(1/4)\varepsilon$ to reach an
unknown state, then we are guaranteed to terminate. This is since the
probability of $\policy$ to reach an \texttt{unknown} state is identical in
$M$ and $\widetilde{M}'_\ttime$. Since the expected return of $\policy$
in $\widetilde{M}'_\ttime$ and $\widetilde{M}$ differ by at most
$\varepsilon/4$, the probability of $\policy$ to reach an \texttt{unknown}
state in $\widetilde{M}_\ttime$ is at most $\varepsilon/2$. This is
exactly our termination condition.
%



Assume termination at time $\ttime$. At time $\ttime$ every policy
$\policy$ has a probability of at most $(1/2)\varepsilon$ to reach
some \texttt{unknown} state in $\widetilde{M}_\ttime$. This implies that
$\policy$ has a probability of at most $(3/4)\varepsilon$ to reach
some \texttt{unknown} state in $M$.


%We claim that at termination the probability of any policy to reach
%an \texttt{unknown} state-action pair is at most $\varepsilon$. This follows
%since the expected value of any policy is the probability it reaches
%an \texttt{unknown} state-action pair. Since we are considering the optimal
%policy, it upper bounds the probability of any other policy. This
%implies that for any reward function, the effect of the \texttt{unknown}
%state-action pairs on the expected value of any policy can be
%bounded by $\varepsilon \tHorizon$.

After the algorithm terminates, we define the model $M'$ using the
observed distributions and rewards for any \texttt{known} state-action pair.
Since every \texttt{known} state-action pair is sampled $m$ times, we have
that with probability $1-\delta$ the model $M'$
is an $\alpha$-approximation of the true model $M$, in the \texttt{known}
state-action pairs.

When we compare
$|\Value^\policy_{M'}(\state_0)-\Value^\policy_M(\state_0)|$ we
separate the difference due to trajectories that include \texttt{unknown} states and due to trajectories in which all the states are \texttt{known} states. 
The contribution of trajectories with  \texttt{unknown} states is at most $\varepsilon\tHorizon\Rmax$,
since the probability of reaching any \texttt{unknown} state is at most
$(3/4)\varepsilon<\varepsilon$ and the maximum return is
$\tHorizon\Rmax$. The difference in trajectories in which all the states are \texttt{known} states is at most
$\varepsilon/4<\varepsilon$ since $M$ and $M'$ are $\alpha$
approximate, and the selection of $\alpha$ guarantees that the
difference in expectation is at most $\varepsilon/4$
(Lemma~\ref{lemma:approx-model-FH}).

In each iteration, until we terminate, we have a probability of at
least $\varepsilon/4$ to reach some \texttt{unknown} state-action. We can reach
unknown state-action pairs at most $m |\States|\;|\Actions|$.
Therefore the expected number of time steps is  $O(m\tHorizon
|\States|\;|\Actions|/\varepsilon)$.
\end{proof}

\subsection{On-policy learning MDP: {\tt R-max}}

In this section we introduce {\tt R-max}. The main difference
between {\tt R-max} and $E^3$ is that {\tt R-max} will have a single
continuous phase, and there will be no need to explicitly switch
from exploration to exploitation.

Similar to the DDP, we will use the principle of {\em Optimism in
face of uncertainty}. Namely, we substitute the \texttt{unknown} quantities
by the maximum possible values.
%For rewards, this means $\Rmax$. For transitions
%
In addition, similar to DDP and $E^3$, we will partition the
state-action pairs $(\state,\action)$ \texttt{known} and \texttt{unknown}.
%
The main difference from DDP, and similar to $E^3$, is that in a DDP
it is sufficient to have a single sample to move $(\state,\action)$
from \texttt{unknown} to \texttt{known}. In a general MDP we need to have a
larger sample to move $(\state,\action)$ from \texttt{unknown} to {\tt
known}. Otherwise, the {\tt R-max} algorithm would be very similar
to the one in DDP. In the following, we describe algorithm {\tt
R-max}, which performs on-policy learning of MDPs.

We can now specify the {\tt R-max} algorithm. The algorithm has two
parameters: (1) $m$, how many samples we need to change a
state-action from \texttt{unknown} to \texttt{known}, and (2) $\tHorizon$, which is
the finite horizon parameter.

{\em Initialization:} Initially, we set for each $(\state,\action)$
a next state distribution which always returns to $\state$, i.e.,
$\transitionprob(\state|\state,\action)=1$ and $\transitionprob(\state'|\state,\action)=0$ for
$\state'\neq \state$. We set the reward to be maximal, i.e.,
$\reward(\state,\action)=\Rmax$. We mark $(\state,\action)$ to be
\texttt{unknown}.

{\em Execution:} At time $\ttime$. (1) Build a model
$\widehat{M}_\ttime$, explained later. (2) Compute
$\widehat{\policy}^*_\ttime$ the optimal finite horizon policy for
$\widehat{M}_\ttime$, where $\tHorizon$ is the horizon, and (3) Execute
$\widehat{\policy}^*_\ttime(\state_\ttime)$ and
observe a trajectory $(\state_0,\action_0,\reward_0,\state_1, \ldots, \state_\tHorizon)$.
%$\reward_\ttime$ and $\state_{\ttime+1}$.

{\em Building a model:} At time $\ttime$, if the number of samples
of $(\state,\action)$ is for the {\em first time} at least $m$,
then: modify $\transitionprob(\cdot|\state,\action)$ to the observed transition
distribution $\widehat{\transitionprob}(\cdot|\state,\action)$, and $\reward(\state,\action)$ to the average observed
reward $\widehat{\reward}(\state,\action)$, and mark $(\state,\action)$ as {\tt
known}. Note that we update each $(\state,\action)$ only once, when
it moves from \texttt{unknown} to \texttt{known}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Given a set of observations, we define the model $\widetilde{M}$ as
% follows. For each \texttt{known} $(\state,\action)$, we set the next state
% distribution $\tilde{\transitionprob}$ to be the observed distribution
% $\widehat{\transitionprob}(\cdot|\state,\action)$, and the reward
% $\tilde{\reward}$ to be the observed distribution, i.e.,
% $\widehat{\reward}(\state,\action)$.
% %
% For \texttt{unknown} $(\state,\action)$, we define
% $\tilde{\transitionprob}(\state,\action)=\state$ and
% $\tilde{\reward}(\state,\action)=\Rmax$. Note that the main
% difference from $E^3$ is the fact that we set the rewards of the
% known state-action pairs to be their observed reward (and not zero,
% as $E^3$ does).



% Initially all state-action pairs are \texttt{unknown} and we set $\widetilde{M}$
% accordingly. We initialize the time $\ttime=0$, and at iteration $\ttime$ do
% the following:
% \begin{enumerate}
% \item
% Let $k$ be the number of steps remaining in the current episode.
% %If $\ttime$ is a start on a new episode, then
% Compute $\tilde{\policy}^*_\ttime$, the optimal policy for
% $\widetilde{M}$, for the finite horizon return with horizon $k$.
% \item
% Use $\action_\ttime=\tilde{\policy}^*_\ttime(\state_{\ttime})$.
% \item
% Observe the reward $\reward_\ttime$ and the next state
% $\state_{\ttime+1}$ and add
% $(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+_1})$
% to the observations.
% \item
% If $(\state_\ttime,\action_\ttime)$ became \texttt{known} for the first time,
% update $\tilde{M}$ entries for $(\state_\ttime,\action_\ttime)$.
% \end{enumerate}

Note that there are two main differences from $E^3$. First, when a
state-action becomes \texttt{known}, we set the reward to be the observed
reward (and not zero, as in $E^3$). Second, there is no test for
termination, but we continuously run the algorithm (although at
some point the policy will stop changing).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here is the basic intuition for algorithm {\tt R-max}. We consider
the finite horizon return with horizon $\tHorizon$.  In each episode
we run $\widehat{\policy}^*_\ttime$ for $\tHorizon$ time steps.
Either, with some non-negligible probability we explore a
state-action $(\state,\action)$ which is \texttt{unknown}, in this case
we make progress on the exploration.  This can happen at most
$m|\States|\;|\Actions|$ times. Alternatively, with high probability
we do not reach any state-action $(\state,\action)$ which is {\tt
unknown}, in which case we are optimal on the observed model, and
near optimal on the true model.

For the analysis define an event $NEW_\ttime$, which is the event that we
visit some \texttt{unknown} state-action $(\state,\action)$ during the iteration $\ttime$.
\begin{claim}
For the return of $\widehat{\policy}^*_\ttime$, we have,
\[
V^{\widehat{\policy}^*_\ttime}(\state_0)\geq V^*(\state_0) -
\Pr[NEW_\ttime]\tHorizon-\lambda
\]
where $\lambda$ is the approximation error for any two models which are $\alpha$-approximate.
%, i.e., $\lambda\geq |V^{\widehat{\policy}}(\state_0;M)-V^{\widehat{\policy}}(\state_0;\widehat{M}_t)$, for any policy $\policy$.
\end{claim}

\begin{proof}
Let $\policy^*$ be the optimal policy in the true model $M$. Since we selected policy $\widehat{\policy}^*_\ttime$ for our model $\widehat{M}_t$, we have $V^{\widehat{\policy}^*_\ttime}(\state_0;\widehat{M}_t)\geq V^{\policy^*}(\state_0;\widehat{M}_t)$. 

We now define an intermediate model $\widehat{M'}_t$ which replaces the transitions and rewards in the \texttt{known} state-action pairs by the true transition probabilities and rewards. We have that $\widehat{M'}_t$ and $\widehat{M}_t$ are $\alpha$-approximate.
By the definition of $\lambda$ we have $V^{\policy^*}(\state_0;\widehat{M}_t)\geq V^{\policy^*}(\state_0;\widehat{M'}_t)-\lambda$. In addition, $V^{\policy^*}(\state_0;\widehat{M'}_t)\geq V^{\policy^*}(\state_0;M)=V^*(\state_0)$, since in $\widehat{M'}_t$ we only increased the rewards of the \texttt{unknown} state-action pairs such that when we reach them we are guarantee maximal rewards until the end of the trajectory.

For our policy $\widehat{\policy}^*_\ttime$ we have that 
 $V^{\widehat{\policy}^*_\ttime}(\state_0;\widehat{M'}_t)+\lambda\geq V^{\widehat{\policy}^*_\ttime}(\state_0;\widehat{M}_t)$, since the models are $\alpha$-approximate.
%
In $M$ and $\widehat{M'}_t$, any trajectory that does not reach any \texttt{unknown} state-action pair, has the same probability in both models. This implies that 
  $V^{\widehat{\policy}^*_\ttime}(\state_0;M)\geq V^{\widehat{\policy}^*_\ttime}(\state_0;\widehat{M'}_t)-\Pr[NEW_\ttime]\tHorizon$, since the maximum return is $\tHorizon$. Combining all the inequalities derives the claim.
 %In addition, we can bound trivially the difference in return between any trajectoory 
%=V^*(\state_0)-\lambda$.
\end{proof}


%The reason is that for trajectories which do not reach an \texttt{unknown} state-action pair, we have (approximately) the same 

We set the sample size $m$ such that $\lambda \leq\varepsilon/2$.
%and we can bound it by
%$\varepsilon/2$ by setting $m$ large enough.

We consider two cases, depending on the probability of $NEW_\ttime$. First,
we consider the case that the probability of $NEW_\ttime$ is small. If
$\Pr[NEW]\leq \frac{\varepsilon}{2\tHorizon}$, then
$V^{\widehat{\policy}^*_\ttime}(\state_0)\geq V^*(\state_0)
-\varepsilon/2 -\varepsilon/2$, since we assume that $\lambda\leq
\varepsilon/2$.

Second, we consider the case that the probability of $NEW_\ttime$ is large.
If $\Pr[NEW_\ttime] > \frac{\varepsilon}{2\tHorizon}$. Then, there is a good
probability to visit an \texttt{unknown} state-action pair $(\state,\action)$, but this
can happen at most $m|\States|\;|\Actions|$. Therefore, the expected
number of such iterations is at most
$m|\States|\;|\Actions|\frac{2\tHorizon}{\varepsilon}$.
This implies the following theorem.


% \begin{lemma}
% Consider a sequence of Bernoulli trials $x_1,x_2,\dots$, where $x_i\sim Bern(p)$. Let $k$ denote the first index for which $n$ successes were observed. Then $\mathbb{E}[k] = n/p$, and $P(k - n/p > \varepsilon) \leq ...$.
% \end{lemma}
% \begin{proof}
% Let $y_1$ denote the number of trials until the first success, $y_2$ denote the number of additional trials until the second success, and so on. We have that the $y_i$'s are i.i.d. Geometric random variables, and that $k = \sum_{i=1}^n y_i$.
% From the linearity of expectation, we have that $\mathbb{E}[k] = \sum_{i=1}^n \mathbb{E}[y_i] = n/p$.\\
% Now, 
% \end{proof}
\begin{theorem}
With probability $1-\delta$ algorithm \texttt{R-MAX} will not be $\varepsilon$-optimal, i.e., have an expected return less than $V^*-\varepsilon$, in at most 
\[
m|\States|\;|\Actions|\frac{2\tHorizon}{\varepsilon}
\]
Iterations.
\end{theorem}
% This implies that only in
% \[
% m|\States|\;|\Actions|\frac{2\Vmax}{\varepsilon}
% \]
% iterations, the algorithm \texttt{R-MAX} will not be $\varepsilon$-optimal, i.e., have an expected return less than $V^*-\varepsilon$.

\noindent\textit{Remark:} Note that we do not guarantee a termination after which we can fix the policy. The main technical issue is that the probability of the event $NEW_\ttime$ is non-increasing. This is because when we switch policies, we might considerably increase the probability of reaching \texttt{unknown} state-action pairs. For this reason, we settle for a weaker guarantee that the number of sub-optimal iterations is bounded. Note that in $E^3$, we separated the exploration and exploitation and have a clear transition between the two; therefore, we can terminate and output a near-optimal policy.


\section{Bibliography Remarks}


The first polynomial-time model based learning algorithm is $E^3$
(Explicit Explore or Exploit) of \cite{KearnsS02}. 
%While we did not outline the $E^3$ algorithm, we did describe the effective horizon and the simulation lemma from there.

The improved sampling bounds for the optimal policy using
approximate Value Iteration is following \cite{KearnsS98a}.

The {\tt R-MAX} algorithm is due to \cite{BrafmanT02}.

Analysis of related models, especially the {\tt PAC-MDP} model
appears in \cite{StrehlLL09,Li2012}.
