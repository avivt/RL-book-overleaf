

% \chapter{Stochastic Bandits and Regret Minimization}
% \label{chapter-MAB}

We consider a simplified model of an MDP where there is only a
single state and a fixed set $\Actions$ of $k$ actions (a.k.a., arms). We consider a finite horizon problem, where the horizon is $\tHorizon$.
Clearly, the planning problem is trivial, simply select the action
with the highest expected reward. We will concentrate on the
learning perspective, where the expected reward of each action is
unknown. In the learning setting we would have a \emph{single} episode of length $\tHorizon$.

At each round $1\leq \ttime\leq \tHorizon$ the learner selects and executes an action. After executing the action, the leaner observes the reward of the action. However, the rewards of the other actions in $\Actions$ are not revealed to the learner.

The reward for action $i$ at round $\ttime$ is denoted by $\reward_{\ttime}(i)\sim D_{i}$, where the support of the reward distribution $D_{i}$ is $[0,1]$. We assume that the rewards are i.i.d. (independent and identically distributed) across time steps, but can be correlated across actions in a single time step.

\paragraph{Motivation}
\begin{enumerate}
\item \textbf{News}: a user visits a news site and is presented with a news header. The user either clicks on this header or not. The goal of
the website is to maximize the number of clicks. So each possible
header is an action in a bandit problem, and the clicks are the
rewards
\item \textbf{Medical Trials}: Each patient in the trial is prescribed one treatment out of several possible treatments. Each treatment is an
action, and the reward for each patient is the effectiveness of the
prescribed treatment.
\item \textbf{Ad selection}: In website advertising, a user visits a webpage, and a learning algorithm selects one of many possible ads to
display. If an advertisement is displayed, the website observes
whether the user clicks on the ad, in which case the advertiser pays
some amount $v_{a} \in[0, 1]$. So each advertisement is an action,
and the paid amount is the reward.
\end{enumerate}

\paragraph{Model}
\begin{itemize}
\item A set of actions $\Actions=\left\{ \action_{1}\dots,\action_{k}\right\} $. For simplicity we identify action $\action_i$ with the integer $i$.
\item Each action $\action_{i}$ has a reward distribution $D_{i}$ over
$[0,1]$.
\item The expectation of distribution $D_{i}$ is:
\[
\mu_{i}=\E_{X\sim D_{i}}\left[X\right]
\]
\item $\mu^{*}= \max_{i}\mu_{i}$ and $\action^*=\arg\max_i\mu_i$.
\item $\action_{\ttime}$ is the action the learner chose at round $\ttime$.
\item The leaner observes either full feedback, the reward for each possible action, or bandit feedback, only the reward $\reward_\ttime$ of the selected action $\action_\ttime$. For most of the chapter we will consider the bandit setting.
\end{itemize}

We need to define the objective of the learner. The simple objective is to maximize the cumulative reward during the entire episode, namely $\sum_{\ttime=1}^\tHorizon \reward_\ttime$. We will measure the performance by comparing the learner's cumulative reward to the optimal cumulative reward. The difference would be called the \emph{regret}. Our goal would be that the average regret would be vanishing and $\tHorizon$ goes to infinity.
Formally we define the regret as follows.
\[
\text{Regret}=\max_{i\in \Actions}{\displaystyle
\sum_{\ttime=1}^{\tHorizon}\underbrace{\reward_{\ttime}(i)}_{\text{Random variable}}}-{\displaystyle
\sum_{\ttime=1}^{\tHorizon}\underbrace{\reward_{\ttime}(\action_{\ttime})}_{\text{Random variable}}}
\]
The regret as define above is a random variable and we can consider the expected regret, i.e., $\E[\text{Regret}]$. This regret is a somewhat unachievable objective, since even if the learner would have known the complete model, and would have selected the optimal action in each time, it would still have a regret. This would follow from the difference between the expectation and the realizations of the rewards.
For this reason we would concentrate on the Pseudo Regret, which compares the learner's expected cumulative reward to the maximum expected cumulative reward. 
\[
\begin{array}{ccc}
\text{Pseudo Regret} & = & {\displaystyle \max_{i}}\E\left[{\displaystyle \sum_{\ttime=1}^{\tHorizon}\reward_{\ttime}(i)}\right]-\E\left[{\displaystyle \sum_{\ttime=1}^{\tHorizon}\reward_{\ttime}(\action_{\ttime})}\right]\\
\\
 & = & \mu^{*}\cdot \tHorizon-{\displaystyle \sum_{\ttime=1}^{\tHorizon}\mu_{\action_{\ttime}}}
\end{array}
\]
Note that the difference between the regret and the Pseudo Regret is related to the difference between taking the expected maximum (in Regret) versus the maximum expectation (Pseudo Regret). In this chapter we will only consider pseudo regret (and sometime call it simply regret). 

% \ymignore{
% \begin{leftbar}
% \section{Sub-Gaussian Random Variable}

% A random variable $X$ is called $\sigma^{2}-sub-gaussian$ if for any
% $\lambda\in\mathbb{R}$:

% \[
% E\left[e^{\lambda X}\right]\le e^{\sigma^{2}\lambda^{2}/2}
% \]

% \paragraph{Examples}
% \begin{enumerate}
% \item $X\sim N\left(0,\sigma^{2}\right)$, $E\left[e^{\lambda X}\right]=e^{\sigma^{2}\lambda^{2}/2}$
% $\Rightarrow$ X is $\sigma^{2}-sub-gaussian$
% \item X s.t $\begin{cases}
% E\left[X\right]=0\\
% \left|X\right|\le B
% \end{cases}$, then X is $B^{2}-sub-gaussian$
% \end{enumerate}

% \subsection{Properties of $\sigma^{2}-sub-gaussian$ random variable $X$}
% \begin{itemize}
% \item $E\left[X\right]=0$, $Var\left(X\right)\le\sigma^{2}$
% \item $c\cdot X$ is $c^{2}\sigma^{2}-sub-gaussian$
% \item If $X_{1}\dots,X_{m}$ are $\sigma^{2}-sub-gaussian$ then $S={\displaystyle \sum_{i=1}^{m}X_{i}}$
% is $m\sigma^{2}-sub-gaussian$\\
%  $\frac{1}{m}S=\frac{1}{m}{\displaystyle \sum_{i=1}^{m}X_{i}}$ is
% $\frac{\sigma^{2}}{m}-sub-gaussian$
% \end{itemize}

% \begin{theorem}
% \label{thm:sub-gaus}
% Let X be $\sigma^{2}-sub-gaussian$ random
% variable, then
% $Pr[X\geqslant\epsilon]\le\exp(-\frac{\epsilon^{2}}{2\sigma^{2}})$.
% \end{theorem}

% \begin{proof}
% \[
% \begin{array}{ccc}
% Pr[X\geqslant\epsilon] & = & Pr[e^{\lambda X}\geqslant e^{\lambda\epsilon}]\\
% \\
%  & \le & \frac{E[e^{\lambda X}]}{e^{\lambda\epsilon}}\\
% \\
%  & \le & \exp(\sigma^{2}\lambda^{2}/2-\lambda\epsilon)
% \end{array}
% \]
% where we used Markov's inequality for the first stage and the fact
% that the random variable is $\sigma^{2}-sub-gaussian$ for the
% second.

% If we choose $\lambda=\frac{\epsilon^{2}}{\lambda^{2}}$, then we
% get:
% \[
% \begin{array}{ccc}
% Pr[X\geqslant\epsilon] & \le & \exp(-\frac{\epsilon^{2}}{2\sigma^{2}})\end{array}
% \]
% \end{proof}
% \end{leftbar}

% \subsection{Hoeffding's inequality}
% }

We will use extensively the following concentration bound.

\begin{theorem}[Hoeffding's inequality]
\label{thm:hoeffding}
%
Given $X_{1},\dots,X_{m}$ i.i.d random
variables s.t $X_{i}\in[0,1]$ and $\E[X_{i}]=\mu$ we have
\[
Pr[\frac{1}{m} \sum_{i=1}^{m}X_{i}-\mu \ge\epsilon] \le  \exp(-2\epsilon^{2}m)
%Pr[\underbrace{\frac{1}{m}{\displaystyle \sum_{i=1}^{m}X_{i}-\mu}}_{\frac{1}{m}S}\ge\epsilon] & \le & \exp(-\frac{\epsilon^{2}m}{2})\end{array}
\]
or alternatively, for $m\geq \frac{1}{2\epsilon^2}\log(1/\delta)$, with probability $1-\delta$ we have that $\frac{1}{m} \sum_{i=1}^{m}X_{i}-\mu \le\epsilon$.
\end{theorem}

% \ymignore{
% \begin{leftbar}
% This is a direct conclusion from Theorem~\ref{thm:sub-gaus}. Let
% $\bar{X_{i}}=X_{i}-\mu$. Using the facts that $E[\bar{X_{i}}]=0$
% and:
% \[
% \begin{array}{ccc}
% X_{i}\in[0,1] & \Rightarrow & |\bar{X_{i}|}\le1\\
% \\
%  & \Rightarrow & X_{i}\text{ is 1-sub-gaussian}
% \end{array}
% \]
% \end{leftbar}
% }

\subsection{Warmup: Full information two actions }

We start with a simple case where there are two actions and we
observe the reward of both actions at each time $\ttime$. We will analyze
the greedy policy, which selects the action with the higher average
reward (so far).

The greedy policy at time $\ttime$ does the following:
\begin{itemize}
\item We observe $\big\langle \reward_{\ttime}(1),\reward_{\ttime}(2)\big\rangle$
\item Define
\[
avg_{\ttime}(i)=\frac{1}{\ttime} \sum_{\tau=1}^{\ttime}\reward_{\tau}(i)
\]
\item In time $\ttime+1$ we choose:
\[
a_{\ttime+1}=\arg\max_{i\in\{1,2\}}avg_{\ttime}(i)
\]
\end{itemize}


We now would like to compute the expected regret of the greedy
policy. W.l.o.g., we assume that $\mu_{1}\ge\mu_{2}$, and define
$\Delta=\mu_{1}-\mu_{2}\ge0$.
\[
\text{Pseudo Regret}= \sum_{\ttime=1}^{\infty}(\mu_{1}-\mu_{2})
\Pr\left[avg_{\ttime}(2)\ge avg_{\ttime}(1)\right]
\]
Note that the above is an equivalent formulation of the pseudo regret. In each time step that greedy selects the optimal action, clearly the difference is zero, so we can ignore those time steps. In time steps which greedy selects the alternative action, action $2$, it has a regret of $\mu_1-\mu_2$ compared to action $1$. This is why we sum over all time steps, the probability that we select action $2$ time the regret in that case, i.e., $\mu_1-\mu_2$. Since we select action $2$ at time $\ttime$ when $avg_{\ttime}(2)\ge avg_{\ttime}(1)$, the probability that we select action $2$ is  exactly the probability that $avg_{\ttime}(2)\ge avg_{\ttime}(1)$.

We would like now to upper bound the probability of $avg_{\ttime}(2)\ge avg_{\ttime}(1)$. 
Clearly, at any time $\ttime$,
\[
E[avg_{\ttime}(2)-avg_{\ttime}(1)]=\mu_{2}-\mu_{1}=-\Delta
\]
We can define a random variable $X_{\ttime}=\reward_{\ttime}(2)-\reward_{\ttime}(1)+\Delta$ and
$\E[X_{\ttime}]=0$. Since $(1/\ttime)\sum_{\ttime} X_{\ttime} = avg_{\ttime}(2)- avg_{\ttime}(1)+\Delta$, by
Theorem~\ref{thm:hoeffding}
\[
Pr[avg_\ttime(2)\geq avg_\ttime(1)]= Pr\left[avg_{\ttime}(2)-avg_{\ttime}(1)+\Delta
\ge\Delta\right]\le e^{-2\Delta^{2}\ttime}
\]
We can now bound the pseudo regret as follows,

\begin{align*}
E\left[\text{Pseudo Regret}\right] & = 
 \sum_{\ttime=1}^{\infty}\Delta\Pr\left[avg_\ttime(2)\geq avg_\ttime(1)\right]\\
 & \le   \sum_{\ttime=1}^{\infty}\Delta e^{-2\Delta^{2}\ttime}\\
 & \le  \int_{0}^{\infty}\Delta e^{-2\Delta^{2}\ttime}dt\\
 & =  \left[-\frac{1}{2\Delta}e^{-2\Delta^{2}\ttime}\right]_{0}^{\infty}\\
 & =  \frac{1}{2\Delta}
\end{align*}
We have established the following theorem.
\begin{theorem}
In the full information two actions multi-arm bandit model, the greedy algorithm guarantees a pseudo regret of at most $1/2\Delta$, where $\Delta=|\mu_1-\mu_2|$.
\end{theorem}

Notice that this regret bound does not depend on  the horizon $\tHorizon$!

\subsection{Stochastic Multi-Arm Bandits: lower bound}

%\subsection{Bandits}

We will now see that we cannot get a regret that does not depend on $\tHorizon$ for the bandit feedback, when we observe only the reward of the action we selected.

Considering the following example. For action $\action_1$ we have the following distribution,
\[
\action_{1}\sim Br\left(\frac{1}{2}\right)\]

For action $\action_2$ there are two alternative equally likely  distributions, each with probability $1/2$,
\[
\action_{2}\sim Br\left(\frac{1}{4}\right) \left(w.p. \frac{1}{2}\right)
\qquad or \qquad \action_{2}\sim Br\left(\frac{3}{4}\right) \left(w.p.
\frac{1}{2}\right)
\]

In this setting, since the distribution of action $\action_1$ is known, the optimal policy will select action $\action_2$ for some time $M$ (potentially, $M=\tHorizon$ is also possible) and then switches to action $\action_1$. The reason is that once we switch to action $\action_1$ we will not receive any new information regarding the optimal action, since the distribution of action $\action_1$ is known. 

Let $S_i=\{\ttime:\action_\ttime=i\}$ be the set of times where we played action $i$. 
Assume by way of contradiction
\[
\E\left[ \sum_{i \in \{1,2\}} \Delta_i |S_i|\right] = \E\left[Pseudo Regret\right]=R
\]

where $R$ does not depend on $\tHorizon$. \\ By Markov inequality:
\[
Pr\left[Pseudo Regret\ge2R\right]\le\frac{1}{2}
\]

Since $\mu_1$ is known, an optimal algorithm will first check $\action_2$
in order to decide which action is better and stick with it.

Assuming $\mu_2 = \frac{1}{4}$, and the algorithm decided to stop
playing $\action_2$ after $M$ rounds, Then:
\[
Pseudo Regret = \frac{1}{4}M
\]
Thus,
\[
Pr\left[Pseudo Regret\ge 2R \right] = Pr\left[ M\ge 8R
\right]\le\frac{1}{2}
\]
And,
\[
Pr\left[M < 8R \right]>\frac{1}{2}
\]
Hence, the probability that after $8R$ rounds, the algorithm will
stop playing $\action_2$ (if $\mu_2 = \frac{1}{4}$) is at least
$\frac{1}{2}$. This implies that there is some sequence of $8R$
outcomes which will result in stopping to try action $\action_2$. For simplicity, assume that the sequence is the all zero sequence. (It is sufficient to note that any sequence of length $8R$ has probability at least $4^{-8R}$.)
\textcolor{red}{Do we need to add that the algorithm is deterministic?}

Assume $\mu_2 = \frac{3}{4}$, but all $8R$ first rounds, playing
$\action_2$ yield the value zero (which happens with probability
$\left(\frac{1}{4}\right)^{8R}$). We assumed that after $8R$ zeros
for action $\action_2$ the algorithm will stop playing $\action_2$, even though
it is the preferred action. In this case, we will get:
\[
Pseudo Regret = \frac{1}{4} (\tHorizon - M) \approx \frac{1}{4}\tHorizon
\]

The expected Pseudo Regret is,
\[
\E\left[Pseudo Regret\right] = R \geq
\underbrace{\frac{1}{2}}_{\action_{2}\sim Pr(Br\left(\frac{3}{4}\right))}
\cdot \underbrace{\left(\frac{1}{4}\right)^{8R}}_{Pr(\forall \ttime\leq 8R \; \reward_\ttime= 0 |
\action_{2}\sim Pr(Br\left(\frac{3}{4}\right))} \cdot (\tHorizon - 8R) \approx
e^{-O(R)}\tHorizon
\]

Which implies that:
\[
R=\Omega\left(\log \tHorizon\right)
\]
Contrary to the assumption that $R$ does not depend on $\tHorizon$.
%\footnote{More formally, after $8R$ steps, there exists some
%sequence of outcomes that cause the algorithm to switch to action
%$\action_2$. The probability of that sequence is at least
%$\left(\frac{1}{4}\right)^{8R}$.}

\topic{Explore-Then-Exploit}{random}

We will now develop an algorithm with a vanishing average regret. The algorithm will have two phases. In the first phase it will explore each action for $M$ times. In the second phase it will exploit the information from the exploration, and will always play on the action with the highest average reward in the first phase.

\begin{enumerate}
%
\item We choose a parameter $M$.
% time frame $kM$ to explore.
%
For $M$ phases we choose each action once (for a total of $kM$
rounds of exploration).
%
\item After $kM$rounds we always choose the action that had highest
average reward during the explore phase.
\end{enumerate}

Define:
\begin{align*}
S_{j}&= \left\{ t:\action_{\ttime}=j,t\le k\cdot M\right\}\\
\hat{\mu}_{j}&=\frac{1}{M} \sum_{t\in S_{j}}\reward_{j}(\ttime)\\
\mu_{j}&=\E[\reward_{j}(\ttime)]\\
\Delta_{j}&=\mu^{*}-\mu_{j}
\end{align*}
where $\Delta_j$ is the difference in expected reward of action $j$
and the optimal action.

We can now write the regret as a function of those parameters:
\[
\E\left[\text{Pseudo regret}\right]=\underbrace{{\displaystyle
\sum_{j=1}^{k}\Delta_{j}\cdot
M}}_{Explore}+\underbrace{\left(\tHorizon-k\cdot M\right){\displaystyle
\sum_{j=1}^{k}\Delta_{j}Pr\left[j=\arg\max_{i}\hat{\mu}_{i}\right]}}_{Exploit}
\]

For the analysis define:
\[
\lambda=\sqrt{\frac{2\log \tHorizon}{M}}
\]

By Theorem~\ref{thm:hoeffding} we have
\[
\Pr\left[\left|\hat{\mu}_{j}-\mu_{j}\right|\ge\lambda\right]  \le
2e^{-2\lambda^2 M}=\frac{2}{\tHorizon^{4}}
\]
which implies (using the union bound) that
\[
\Pr\underbrace{\left[\exists_{j}:\left|\hat{\mu}_{j}-\mu_{j}\right|\ge\lambda\right]}_{B}
 \le  \frac{2k}{\tHorizon^{4}}\underset{for\,k\leq T}{\leq}\frac{2}{\tHorizon^{3}}
\]

Define the ``bad event''
$B=\{\exists_{j}:\left|\hat{\mu}_{j}-\mu_{j}\right|\ge\lambda\}$. If
$B$ did not happen then for each action $j$, such that
$\hat{\mu}_{j}\ge\hat{\mu}^{*}$, we have
\[
\mu_{j}+\lambda\ge\hat{\mu}_{j}\ge\hat{\mu}^{*}\ge\mu^{*}-\lambda
\]
therefore:
\[
2\lambda\ge\mu^{*}-\mu_{j}=\Delta_{j}
\]
and therefore:
\[
\Delta_{j}\le2\lambda
\]

Then, we can bound the expected regret as follows:
\begin{align*}
\E[PseudoRegret] & \leq  \underbrace{\left({\displaystyle
\sum_{j=1}^{k}\Delta_{j}}\right)M}_{Explore}+\underbrace{\left(\tHorizon-k\cdot
M\right)\cdot2\lambda}_{\text{B didn't
happen}}+\underbrace{\frac{2}{\tHorizon^{3}}\cdot \tHorizon}_{\text{B happened}}\\
 & \leq  k\cdot M+2\cdot\sqrt{\frac{2\log \tHorizon}{M}}\cdot \tHorizon+\frac{2}{\tHorizon^{2}}
\end{align*}

If we optimize the number of exploration phases $M$ and choose $M=\tHorizon^{\frac{2}{3}}$,
we get:
\[
\E[PseudoRegret]  \leq 
k\cdot \tHorizon^{\frac{2}{3}}+2\cdot\sqrt{2\log \tHorizon}\cdot
\tHorizon^{\frac{2}{3}}+\frac{2}{\tHorizon^{2}}
\]
which is sub-linear but more than the $O(\sqrt{\tHorizon})$ rate we would
expect.

\section{Improved Regret Minimization Algorithms}

We will look at some more advanced algorithms that mix the
exploration and exploitation.

Define:

$n_{\ttime}(i)$ - the number of times we chose action $i$ by round $\ttime$

$\hat{\mu}_{\ttime}(i)$ - the average reward of action $i$ so far, that
is:
\[
\hat{\mu}_{\ttime}(i)={\displaystyle
\sum_{\ttime=1}^{\tHorizon}\reward_{i}(\ttime)}\mathbb{I}\left(\action_{\ttime}=i\right)\frac{1}{n_{i}(\ttime)}
\]
Notice that $n_{i}(\ttime)$ is a random variable and not a number!

We would like to get the following result:
\[
\Pr\left[\left|\hat{\mu}_{\ttime}(i)-\mu_{i}\right|\le\underbrace{\sqrt{\frac{2\log
T}{n_{i}(\ttime)}}}_{\lambda_{\ttime}(i)}\right]\ge1-\frac{2}{\tHorizon^{4}}
\]

We would like to look at the $m^{th}$ time we sampled action $i$:
\[
\hat{\mathbb{V}}_{m}(i)=\frac{1}{m} \sum_{\tau=1}^{m}\reward_{i}(\ttime_{\tau})
\]
Where the $t_{\tau}$'s are the rounds when we chose action $i$

Now we fix $m$ and get:
\[
\forall{i}\forall{m}\;\;\;
\Pr\left[\left|\hat{\mathbb{V}}_{m}(i)-\mu_{i}\right|\le\sqrt{\frac{2\log
\tHorizon}{m}}\right]\ge1-\frac{2}{\tHorizon^{4}}
\]
and notice that $\hat{\mu}_{\ttime}(i)\equiv\hat{\mathbb{V}}_{i}(m)$ when
$m=n_{i}(\ttime)$.

Define the ``good event'' $G$:
\[
G=\left\{ \forall_{i}\forall_{\ttime}\left|\hat{\mu}_{i}(\ttime)-\mu_{i}\right|\le\lambda_{i}(\ttime)\right\}
\]
The probability of $G$ is,
\[
Pr\left(G\right)\ge1-\frac{2}{\tHorizon^{2}}
\]

\section{Refine Confidence Bound}

Define the upper confidence bound:
\[
UCB_{\ttime}(i)=\hat{\mu}_{\ttime}(i)+\lambda_{\ttime}(i)
\]
and similarly, the lower confidence bound:
\[
LCB_{\ttime}(i)=\hat{\mu}_{\ttime}(i)-\lambda_{\ttime}(i)
\]
if $G$ happened then:
\[
\forall{i}\forall{\ttime}\;\;\;\mu_{i}\in\left[LCB_{\ttime}(i),UCB_{\ttime}(i)\right]
\]
Therefore:
\[
Pr\biggl[\forall{i}\forall{\ttime}\;\;\;
\mu_{i}\in\left[LCB_{\ttime}(i),UCB_{\ttime}(i)\right]\biggr]\ge1-\frac{2}{\tHorizon^{2}}
\]

\subsection{Successive Action Elimination}

We maintain a set of actions S.

Initially $S=\Actions$

In each phase:
\begin{itemize}
\item We try every $i\in S$ once
\item For each $j\in S$ if there exists $i\in S$ such that:
\[
UCB_{\ttime}(j)<LCB_{\ttime}(i)
\]
 We remove $j$ from $S,$ that is we update:
\[
S\leftarrow S-\left\{ j\right\}
\]
\end{itemize}
We will get the following results:
\begin{itemize}
\item As long as action $i$ is still in $S$, we have tried action $i$ exactly the
same number of times as all of any other action $j\in S$.
\item The best action, under the assumption that the event $G$ holds, is never eliminated
from $S$.
\end{itemize}
To see that the best action is never eliminated, under the good event $G$, note the following for time $\ttime$. For the best action we have $\mu^*< UCB_{\ttime}(\action^*)$, and for any action $i$ we have $LCB_\ttime(i)\leq \mu_i$. Since $LCB_\ttime(i)\leq \mu_i\leq \mu^*\leq UCB_\ttime(\action^*)$, the best action $\action^*$ is never eliminated.

Under the assumption of $G$ we get:
\[
\mu^{*}-2\lambda\le\hat{\mu}^{*}-\lambda=LCB_\ttime(\action^*)<UCB_\ttime(i)=\hat{\mu_{i}}+\lambda\leq\mu_{i}+2\lambda
\]
Where $\lambda=\lambda_{i}=\lambda^{*}$ because we have chosen action $i$ and the best action the same number of times so far.

Therefore, assuming event $G$ holds,
\[
\Delta_{i}=\mu^{*}-\mu_{i}\le4\lambda=4\sqrt{\frac{2\log
T}{n_{\ttime}(i)}}
\]
\[
\Rightarrow\;\;\; n_{\ttime}(i)\le\frac{32}{\Delta_{i}^{2}}\log \tHorizon
\]
for any time $\ttime$ where action $\action_i$ is played, and therefore it bounds the total number of times action $\action_i$ is played.

This implies that we can bound the pseudo regret as follows,
\begin{align*}
\E\left[\text{Pseudo Regret}\right]  = &  \sum_{i=1}^{k}\Delta_{i}n_{i}(\ttime)\\
  \le &  \sum_{i=1}^{k}\frac{32}{\Delta_{i}}\log \tHorizon
  +\underbrace{\frac{2}{\tHorizon^{2}}\cdot T}_{\text{The bad event}}
\end{align*}

%meaning that the expected pseudo regret is bounded by $O\left(\frac{1}{\tHorizon}\right)$.

\begin{theorem}
\label{thm:MAB:SE1}
The pseudo regret of successive action elimination is bounded by $O(\frac{1}{\Delta_{i}}\log \tHorizon)$
\end{theorem}

Note that the bound is  when $\Delta_i\approx 0$. This is not a really issue, since such actions also have very small regret when we use them. Formally, we can partition the action according to $\Delta_i$. Let $\Actions_1=\{i:\Delta_i< \sqrt{k/\tHorizon}\}$ be the set of actions with low $\Delta_i$, and $\Actions_2=\{i:\Delta_i\geq \sqrt{k/\tHorizon}\}$. We can now re-analyze the pseudo regret, as follows,
\begin{align*}
\E\left[\text{Pseudo Regret}\right]  = &  \sum_{i=1}^{k}\Delta_{i}n_{i}(\ttime)\\
= &  \sum_{i\in \Actions_1}^{k}\Delta_{i}n_{i}(\ttime)+ \sum_{i\in \Actions_2}^{k}\Delta_{i}n_{i}(\ttime)\\
  \le & \sum_{i\in\Actions_1}\sqrt{\frac{k}{\tHorizon}}n_{i}(\ttime) +\sum_{i\in\Actions_2}\frac{32}{\Delta_{i}}\log \tHorizon
  +\underbrace{\frac{2}{\tHorizon^{2}}\cdot T}_{\text{The bad event}}\\
  \leq & \sqrt{\tHorizon}+ 32 k \sqrt{\frac{\tHorizon}{k}}\log\tHorizon+ \frac{2}{\tHorizon}\\
  \leq & 34  \sqrt{k\tHorizon}\log\tHorizon
\end{align*}

We have established the following regret bound

\begin{theorem}
\label{thm:MAB:SE2}
The pseudo regret of successive action elimination is bounded by $O( \sqrt{k\tHorizon}\log\tHorizon)$
\end{theorem}


\subsection{Upper confidence bound (UCB)}

The UCB algorithm simply uses the UCB bound. The algorithm works as
follows:
\begin{itemize}
\item We try each action once (for a total of $k$ rounds)
\item Afterwards we choose:
\end{itemize}
\[
a_{\ttime}=\arg\max_{i}UCB_{\ttime}(i)
\]

If we chose action $i$ then, assuming $G$ holds, we have
\[
UCB_{\ttime}(i)  \ge  UCB_{\ttime}(a^*)\geq\mu^{*}
\]
where $a^*$ is the optimal action.

Using the definition of UCB and the assumption that $G$ holds, we
have
\[
UCB_{\ttime}(i)=\hat{\mu}_{\ttime}(i)+\lambda_{\ttime}(i)\le\mu_{i}+2\lambda_{\ttime}(i)
\]
Since we selected action $i$ at time $\ttime$ we have
\[
\mu_{i}+2\lambda_{\ttime}(i)\ge\mu^{*}
\]
Rearranging, we have,
\[
2\lambda_{\ttime}(i)\ge\mu^{*}-\mu_{i}=\Delta_{i}
\]
Each time we chose action $i$, we could not have made a very big
mistake because:
\[
\Delta_{i}\leq\text{ }2\cdot\sqrt{\frac{2\log \tHorizon}{n_{\ttime}(i)}}
\]

And therefore if $i$ is very far off from the optimal action we
would not choose it too many times. We can bound the number of times
action $i$ is used by,
\[
n_{\ttime}(i)\leq\frac{8}{\Delta_{i}^{2}}\log \tHorizon
\]

And over all we get:
\begin{align*}
\E\left[\text{Pseudo Regret}\right] & =
\sum_{i=1}^{k}\Delta_{i}\E\left[n_{\ttime}(i)\right]+\underbrace{\frac{2}{\tHorizon^{2}}\cdot
\tHorizon}_{\text{The bad event}}
\\
 & \le  \sum_{i=1}^{k}\frac{c}{\Delta_{i}}\cdot\log \tHorizon+\frac{2}{\tHorizon}
\end{align*}

\begin{theorem}
\label{thm:MAB:UCB1}
The pseudo regret of UCB is bounded by $O(\frac{1}{\Delta_{i}}\log \tHorizon)$
\end{theorem}

Similar to successive action elimination, we can establish the follwoing instance-independent regret bound. 

\begin{theorem}
\label{thm:MAB:UCB2}
The pseudo regret of UCB is bounded by $O( \sqrt{k\tHorizon}\log\tHorizon)$
\end{theorem}

\section{From Multi-Arm Bandits to MDPs}

Much of the techniques used in the case of Multi-arm bandits, can be extended naturally to the case of MDPs. In this section we sketch a simple extension where the dynamics of the MDPs is known, but the rewards are unknown.

We first need to define the model for the online learning in MDPs, which will be very similar to the one in MAB. We will concentrate on the case of a finite horizon return. The learner interacts with the MDP for $K$ episodes.

At each episode $\ttime\in[K]$, the learner selects a policy $\policy_\ttime$ and observes a trajectory $(\state^\ttime_1, \action^\ttime_1,\reward^\ttime_1 \ldots , \state_\tHorizon^\ttime)$, where the actions are selected using $\policy_\ttime$, i.e., $\action_\tau^\ttime=\policy_\ttime(\state_\tau^\ttime)$.

The goal of the learner is to minimize the pseudo regret. Let $\Value^*(\state_1)$ be the optimal value function from the initial state $\state_1$. The pseudo regret is define as,
\[
\E[Regret] = \E[\sum_{\ttime\in[K]}\Value^*(\state_1)-\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
\]

We now like to introduce a UCB-like algorithm. We will first assume that the learner knows the dynamics, but does not know the rewards. This will imply that the learner, given a reward function, can compute an optimal policy.

Let $\mu_{\state,\action}=\E[r_{\state,\action}]$ be the expected reward for $(\state,\action)$.
%
As in the case of UCB we will define an Upper Confidence Bound for each reward. Namely, for each state $\state$ and action $\action$ we will maintain an empirical average $\hat{\mu}^\ttime_{\state,\action}$ and a confidence parameter $\lambda^\ttime_{\state,\action}=\sqrt{\frac{2\log KSA}{n_{\state,\action}^\ttime}}$, where $n_{\state,\action}^\ttime$ is the number of times we visited state $\state$ and performed action $\action$.

We define the good event similar to before
\[
G=\{\forall\state,\action, \ttime\; |\hat{\mu}^\ttime_{\state,\action}-\mu_{\state,\action}|\leq \lambda_{\state,\action}^\ttime\}
\]
and similar to before, we show that it holds with high probability, namely $1-\frac{2}{K^2}$.

\begin{lemma}
    We have that  $\Pr[G]\geq 1-\frac{2}{K^2}$.
\end{lemma}

\begin{proof}
    Similar to the UCB analysis using Chernoff bounds.
\end{proof}

We now describe the UCB-RL algorithm. For each episode $\ttime$ we compute a UCB for each state-action, denote the resulting reward function by $\bar{R}^\ttime$. Recall that $\bar{R}^\ttime(\state,\action)=\hat{\mu}_{\state,\action}^\ttime+\lambda_{\state,\action}^\ttime$.  Let $\policy^\ttime$ the optimal policy with respect to the rewards $\bar{R}^\ttime$ (the UCB rewards).

The following lemma shows that we have ``optimism'', namely the expected value of $\policy^\ttime$ w.r.t. the reward function $\bar{R}^\ttime$ upper bounds the optimal reward function $\Value^*$.

In the following we use the notation $\Value(\cdot|R)$ to imply that we are using the reward function $R$. We denote by $R^*$ the true reward function, i.e., $R^*(\state,\action)=\E[r_{\state,\action}]$.

\begin{lemma}
    Assume the good event $G$ holds. Then, for any episode $\ttime$ we have that $\Value^{\policy^\ttime}(\state | \bar{R}^\ttime)\geq \Value^*(s|R^*)$.
    %, where $R^*$ is the true reward function.
    %$\E[\sum_{\tau=1}^\tHorizon R^\ttime$
\end{lemma}

\begin{proof}
    Since $\policy^\ttime$ is optimal for the rewards $\bar{R}^\ttime$, we have that $\Value^{\policy^\ttime}(\state | \bar{R}^\ttime)\geq \Value^{\policy^*}(\state | \bar{R}^\ttime)$.

Since $\bar{R}^\ttime\geq R^*$, then we have
$\Value^{\policy^*}(\state | \bar{R}^\ttime)\geq \Value^{\policy^*}(\state | R^*)$.

Combining the two inequalities, yields the lemma.
\end{proof}

The optimism is very powerful property, as it let's us bound the pseudo regret as a function of quantities we observe, namely $\bar{R}^\ttime$, rather than unknown quantities, such as the true rewards $R^*$ or the unklnown optimal policy $\policy^*$.

\begin{lemma}
\label{MAB:lemma:UCB-RL:optimism}
Assume the good event $G$ holds. Then,
    \[
    \E[Regret] \leq \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
    \E[2\lambda^\ttime_{\state^\ttime_\tau,\action^\ttime_\tau}]
    \]
\end{lemma}

\begin{proof}
    The definition of the pseudo regret is
    \[
    \E[Regret]= \E[\sum_{\ttime\in [K]} \Value^*(\state_1)-\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
    \]

Using Lemma~\ref{MAB:lemma:UCB-RL:optimism}, we have that,
    \[
    \E[Regret]= \E[\sum_{\ttime\in [K]} \Value^*(\state_1)-\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
    \leq
    \sum_{\ttime\in [K]}
    \E[\Value^{\policy^\ttime}(\state_1|\bar{R}^\ttime)
    -\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
    %\Value^{\policy^\ttime}(\state_1|R^*)]
    \]
Since the good event $G$ holds, we have [[YM: needed??]]
\[
\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} \geq \sum_{\tau=1}^\tHorizon
 \hat{\mu}_{\state_\tau^\ttime,\action_\tau^\ttime}-\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}
\]
Note that
\[
\E[\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} ] =\E[\Value^{\policy^\ttime}(\state_1|R^*)]
\]
and we have,
\[
\E[Regret]\leq \E[\Value^{\policy^\ttime}(\state_1|\bar{R}^\ttime)] -
\E[\Value^{\policy^\ttime}(\state_1|R^*)]=
\E[\sum_{\tau=1}^\tHorizon \lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} ]
\]
which compltes the proof of the lemma.
%Combining all the identities yields the lemma.
\end{proof}

We are now left with only upper bounding the sum of the confidence bounds.
We can upper bound this sum regardless of the realization. [[YM: hopefully correct]]

\begin{lemma}
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
   % \sqrt{\frac{2\log KSA}{n_{\state,\action}^\ttime}}
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}
    \leq \sqrt{KSA\log (KSA)}
    \]
\end{lemma}

\begin{proof}
    We first change the order of summation to be over state-action pairs.
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} =
\sum_{\state,\action} \sum_{\tau=1}^{n_{\state,\action}^K}
\sqrt{\frac{2\log KSA}{\tau}}
    \]
In the above, $\tau$ is the index of the $\tau$-th visit to the state-action pair $(\state,\action)$ at some time $\ttime$. During that visit we have that  $n_{\state,\action}^\ttime=\tau$.
This explain the expression for the confidence intervals.

Since $1/\sqrt{x}$ is a convex function, we can upper bound the sum using Jensen inequality, and have $\sum_{\tau=1}^N 1/\sqrt{\tau}\leq \sqrt{2N}$, and have
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} \leq
\sqrt{2\log KSA}
\sum_{\state,\action}
\sqrt{2 n_{\state,\action}^K}
    \]
Recall that $\sum_{\state,\action} n_{\state,\action}^K =K$. This implies that $\sum_{\state,\action}
\sqrt{2 n_{\state,\action}^K}$ is maximized when all the $n_{\state,\action}^K$ are equal, i.e, $n_{\state,\action}^K=K/(SA)$. Hence,
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} \leq
2
\sqrt{SAK\log KSA}
    \]
\end{proof}

We can now derive the upper bound on the pseudo regret
\begin{theorem}
    \[
    \E[Regret] \leq 2\sqrt{KSA\log (KSA)}
    \]
\end{theorem}

\section{Best Arm Identification}

We would like to identify the best action, or an almost best action.
We can define the goal in one of two ways.

\paragraph{PAC criteria }
An action $i$ is $\epsilon$-optimal if  $\mu_i\geq \mu^*-\epsilon$. The PAC criteria is
that, given $\epsilon,\delta>0$, with probability at least
$1-\delta$, find an $\epsilon$ optimal action.

\paragraph{Exact identification}
Given $\Delta\le\min_{i\neq a^*}\mu^{*}-\mu_{i}$ (for every suboptimal action $i$),
find the optimal action $\action_{*}$, with probability at least
$1-\delta$.

\subsection{Naive Algorithm (PAC criteria):}

We sample each action $i$ for
$m=\frac{8}{\epsilon^{2}}\log\frac{2k}{\delta}$ times, and return
$a= \arg\max_{i}\hat{\mu}_{i}$ .

For rewards in $[ 0 ,1]$, then, by Theorem \ref{thm:hoeffding}, for
every action $i$ we have
\[
Pr\left[\underbrace{\left|\hat{\mu}_{i}-\mu_{i}\right|>\frac{\epsilon}{2}}_{\text{bad
event}}\right]\le 2
e^{-\left(\frac{\epsilon}{2}\right)^{2}m/2}=\frac{\delta}{k}
\]
By union bound we get:
\[
Pr\left[\exists_{i}\left|\hat{\mu}_{i}-\mu_{i}\right|>\frac{\epsilon}{2}\right]\le\delta
\]

If the bad event
$B=\{\exists_{i}\left|\hat{\mu}_{i}-\mu_{i}\right|>\frac{\epsilon}{2}\}$
did not happen, then both: (1)
$\mu^{*}-\frac{\epsilon}{2}\le\hat{\mu}^{*}$ and (2)
$\mu_{i}+\frac{\epsilon}{2}\le\hat{\mu}_{i}$.

This implies,
\[
\Rightarrow\text{ }\mu_{i}+\frac{\epsilon}{2}\ge\hat{\mu}_{i}\ge\hat{\mu}^{*}\ge\mu^{*}-\frac{\epsilon}{2}
\]
\[
\Rightarrow\text{ }\epsilon\ge\mu^{*}-\mu_{i}
\]

And therefore $a={\displaystyle \arg\max_{i}\hat{\mu}_{i}}$ is the
optimal action in probability $1-\delta$.

We would like to slightly improve the sample size of this algorithm

\subsection{Median Algorithm}

The idea: the algorithm runs for $l$ phases, after each phase we
eliminate half of the actions. This elimination allows us to sample
each action more times in the next phase which makes eliminating the
optimal action less likely.

\begin{algorithm}
\textbf{Input}: $\epsilon,\delta>0$

\textbf{Output}: $\bar{a}\in A$

\textbf{Init}: $S_{1}=A$, $\epsilon_{1}=\frac{\epsilon}{4}$,
$\delta_{1}=\frac{\delta}{2}$, $l=1$

\textbf{Repeat}:

~~~~~~$\forall_{i}\in S_{l}$, sample action $i$ for $m(\epsilon_l,\delta_l)=\frac{1}{\left(\frac{\epsilon_{l}}{2}\right)^{2}}\log\left(\frac{3}{\delta_{l}}\right)$
times

~~~~~~$\hat{\mu}_{i}\leftarrow \text{average reawrd of action $i$ (only of samples during the \ensuremath{l^{th}}phase)}$

~~~~~~$\text{median}_{l}\leftarrow median\left\{ \hat{\mu}_{i}:i\in S_{l}\right\} $

~~~~~~$S_{l+1}\leftarrow\left\{ i\in S_{l}:\hat{\mu}_{i}\ge\text{median}_{l}\right\} $

~~~~~~$\epsilon_{l+1}\leftarrow\frac{3}{4}\epsilon_{l}$

~~~~~~$\delta_{l+1}\leftarrow\frac{\delta_{l}}{2}$

~~~~~~$l\leftarrow l+1$

\textbf{Until} $\left|S_{l}\right|=1$

\textbf{Output} $\hat{a}$ where $S_l=\{\hat{a}\}$


\caption{Best Arm Identification}
\end{algorithm}

\paragraph{Complexity:}

During phase $l$ we have $\left|S_{l}\right|=\frac{k}{2^{l-1}}$
actions. We are setting the accuracy and confidence parameters as follows.
\[
\epsilon_{l}=\frac{3}{4}\epsilon_{l-1}=\frac{\epsilon}{4}\left(\frac{3}{4}\right)^{l-1},\;\;\;\delta_{l}=\frac{\delta}{2^{l}}
\]
This implies that the sum of the accuracy and confidence parameters over the phases would be,
\[
 \sum_l\epsilon_{l}\le\frac{\epsilon}{4}\left(\frac{3}{4}\right)^{l-1}\le \epsilon, \text{ and } \sum_l\delta_{l}\le\sum_l \frac{\delta}{2^{l}} \le\delta
\]

In phase $l$ we have $S_l$ as the set of actions. For each action in $S_l$ we sample $m(\epsilon_l,\delta_l)$ samples.
The total number of samples is therefore:

\begin{align*}
 \sum_l |S_l| \cdot \frac{4}{\epsilon_l^2}\log \frac{3}{\delta_l} &=
\sum_{l}\frac{k}{2^{l-1}}\frac{64}{\epsilon^{2}}\left(\frac{16}{9}\right)^{l-1}\log\frac{3\cdot2^{l}}{\delta}\\
& =  {\displaystyle \sum_{l}k\left(\frac{8}{9}\right)^{l-1}\left[c\cdot\frac{\log\frac{1}{\delta}}{\epsilon^{2}}+\frac{\log3}{\epsilon^{2}}+\frac{l}{\epsilon^{2}}\right]}\\
\\
 & =  O\left(\frac{k}{\epsilon^{2}}\log\frac{1}{\delta}\right)
\end{align*}


\paragraph{Correctness:}
The following lemma is the main tool in establishing the correctness of the algorithm. It shows that when we move from phase $l$ to phase $l+1$ with high probability ($1-\delta_l$) the decrease in accuracy is at most $\epsilon_l$

\begin{lemma}
Given $S_l$, we have
$$Pr\left[\underbrace{{\displaystyle \max_{j\in
S_{l}}\mu_{j}}}_{\text{best action
\ensuremath{l}}}\le\underbrace{{\displaystyle \max_{j\in
S_{l+1}}\mu_{j}}}_{\text{best action
\ensuremath{l+1}}}+\epsilon_{l}\right]\ge1-\delta_{l}$$
\end{lemma}

\begin{proof}
%We do the proof for $l=1$, general $l$ is similar.
Let $\mu^*_l=\max_{j\in S_l} \mu_j$, the expected reward of the best action in $S_l$, and $a^*_l=\arg\max_{j\in S_l} \mu_j$ be the best action in $S_l$.
Define the bad event $E_{l}=\left\{\hat{\mu}^{*}_l<\mu^{*}_l-\frac{\epsilon_{1}}{2}\right\}$. (Note that $E_l$ depends only on the action $a^*_l$. Since we sample $a^*_l$ for $m(\epsilon_l,\delta_l)$ times, we have that $Pr\left[E_{l}\right]\le\frac{\delta_{l}}{3}$. If $E_{l}$ did not happen, we define a bad set of actions:
\[
\text{Bad}=\left\{ j\text{ }:\text{ }\mu^{*}_l-\mu_{j}>\epsilon_{l},\text{ }\hat{\mu}_{j}\ge\hat{\mu}^{*}\right\}
\]
The set $Bad$ includes the actions which have a better empirical average than $a_l^*$, and the difference in the expectation is more than $\epsilon_l$. We would like to show that $S_{l+!}\not\subseteq Bad$, and hence includes at least one action which has expectation of at most $\epsilon_l$ from $\mu^*_l$.


Consider an action $j$ such that $\mu^*-\mu_j > \epsilon_l$, then:
\begin{align*}
Pr[\hat{\mu}_{j}>
\hat{\mu}^{*}|\underbrace{\hat{\mu}^{*}_l\ge\mu^{*}-\frac{\epsilon_{l}}{2}}_{\urcorner
E_{l}}\big] & \le Pr[\hat{\mu}_{j}> \mu^{*}_l-\frac{\epsilon_{l}}{2}]\\
&\leq
Pr[\hat{\mu}_{j}\ge\mu_{j}+\frac{\epsilon_{l}}{2}|\urcorner E_{l}]
\le  \frac{\delta_{1}}{3}
\end{align*}
where the second inequality follows since $\mu^*_l-\epsilon_l/2 > \mu_j+\epsilon_l/2$, which follows since $\mu^*_l-\mu_j>\epsilon_l$.



Note that the failure probability is not negligible, and our main aim is to avoid a union bound which will introduce a $\log k$ factor. We will show that it cannot happen to too many such actions. We will bound the expectation of the size of $Bad$,
\[
E[|\text{Bad}||\urcorner E_{l}]\le |Bad|\frac{\delta_{1}}{3}
\]
 with Markov's inequality we get:
\[
Pr\left[\left|\text{Bad}\right|\ge\frac{k}{2}\right|\urcorner E_{l}]
 \le  \frac{E\left|\text{Bad}\right|}{|Bad|/2}
  =  \frac{2}{3}\delta_{l}
\]
 with probability $1-\delta_{l}$: $\hat{\mu}^{*}\ge\mu^{*}-\frac{\epsilon_{1}}{2}$ and $\left|\text{Bad}\right|\le\frac{k}{2}$. Therefore: $\exists_{j}\notin\text{Bad}$ and $j\in S_{l+1}$.
\end{proof}

Given the above lemma, we can conclude with the following theorem.

\begin{theorem}
The median elimination algorithm guarantees that with probability at least $1-\delta$ we have that $\mu^*-\mu_{\hat{a}}\leq \epsilon$
\end{theorem}

\begin{proof}
With probability at least $1- \sum_l \delta_l \geq 1-\delta$ we have that during each phase $l$, it holds that $\max_{j\in
S_{l}}\mu_{j} \leq \max_{j\in S_{l+1}}\mu_{j}+ \epsilon_{l}$.

By summing all the inequalities of the different phases,
this implies that
\[
\mu^*=\max_{j\in
A} \mu_{j} \leq \mu_{\hat{a}} \sum_l \epsilon_{l}\leq \mu_{\hat{a}} +\epsilon.
\]
Recall that $S_1=A$ and $S_{\log K} =\{\hat{a}\}$.
\end{proof}

\section{Bibliography Remarks}

Multi-arm bandits date back to Robbins \cite{Robbins52} who defines (implicitly) asymptotic vanishing regret for two stochastic actions. The tight regret bounds where given by Lai and Robbins \cite{LaiR85}. The UCB algorithm was presented in \cite{AuerCF02}. The action elimination is from \cite{Even-DarMM06}, as well as the Median algorithm. Our presentation of the analysis of UCB and action elimination borrows from the presentation in  \cite{Slivkins-book-19}.


There are multiple books that cover online learning and multi-arm bandit. By now, the classical book of Cesa{-}Bianchi and Lugosi \cite{Cesa-Bianchi-Lugosi-book}, mainly on adversarial online learning. The book of Slivkins \cite{Slivkins-book-19} covers mainly stochastic multi-arm bandits. The book of Lattimore and Szepesv{\'a}ri \cite{Lattimore-Csaba-book-2020} covers many topics of adversarial bandits and online learning.
