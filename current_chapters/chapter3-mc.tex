

%\section{Markov Chains}

Extending the deterministic decision making framework of Chapter \ref{chapter:DDP} to stochastic models requires a mathematical model for decision making under uncertainty. With the goal of presenting such a model in mind, in this chapter we cover the fundamentals of the Markov chain stochastic process.

% We start by considering a stochastic dynamics model which does not have any actions, so that we can concentrate on the dynamics.

A \emph{Markov chain} $\{ {X_\ttime},\;\ttime = 0,1,2, \ldots \} $, with
${X_\ttime} \in X$, is a discrete-time stochastic process, over a
finite or countable state-space $X$, that satisfies the following
Markov property:
    \[{\cal P}({X_{\ttime +1}} = j|{X_\ttime} = i,{X_{\ttime - 1}}, \ldots {X_0}) = {\cal P}({X_{\ttime +1}} = j|{X_\ttime} = i).\]
We focus on time-homogeneous Markov chains, where
\[{\cal P}({X_{\ttime +1}} = j|{X_\ttime} = i) = {\cal P}({X_1} = j|{X_0} = i) \buildrel \Delta \over = {p_{i,j}}.\]
The ${p_{i,j}}$'s  are the transition probabilities, which satisfy
$p_{i,j} \ge 0$, and for each $ i\in X$ we have $\sum_{j \in X}
{{p_{i,j}} = 1} $, namely, $\{p_{i,j}:j\in X\}$ is a distribution on
the next state following state $i$.  The matrix $P = ({p_{i,j}})$ is
the transition matrix. The matrix is row-stochastic (each row sums
to $1$ and all entries non-negative).

Given the initial distribution ${p_0}$ of ${X_0}$, namely ${\cal
P}({X_0} = i) = {p_0}(i)$, we obtain the finite-dimensional
distributions:
\[{\cal P}({X_0} = {i_0}, \ldots ,{X_\ttime} = {i_\ttime}) = {p_0}(i_0){p_{{i_0},{i_1}}} \cdot  \ldots  \cdot {p_{{i_{\ttime - 1}},{i_\ttime}}}.\]

Define $p_{i,j}^{(m)} = {\cal P}({X_m} = j|{X_0} = i),$ the $m$-step
transition probabilities.  It is easy to verify that $p_{i,j}^{(m)}
= {[{P^m}]_{ij}}$, where ${P^m}$ is the $m$-th power of the matrix
$P$.

\begin{example}
\footnote{Simple MC, two states, running example.} Consider the
following two state Markov chain, with transition probability $P$
and initial distribution $p_0$, as follows:
$$
P=
\begin{pmatrix}
0.4 & 0.6  \\
0.2 & 0.8
\end{pmatrix}
\qquad p_0=
\begin{pmatrix}
0.5  & 0.5
\end{pmatrix}
$$
Initially, we have both states equally likely. After one step, the
distribution of states is $p_1=p_0 P=(0.3\;,\; 0.7)$. After two
steps we have $p_2=p_1 P=p_0 P^2 = (0.26 \;,\; 0.74 )$. The limit of
this sequence would be $p_\infty =(0.25 \;,\; 0.75)$, which is
called the {\em steady state distribution}, and would be discussed
later.
\end{example}

\paragraph{State classification:}

\begin{itemize}
\item State $j$ is \textit{accessible} (or \textit{reachable}) from $i$  (denoted by $i \to j$) if $p_{i,j}^{(m)} > 0$ for some $m \ge
1$.\\
For a finite $X$  we can compute the accessibility property as
follows. Construct a directed graph $G(X,E)$ where the vertices are
the states $X$ and there is a directed edge $(i,j)$ if $p_{i,j}>0$.
State $j$ is accessible from state $i$ iff there exists a directed
path in $G(X,E)$ from $i$ to $j$.\\
Note that the relation is transitive. If $i \to j$ and $j \to k$ then $i \to k$. This follows since $i \to j$ implies that there is $m_1$ such that  $p_{i,j}^{(m_1)} > 0$. Similarly, since $j \to k$ there is $m_2$ such that  $p_{j,k}^{(m_2)} > 0$. Therefore, for $m=m_1+m_2$ we have $p_{i,k}^{(m)} > p_{i,j}^{(m_1)}p_{j,k}^{(m_2)}>0$.
\item States $i$ and $j$ are \textit{communicating} states (or communicate) if $i \to j$ and $j \to
i$.\\
For a finite $X$, this implies that in $G(X,E)$ there is both a
directed path from $i$ to $j$ and from $j$ to $i$.
\item A \textit{communicating class} (or just
\emph{class}) is a maximal collection of states that communicate.\\
For a finite $X$, this implies that in $G(X,E)$ we have $i$ and $j$
in the same strongly connected component of the graph. (A strongly
connected component has a directed path between any pair of
vertices.)
\item The Markov chain is
\textit{irreducible} if all states belong to a single class (i.e.,
all states communicate with each other).\\
For a finite $X$, this implies that $G(X,E)$ is strongly
connected.
\item
State $i$ has a \textit{period} $d_i=GCD \{m\geq
1:p_{i,i}^{(m)}>0\}$, where $GCD$ is the greatest common divisor. A
state is aperiodic if $d_i=1$.\\
State $i$ is periodic with period $d_i \ge 2$ if  $p_{i,i}^{(m)} =
0$ for $m \pmod {d_i} \neq 0$ and for any $m$ such that
$p_{i,i}^{(m)} > 0$ we have $m \pmod {d_i} =0$.\\
If a state $i$ is aperiodic, then there exists an integer $m_0$ such
that for any $m\geq m_0$ we have $p_{i,i}^{(m)}>0$.
\item
Periodicity is a class property: all states in the same class
have the same period. Specifically, if some state is
\textit{a-periodic},
then all states in the class are a-periodic.\\
\begin{claim}
For any two states $i$ and $j$ with periods $d_i$ and $d_j$, in the
same communicating class, we have $d_i=d_j$.
\end{claim}
\begin{proof}
For contradiction, assume that $d_j \pmod {d_i}\neq 0$. Since they
are in the same communicating class, we have a trajectory from $i$
to $j$ of length $m_{i,j}$ and from $j$ to $i$ of length $m_{j,i}$.
This implies that $(m_{i,j}+m_{j,i})\pmod {d_i}=0$. Now, there is a
trajectory (which is a cycle) of length $m_{j,j}$ from $j$ back to
$j$ such that $m_{j,j}\pmod {d_i}\neq 0$ (otherwise $d_i$ divides
the period of $j$). Consider the path from $i$ to itself of length
$m_{i,j}+m_{j,j}+m_{j,i}$. We have that $(m_{ij}+m_{jj}+m_{ji})\pmod
{d_i} = m_{jj}\pmod {d_i} \neq 0$. This is a contradiction to the
definition of $d_i$. Therefore, $d_j \pmod {d_i}=0$ and similarly
$d_i \pmod {d_j}=0$, which implies that $d_i=d_j$.
\end{proof}
The claim shows that periodicity is a class property, and all the
states in a class have the same period.
\end{itemize}

\begin{example}
Add figures explaining the definitions.
\end{example}

\paragraph{Recurrence:}
\begin{itemize}
\item
State $i$ is \textit{recurrent} if ${\cal P}({X_\ttime} = i{
\textrm{ for some }}\ttime \ge 1|{X_0} = i) = 1$. Otherwise, state $i$ is
\textit{transient}.\\
%\item
We can relate the state property of recurrent and transient to the
expected number of returns to a state.

\begin{claim}
State $i$ is transient iff $\sum\nolimits_{m = 1}^\infty
{p_{i,i}^{(m)}}  < \infty $.
\end{claim}

\begin{proof}
Assume that state $i$ is transient. Let $q_i={\cal P}({X_\ttime} =
i{ \textrm{ for some }}\ttime \ge 1|{X_0} = i) $. Since state $i$ is
transient we have $q_i<1$. Let $Z_i$ be the number of times the
trajectory returns to state $i$. Note that $Z_i$ is geometrically
distributed with parameter $q_i$, namely $\Pr[Z_i=k]=q_i^k (1-q_i)$.
Therefore the expected number of returns to state $i$ is $1/(1-q_i)$
and is finite.
%
The expected number of returns to state $i$ is equivalently
$\sum\nolimits_{m = 1}^\infty {p_{i,i}^{(m)}} $, and hence if a
state is transient we have $\sum\nolimits_{m = 1}^\infty
{p_{i,i}^{(m)}}  < \infty $.

For the other direction, assume that $\sum\nolimits_{m = 1}^\infty
{p_{i,i}^{(m)}}  < \infty $. This implies that there is an $m_0$
such that $\sum\nolimits_{m = m_0}^\infty {p_{i,i}^{(m)}}  < 1/2$.
Consider the probability of returning to $i$ within $m_0$ stages.
This implies that ${\cal P}({X_\ttime} = i{ \textrm{ for some }}t
\ge m_0|{X_0} = i)< 1/2$. Now consider the probability $q'_i={\cal
P}({X_\ttime} = i{ \textrm{ for some }}m_0\geq \ttime \ge 1|{X_0} = i) $.
%
If $q'_i<1$, this implies that ${\cal P}({X_\ttime} = i{ \textrm{
for some }}\ttime \ge 1|{X_0} = i)<q'_i +(1-q'_i)/2=(1+q'_i)/2<1$, which
implies that state $i$ is transient.
%
If $q'_i=1$, this implies that after at most $m_0$ stages we are
guaranteed to return to $i$, hence the expected number of return to
state $i$ is infinite, i.e., $\sum\nolimits_{m = 1}^\infty
{p_{i,i}^{(m)}}  = \infty $. This is in contradiction to the
assumption that $\sum\nolimits_{m = 1}^\infty {p_{i,i}^{(m)}}  <
\infty $.
%This implies that a state is transient iff  the expected number of
%returns to state $i$ is finite and that a state $i$ is recurrent iff
%the expected number of returns to state $i$ is infinite. Since the
%expected number of returns to state $i$ is $\sum\nolimits_{m =
%1}^\infty {p_{i,i}^{(m)}} $, an equivalent criteria is that state
%$i$ is recurrent if and only if $\sum\nolimits_{m = 1}^\infty
%{p_{i,i}^{(m)}}  = \infty $.
\end{proof}


%
%If $\sum\nolimits_{m = 1}^\infty {p_{i,i}^{(m)}}  < \infty $ then
%state $i$ is transient. (By Borel-Cantteli this implies (with
%probability $1$) that such states will only occur a finite number of
%times.)
%
%{\tt The problem is the we cannot use the Borel-Cantteli for the
%other direction, since we needs independence. We still need to show:
%
% State $i$ is
%recurrent if and only if $\sum\nolimits_{m = 1}^\infty
%{p_{i,i}^{(m)}}  = \infty $.
%
%Here is hopefully a correct proof:}
%
%Let $q_i={\cal P}({X_\ttime} = i{ \textrm{ for some }}t \ge 1|{X_0}
%= i) $. Assume that $q_i<1$, i.e., state $i$ is transient. Let $Z_i$
%be the number of times the trajectory returns to state $i$. Note
%that $Z_i$ is geometrically distributed with parameter $q_i$, namely
%$\Pr[Z_i=k]=q_i^k (1-q_i)$. Therefore the expected number of returns
%to state $i$ is $1/(1-q_i)$ and is finite. This implies that a state
%is transient iff  the expected number of returns to state $i$ is
%finite and that a state $i$ is recurrent iff the expected number of
%returns to state $i$ is infinite. Since the expected number of
%returns to state $i$ is $\sum\nolimits_{m = 1}^\infty
%{p_{i,i}^{(m)}} $, an equivalent criteria is that state $i$ is
%recurrent if and only if $\sum\nolimits_{m = 1}^\infty
%{p_{i,i}^{(m)}}  = \infty $.


\item Recurrence is a class property.\\
To see this consider two states $i$ and $j$ in the same communicating class and $i$ is recurrent. Since $i$ is recurrent
$\sum\nolimits_{m = 1}^\infty {p_{i,i}^{(m)}}  = \infty $. Since $j$
is accessible from $i$ there is a $k$ such that $p_{i,j}^{(k)}
>0$. Since $i$ is accessible from $j$, there exists a $k'$ such that
$p_{j,i}^{(k')}>0$. We can lower bound $\sum\nolimits_{m = 1}^\infty
{p_{j,j}^{(m)}}$ by $\sum_{m = 1}^\infty
{p_{j,i}^{(k')}}{p_{i,i}^{(m)}} {p_{i,j}^{(k)}} = \infty $.
Therefore we showed that state $j$ is recurrent.


\item
If states $i$ and $j$ are in the same recurrent (communicating)
class, then state $j$ is (eventually) reached from state $i$ with
probability 1, namely,  ${\cal P}({X_\ttime} = j{\textrm{ for some t}} \ge
1|{X_0} = i) = 1$.\\
This follows from the fact that both states occur infinitely often,
with probability $1$.
\item
Let ${T_i}$ be the {\em return time} to state $i$  (i.e., number of stages
required for $({X_\ttime})$ starting from state $i$ to the first
return to $i$). If $i$ is a recurrent
state, then ${T_i} < \infty $ w.p. 1.\\
Since otherwise, there is a positive probability that we never
return to state $i$, and hence state $i$ is not recurrent.
%
\item State $i$ is {\em positive recurrent} if  $\E({T_i}) < \infty $,
and null recurrent if $\E({T_i}) = \infty $.\\
If the state space $X$
is finite, all recurrent states are positive recurrent.\\
This follows since the set of states that are null recurrent cannot
have transitions from positive recurrent states and cannot have a
transition to transient states. If the chain never leaves the set of
null recurrent states, then some state would have a return time
which is at most the size of the set. If there is a positive
probability of leaving the set (and never returning) then the states
are transient. (See the proof of Theorem \ref{thm:countableMC} for a more formal proof of a similar claim for countable Markov Chains.)
\end{itemize}

In the following we illustrate some of the notions that we define. we start with the classic random walk on the integers, where all integer (states) are null recurrent.

\begin{example}{\bf Random walk}
Consider the following Markov chain over the integers. The states
are the integers. The initial state is $0$. At each state $i$, with
probability $1/2$ we move to $i+1$ and with probability $1/2$ to
$i-1$. Namely, $p_{i,i+1}=1/2$, $p_{i,i-1}=1/2$, and $p_{i,j} =0$
for $j \not\in \{i-1,i+ 1\}$. We will show that $T_i$ is finite with
probability $1$ and $\E[T_i]=\infty$. This implies that all the
states are null recurrent.

To compute $\E[T_i]$ consider what happens after one and two steps. Let
$Z_{i,i-1}$ be the time to move from $i$ to $i-1$. Note that  we
have \[
\E[T_i]=1+0.5\E[Z_{i+1,i}]+0.5\E[Z_{i-1,i}]=1+\E[Z_{1,0}],
\]
since, due to symmetry, $\E[Z_{i,i+1}]=\E[Z_{i+1,i}]=\E[Z_{1,0}]$.

After two steps we are either back to $i$, or at state $i+2$ or state $i-2$.
For $\E[T_i]$ we have that
\begin{align*}
\E[T_i]=&
2+\frac{1}{4}\E[Z_{i+2,i}]+\frac{1}{4}\E[Z_{i-2,i}] \\
=&2+\frac{1}{4} (\E[Z_{i+2,i+1}]+\E[Z_{i+1,i}])+\frac{1}{4}( \E[Z_{i-2,i-1}]+\E[Z_{i-1,i}])\\
 =&2+E[Z_{1,0}]\\
\end{align*}
where the first identity uses the fact that $Z_{i+2,i}=Z_{i+2,i+1}+Z_{i+1,i}$, since in order tp reach from state $i+2$ to state $i$ we need to first reach from state $i+2$ state $i+1$, and then from state $i+1$ to state $i$.

This implies that we have
\[
1+\E[Z_{1,0}]=\E[T_i]=2+E[Z_{1,0}]
\]
Clearly, there is no finite value for $\E[Z_{1,0}]$ which will
satisfy both equations, which implies $\E[Z_{1,0}]=\infty$, and hence
$\E[T_i]=\infty$.

To show that state $0$ is a recurrent state, note that the probability
that at time $2k$ we are at state $0$ is exactly $p_{0,0}^{(2k)}=\binom{2k}{k}2^{-2k}\approx \frac{c}{\sqrt{k}}$ (using Stirling's approximation), for some constant $c>0$. This
implies that
$$
\sum\nolimits_{m = 1}^\infty {p_{0,0}^{(m)}}\approx \sum\nolimits_{m
= 1}^\infty \frac{c}{\sqrt{m}}=\infty
$$
and therefore state $0$ is recurrent. (By symmetry, this shows that
all the states are recurrent.)

Note that this Markov chain has a period of $2$. This follows since any trajectory starting at $0$ and returning to $0$ has an equal number of $+1$ and $-1$ and therefore of even length. Any even number $n$ has a trajectory of this length that starts at $0$ and returns to $0$, for example, having $n/2$ times $+1$ followed by $n/2$ times $-1$.
\end{example}

The next example is a simple modification of the random walk, where each time we either return to the origin or continue to the next integer with equal probability. 
This Markov chain will have all (non-negative) integers as possitive recurrent states.

\begin{example}
{\bf Random walk with jumps.}
%
Consider the following Markov chain over the integers. The states
are the integers. The initial state is $0$. At each state $i$, with
probability $1/2$ we move to $i+1$ and with probability $1/2$ we
return to $0$. Namely, $p_{i,i+1}=1/2$, $p_{i,0}=1/2$, and $p_{i,j}
=0$ for $j \not\in \{0,i+ 1\}$. We will show that $\E[T_i]<\infty$
(which implies that $T_i$ is finite with probability $1$).

From any state we return to $0$ with probability $1/2$, therefore
$\E[T_0]=2$ (The return time is $1$ with probability $1/2$, $2$ with probability $(1/2)^2$, $k$ with probability $(1/2)^k$, and computing the expectation gives $\sum_{k=1}^\infty k/2^k = 2$). We will show that for state $i$ we have
$\E[T_i]\leq2+2\cdot 2^{i-1}$. We will decompose $T_i$ to two parts. The
first is the return to $0$, this part has expectation $2$. The
second is to reach state $i$ from state $0$. Consider an epoch as
the time between two visits to $0$. The probability that an epoch
would reach $i$ is exactly $2^{-i}$. The expected time of an epoch
is $2$ (the expected time to return to state $0$). The expected time
to return to state $0$, given that we did not reach state $i$ is
less than $2$. Therefore, $\E[T_i]\leq 2+2\cdot 2^{i}$.

Note that this Markov chain is aperiodic.
\end{example}



\paragraph{Invariant Distribution:} %changed pi to mu since pi is latter a policy!
The probability vector $\mu  = ({\mu_i})$ is an invariant
distribution (or stationary distribution or steady state
distribution) for the Markov chain if $\mu^\top P = \mu^\top$,
namely
\[{\mu_j} = \sum_i {{\mu_i}} {p_{i,j}}\quad \forall j.\]
Clearly, if ${X_\ttime} \sim \mu $ then ${X_{\ttime + 1}} \sim \mu
$. If ${X_0} \sim \mu $, then the Markov chain $({X_\ttime})$ is a
stationary stochastic process.


\begin{theorem}
\label{Thm:MC-stationary}
 Let $({X_\ttime})$ be an irreducible and
a-periodic Markov chain over a finite state space $X$ with
transition matrix $P$. Then there is a unique distribution $\mu$
such that $\mu^\top P = \mu^\top >0$.
\end{theorem}

\begin{proof}
Assume that $x$ is an eigenvector of $P$ with eigenvalue $\lambda$,
i.e., we have $Px=\lambda x$. Since $P$ is a stochastic matrix, we
have $\|P x\|_\infty \leq \|x\|_\infty$, which implies that $\lambda
\leq 1$. Since the matrix $P$ is row stochastic, $P\vec{1}=\vec{1}$,
which implies that $P$ has a right eigenvalue of $1$ and this is the
maximal eigenvalue. Since the sets of right and left eigenvalues are
identical for square matrices, we conclude that there is $x$ such that $x^\top P=x^\top$.
Our first task is to show that there is such an $x$ with $x\geq0$.

Since the Markov chain is irreducible and a-periodic, there is an
integer $m$, such that $P^m$ has all the entries strictly positive.
Namely, for any $i,j\in X$ we have $p^{(m)}_{i,j}>0$.
%(Formally,

We now show a general property of positive matrices (matrices where
all the entries are strictly positive). Let $A=P^m$ be a positive
matrix and $x$ an eigenvector of $A$ with eigenvalue $1$. First, if
$x$ has complex number then $Re(x)$ and $Im(x)$ are eigenvectors
of $A$ of eigenvalue $1$ and one of them is non-zero. Therefore we
can assume that $x\in \reals^d$. We would like to show that there is
an $x\geq 0$ such that $x^\top A=x^\top$. If $x\geq 0$ we are done.
If $x\leq 0$ we can take $x'=-x$ and we are done. We need to show
that $x$ cannot have both positive and negative entries.

For contradiction, assume that we have $x_{k}>0$ and $x_{k'}<0$.
This implies that for any weight vector $w>0$ we have $|x^\top w| <
|x|^\top w$, where $|x|$ is point-wise absolute value. Therefore,
\[
\sum_j |x_j|= \sum_j |\sum_i x_i P_{i,j}| < \sum_j \sum_i |x_i|
P_{i,j}=  \sum_i |x_i| \sum_j P_{i,j}= \sum_j |x_j|
\]
where the first identity follows since $x$ is an eigenvector. The
second since $P$ is strictly positive.The third is a change of order
of summation. The last follows since $P$ is a row stochastic matrix, so
each row sums to $1$. Clearly, we reached a contradiction, and
therefore $x$ cannot have both positive and negative entries.

%{\tt [YM:old] To conclude the proof we will show a general property
%of positive matrices (matrices where are the entries are strictly
%positive). Let $A=P^m$ be a positive matrix and $x$ an eigenvector
%of $A$ with eigenvalue $1$, then $|x|$ is an eigenvector of $A$ with
%eigenvalue $1$. (By $|x|$ we mean point-wise absolute value,
%including when the entries are complex numbers.)
%
% Since $x^\top A =x$, then $|x|=|x^\top A|\leq |x|^\top
%A$, where the last inequality is since $A$ is a positive matrix. It
%remains to show that $|x| = |x|^\top A$. Let $z=|x|^\top A$ and
%$y=z-|x|$. We know that $y\geq 0$. Assume that some $y_i>0$. Then
%$y^\top A>0$ since $A$ is a positive matrix and similarly $z>0$.
%
%Now there is an $\epsilon >0$ such that $y^\top A > \epsilon z$,
%which implies that $z^\top B >z$, where $B=\frac{A}{1+\epsilon}$.
%Since the largest eigenvalue of $B$ is $1/(1+\epsilon)<1$, we have
%that the limit $z^\top B^k =0$, which implies that $z<0$. However,
%we established the fact that $z>0$. Therefore, there is no such
%$y_i$, and $|x|$ is an eigenvector of $A$ of eigenvalue $1$. }

We have shown so far that there exists a $\mu$ such that $\mu^\top
P=\mu^\top$ and $\mu\geq 0$. This implies that $\mu/\|\mu\|_1$ is a steady state distribution.
Since $A=P^m$ is strictly positive,
then $\mu^\top=\mu^\top A >0$.

To show the uniqueness of $\mu$, assume we have $x$ and $y$ such
that $x^\top P=x^\top$ and $y^\top P=y^\top$ and $x\neq y$. Recall
that we showed that in such a case both $x>0$ and $y>0$. Then there
is a linear combination $z=ax+by$ such that for some $i$ we have
$z_i=0$. Since $z^\top P=z^\top$, we have showed that $z$ is
strictly positive, i.e., $z>0$, which is a contradiction. Therefore,
$x=y$, and hence $\mu$ is unique.
\end{proof}

We define the average fraction that a state $j\in X$ occurs, given
that we start with an initial state distribution $x_0$,  as follows:
\[
\pi^{(m)}_j=\frac{1}{m}\sum_{t=1}^m \I (X_t=j)
\]

\begin{theorem}
Let $({X_\ttime})$ be an irreducible and  a-periodic Markov chain
over a finite state space $X$ with transition matrix $P$. Let $\mu$
be the stationary distribution of $P$. Then, for any $j\in X$ we
have
\[
\mu_j=\lim_{m\rightarrow\infty} \E[\pi^{(m)}_j] =\frac{1}{\E[T_j]}
\]
\end{theorem}

\begin{proof}
We have that
\[
\E[\pi^{(m)}_j]=\E[\frac{1}{m}\sum_{t=1}^m \I
(X_t=j)]=\frac{1}{m}\sum_{t=1}^m \Pr
[X_t=j|X_0=x_0]=\frac{1}{m}\sum_{t=1}^m x_0^\top P^t e_j,
\]
where $e_j$ denotes a vector of zeros, with $1$ only in the $j$'s element. Let $v_1, \ldots , v_n$ be the eigenvectors of $P$ with eigenvalues
$\lambda_1 \geq  \ldots \geq  \lambda_n$. By
Theorem~\ref{Thm:MC-stationary} we have that $v_1=\mu$, the stationary
distribution and $\lambda_1=1>\lambda_i$ for $i\geq 2$. Rewrite
$x_0=\sum_i \alpha_i v_i$. Since $P^m$ is a stochastic matrix,
$x_0^\top P^m$ is a distribution, and therefore $\lim_{m\rightarrow
\infty} x_0^\top P^m=\mu$.
%We claim that $\alpha_1=1$.
%However, for any $v_i$ we have $v_i P^m \rightarrow 0$.

We will be interested in the limit $\pi_j=\lim_{m\rightarrow \infty}
\pi^{m}_j $, and mainly in the expected value $\E[\pi_j]$. From the
above we have that $\E[\pi_j]=\mu_j$.

A different way to express $\E[\pi_j]$ is using a variable time
horizon, with a fixed number of occurrences of $j$. Let $T_{k,j}$ be
the time between the $k$ and $k+1$ occurrence of state $j$. This
implies that
\[
\lim_{m\rightarrow\infty}\frac{1}{m}\sum_{t=1}^m \I (X_t=j) =
\lim_{n\rightarrow\infty} \frac{n}{\sum_{k=1}^n T_{k,j}}
\]
Note that the $T_{k,j}$ are i.i.d. and equivalent to $T_j$. By the
law of large numbers we have that $\frac{1}{n}\sum_{k=1}^n T_{k,j}$
converges to $\E[T_i]$. Therefore,
\[
\E[\pi_j]=\frac{1}{\E[T_j]}
\]
\end{proof}

We have established the following general theorem.

\begin{theorem}[\textbf{Recurrence of finite Markov chains}]
Let $({X_\ttime})$ be an irreducible,  a-periodic Markov chain over
a finite state space $X$.  Then the following properties hold:
\begin{enumerate}
\item All states are \textbf{positive recurrent}
\item There exists a \textbf{unique stationary distribution}
${\mu^*}$, where $\mu^*(i)=1/\E[T_i]$.
\item \textbf{Convergence} to the stationary distribution: ${\lim _{t \to \infty }}p_{i,j}^{(t)} = {\mu_j}\quad (\forall j)$
\item \textbf{Ergodicity}: For any finite $f$: ${\lim _{t \to \infty }}\frac{1}{t}\sum\nolimits_{s = 0}^{\ttime - 1} {f({X_s}) = \sum\nolimits_i {\mu_i f(i) \buildrel \Delta \over = } } \,\pi  \cdot f.$
\end{enumerate}
\end{theorem}

%\begin{proof}
%Item (1): It cannot be that all the states are transient, since then
%all the states will appear only a finite number of times, while the
%run is unbounded. This implies that all the states are recurrent.
%[Need to show {\em positive recurrent}: $\E[T_i]<\infty$]
%
%%Since the Markov chain is irreducible and a-periodic,
%
%Item (2): We will show that for each state $i$ we have
%$\mu^*(i)=1/\E[T_i]$. Let $Z_{i,k}$ be the time from the $k-1$ time
%
%Item (3):
%
%Item (4):
%\end{proof}

For countable Markov chains, there are other possibilities.

\begin{theorem}[\textbf{Countable Markov chains}]
\label{thm:countableMC}
Let $({X_\ttime})$ be an irreducible and a-periodic Markov chain
over a countable state space $X$.  Then:
%\begin{enumerate}
%\item
Either (i) all states are positive recurrent, or (ii) all states are null recurrent, or (iii) all states are transient.
%\item If (i) holds, then properties (2)-(4) of the previous Theorem hold as well.
%\item Conversely, if there exists a stationary distribution $\mu $ then properties (1)-(4) are satisfied.
%\end{enumerate}
\end{theorem}


\begin{proof}
%Item (1):
Let $i$ be a positive recurrent state, then we will show that all
states are positive recurrent. For any state $j$, since the Markov
chain is irreducible, we have for some $m_1,m_2\geq 0$ that
$p^{(m_1)}_{j,i},p^{(m_2)}_{i,j}>0$. This implies that
the return
time to state $j$ is at most $\E[T_j]\leq
1/p^{(m_1)}_{j,i}+\E[T_i]+1/p^{(m_2)}_{i,j}$, and hence $j$ is
positive recurrent.

If there is no positive recurrent state, let $i$ be a null recurrent
state, then we will show that all states are null recurrent. For any
state $j$, since the Markov chain is irreducible, we have for some
$m_1,m_2\geq 0$ that $p^{(m_1)}_{j,i},p^{(m_2)}_{i,j}>0$. This implies that  $\sum_{m=0}^\infty p^{(m)}_{j,j}=\infty$
%
%the probability to return time to state $j$ 
%
is at least
$\sum_{m=0}^\infty
p^{(m_1)}_{j,i}\;p^{(m)}_{i,i}\;p^{(m_2)}_{i,j}=\infty$, since we
have $\sum_{m=0}^\infty p^{(m)}_{i,i}=\infty$, since $i$ is a
recurrent state. This implies that $j$ is a recurrent state. Since
there are no positive recurrent states, it has to be that $j$ is a
null recurrent state.

If there are no positive or null recurrent states, then all states
are transient.
%
%Item (2):
%
%Item (3):
%
\end{proof}

%\begin{leftbar}
\paragraph{Reversible Markov chains:} Suppose there exists a probability vector $\mu  = ({\mu_i})$ so that
\begin{equation}\label{eq:DB}
{\mu _i}{p_{i,j}} = {\mu _j}{p_{j,i}},\quad \quad i,j \in X.
\end{equation}
It is then easy to verify by direct summation that $\mu $ is an
invariant distribution for the Markov chain defined by
$({p_{i,j}})$. This follows since $\sum_i \mu_i p_{i,j}=\sum_i
p_{j,i}\mu_j=\mu_j$. The equations \eqref{eq:DB} are called the
\emph{detailed balance equations}. A Markov chain that satisfies
these equations is called \emph{reversible}.

\begin{example}[\textbf{Discrete-time queue}] Consider a discrete-time queue, with queue length $X_\ttime\in \mathbb{N}_0=\{0,1,2,\dots\}$. At time  $\ttime$, ${A_\ttime}$ new jobs arrive, and then up to ${S_\ttime}$ jobs can be served, so that
\[{X_{\ttime +1}} = {({X_\ttime} + {A_\ttime} - {S_\ttime})^ + }.\]
Suppose that $({S_\ttime})$ is a sequence of i.i.d. RVs, and
similarly $({A_\ttime})$ is a sequence of i.i.d. RVs, with
$({S_\ttime})$, $({A_\ttime})$ and ${X_0}$ mutually independent. It
may then be seen that $({X_\ttime},\;\ttime \ge 0)$ is a Markov chain.
Suppose further that each ${S_\ttime}$ is a Bernoulli RV with
parameter $q$, namely $P({S_\ttime} = 1) = q$, $P({S_\ttime} = 0) =
1 - q$. Similarly, let ${A_\ttime}$ be a Bernoulli RV with parameter
$p$. Then
\[{p_{i,j}} = \left\{ {\begin{array}{*{20}{l}}
{p(1 - q)}&:&{j = i + 1}\\
{(1 - p)(1 - q) + pq}&:&{j = i,\;\;i > 0}\\
{(1 - p)q}&:&{j = i - 1,\;\;i > 0}\\
{(1 - p) + pq}&:&{j = i = 0}\\
0&:&{{\rm{otherwise}}}
\end{array}} \right.\]
%YM: Need to change mu to eta since mu is the steady state
 Denote $\lambda  = p(1 - q)$, $\eta  = (1 - p)q$, and $\rho  = \lambda /\eta $.   The detailed balance equations for this case are:
\[
\mu_i p_{i,i+1}={\mu _i}\lambda  = {\mu _{i + 1}}\eta=\mu_{i+1}p_{i+1,i} ,\quad \quad \forall i \ge 0\]
These equations have a solution with $\sum {_i{\mu _i} = 1}$ if and
only if $\rho  < 1$. The solution is ${\mu _i} = {\mu _0}{\rho ^i}$,
with ${\mu _0} = 1 - \rho $. This is therefore the stationary
distribution of this queue.
\end{example}
%\end{leftbar}


\paragraph{Mixing time:} %Add definition and motivation
The mixing time measures how fast the Markov chain converges to the steady state distribution. We first define the Total Variation distance between distributions $D_1$ and $D_2$ as:
\[
\|D_1-D_2\|_{TV}=\frac{1}{2}\sum_{x\in X} | D_1 (x) - D_2(x) |
\]
The mixing time $\tau$ is defined as the time to reach a total variation of at most $1/4$:
\[
\|s_0 P^{\tau}-\mu\|_{TV}=\|p^{(\tau)}-\mu\|_{TV}\leq \frac{1}{4}\| s_0-\mu\|_{TV}
\]
where $\mu$ is the steady state distribution and $p^{(\tau)}$ is the state distribution after $\tau$ steps starting with an initial state distribution $s_0$.

Note that after $2\tau$ time steps we have 
\[
\|s_0 P^{2\tau}-\mu\|_{TV}=\|p^{(\tau)}P^{\tau}-\mu\|_{TV}\leq \frac{1}{4}\| p^{(\tau)}-\mu\|_{TV}\leq \frac{1}{4^2}\| s_0-\mu\|_{TV}.
\]
In general, after $k\tau$ time steps we have 
\[
\|s_0 P^{k\tau}-\mu\|_{TV}=\|p^{((k-1)\tau)}P^{\tau}-\mu\|_{TV}\leq \frac{1}{4}\| p^{((k-1)\tau)}-\mu\|_{TV}\leq \frac{1}{4^k}\| s_0-\mu\|_{TV}.
\]
where the formal proof is by induction on $k\geq1$.

