%\chapter{Online MDP}

In this chapter we extend the online Multi-Arm Bandit model to the general online MDP model and derive regret bounds.

We consider a finite horizon problem, where the horizon is $\tHorizon$, and we will have multiple episodes, i.e., $\kEpisode$. 
At each episode $1\leq \ktime\leq \kEpisode$ the learner selects and executes a policy $\policy^\ktime$. The policy $\policy^\ktime$ generates in episode $\ktime$ a trajectory 
$\tau^\ktime=(\state^\ktime_1, \action^\ktime_1,\reward^\ktime_1, \ldots, \state^\ktime_{\tHorizon+1})$ 
which the leaner observes. The rewards $\reward^\ktime_\ttime$ is the random reward from action $\action^\ktime_\ttime$ in state $\state^\ktime_\ttime$. The state $\state^\ktime_{\ttime+1}$ is drawn from the next state distribution of preforming action $\action^\ktime_\ttime$ in state $\state^\ktime_\ttime$. Note that the learner observes only the rewards and next states along the trajectory $\tau^\ktime$.

Clearly, the optimal policy in all episodes is $\policy^*$, the optimal policy for the unknown MDP.
We need to define the objective of the learner, and as before we will use the notion of \emph{regret}. Namely, we will measure the performance by comparing the learner's cumulative reward to the optimal cumulative reward. The difference is the \emph{regret}. Our goal would be that the average regret would be vanishing and $\kEpisode$ goes to infinity (we think of the length of an episode $\tHorizon$ as fixed, and potentially small).
Formally we define the regret as follows.
\[
\text{Regret}=\max_{\policy}
\sum_{\ktime=1}^{\kEpisode}\sum_{\ttime=1}^{\tHorizon}\underbrace{\reward^{\ktime}_{\ttime}}_{\text{Random variable}}-
\sum_{\ktime=1}^{\kEpisode}\sum_{\ttime=1}^{\tHorizon}\underbrace{\reward^{\ktime}_{\ttime}}_{\text{Random variable}}
\]
We would concentrate on the Pseudo Regret, which is the expected regret. Namely, it  compares the learner's expected cumulative reward, over all episodes and rime steps, to the  expected cumulative reward of the optimal policy. 
\[
\begin{array}{ccc}
\text{Pseudo Regret} & = & \sum_{\ktime=1}^{\kEpisode}\E\left[ \sum_{\ttime=1}^{\tHorizon}\reward^\ktime_{\ttime}| \policy^*\right]-\sum_{\ktime=1}^{\kEpisode}\E\left[ \sum_{\ttime=1}^{\tHorizon}\reward_{\ttime}^\ktime|\policy^\ktime\right]
\end{array}
\]
where $\policy^*$ is the optimal policy for the MDP, and $\policy^\ktime$ is the learner's policy in episode $\ktime$.
The expectation $\E\left[\reward_{\ttime}| \policy\right]$, is the expected reward at step $\ttime$ of the horizon, given we execute policy $\policy$. The randomness is both over the the randomness of the rewards and the state distribution.

As in the multi-arm bandit case, our goal is that the regret would be sub-linear, i.e., $o(\tHorizon\kEpisode)$. In general the dependency on the number of episodes $\kEpisode$ would be much more interesting, and the main goal is that the regret would be sublinear in the number of episodes $\kEpisode$. The logic is that we can prolong the learning by increasing the number of episodes. In contrast, the horizon $\tHorizon$ define the task objective, and it should remain fixed.

%Note that the difference between the regret and the Pseudo Regret is related to the difference between taking the expected maximum (in Regret) versus the maximum expectation (Pseudo Regret). In this chapter we will only consider pseudo regret (and sometime call it simply regret). 


\section{Reduction to MAB: Policies as actions}

We start with a simple reduction from MDP to MAB. The main drawback would be an exponential dependency on the number of states.

The reduction from the MDP to a MAB is to simply consider the deterministic polices as the actions in a MAB.
Let us be slightly more formal. 

Consider an MDP $M=(\States,\Actions,\transitionkernel,\reward,\state_0)$, with a finite horizon return $\tHorizon$.
Let $\Pi=\{\policy\}$ be the collection of all deterministic policies of $M$. For each policy  $\pi\in \Pi$ we define a stochastic reward which is the reward of a trajectory generated by $\pi$. Namely, we sample a trajectory $\tau^\policy=(\state_1, \action_1,\reward_1, \ldots, \state_{\tHorizon+1})$ following the policy $\policy$, and set the reward to be  $R^\policy=(1/\tHorizon)\sum_{\ttime=1}^\tHorizon \reward_\ttime$. Clearly, $\Value^\policy(\state_1)=\E[R^\policy]$.

We can now define a MAB instance. The set of actions is the set of deterministic policies, i.e., $\Pi$. The reward of an action $\policy\in\Pi$ is $R^\policy$. Note that $R^\policy\in[0,1]$ since $\reward_\ttime\in[0,1]$.

We can now use either Successive Action Elimination (Theorem~\ref{thm:MAB:SE2}) or Upper Confidence Bound (Theorem~\ref{thm:MAB:UCB2}). This will establish the following:

\begin{theorem}
Running successive action elimination or UCB over the set of deterministic policies $\Pi$ has pseudo regret bounded by $O( \tHorizon \sqrt{|\Pi|\kEpisode}\log\kEpisode)$ over $\kEpisode$ episodes.
\end{theorem}

While the above bound has the right dependency on the number of episodes, $\sqrt{\kEpisode}$, the number of deterministic policies is huge. We have $|\Pi|={|\Actions|}^{|\States|}$. Our goal is to have a dependency that grows polynomially with the number of states and actions and not exponentially.
(The dependency on $\tHorizon$ is due to the fact that we normalized the MAB rewards by dividing by $\tHorizon$.)


\section{Explore-Then-Exploit Algorithms}

We will start by examining simple Explore-Then-Exploit algorithms.
The goal of the episodes in the explore phase is to gather information regarding the MDP, so that following the explore phase the learner would be able to identify an near optimal policy. In the exploit phase we would use the near optimal policy we identified in the explore phase and use it.

We can define the exploration in a few ways. The most naive way is to simply regard the different deterministic policies as ``actions" (as we did in the reduction to MAB). This will enable an immediate reduction to the multi-arm bandit setting. The down side is that the number of policies, which is now the number of actions, is huge.

A more sophisticated approach is to use the exploration to identified a model-based learning, that would return a near accurate model of the MDP. Given this model we can learn a near optimal policy. We will use the results of Chapter \ref{chapter-model-based} in order to learn a good approximate model for the MDP.

Using the Explicit Explore or Exploit (EEE) algorithm, by  Theorem~\ref{thm:MBRL:EEE} we have an exploration phase of length $O(\tHorizon^5 |\States|\;|\Actions|\frac{1}{\epsilon^3})$. Following the exploration phase, we can select a policy which is $O(\epsilon\tHorizon)$-optimal.
This implies that the resulting pseudo regret bound is
\[\
PseudoRegret=O( \tHorizon^5 |\States|\;|\Actions|\frac{1}{\epsilon^3}+\epsilon\tHorizon\kEpisode)
\]
Minimizing over $\epsilon$ we have that for $\epsilon=\tHorizon |\States|^{1/4}|\Actions|^{1/4} \kEpisode^{-1/4}$ we have that the pseudo regret is bounded as 
\[\
PseudoRegret=O( \tHorizon^2 |\States|^{1/4}\;|\Actions|^{1/4}\kEpisode^{3/4})
\]

We have established the following

\begin{theorem}
Running the EEE algorithm as an explore phase and then a near optimal policy on the empirical model has pseudo regret bounded by $O( \tHorizon^2 |\States|^{1/4}\;|\Actions|^{1/4}\kEpisode^{3/4})$ over $\kEpisode$ episodes.
\end{theorem}

Note that the main benefit is that now we have a polynomial dependence on the number of actions and states, unlike the case of direct reduction to MAB. The downside is that the dependence on the number of episodes, although sublinear, $O(\kEpisode^{3/4})$, it is far from the optimal regret bound of $O(\kEpisode^{1/2})$.

%\subsection{Learning dynamics first}

%\section{From Multi-Arm Bandits to MDPs}

\section{Unknown rewards, known dynamics}

Much of the techniques used in the case of Multi-arm bandits, can be extended naturally to the case of MDPs. In this section we sketch a simple extension where the dynamics of the MDPs is known, but the rewards are unknown. 

We first need to define the model for the online learning in MDPs, which will be very similar to the one in MAB. We will concentrate on the case of a finite horizon return. The learner interacts with the MDP for $K$ episodes.

At each episode $\ttime\in[K]$, the learner selects a policy $\policy_\ttime$ and observes a trajectory $(\state^\ttime_1, \action^\ttime_1,\reward^\ttime_1 \ldots , \state_\tHorizon^\ttime)$, where the actions are selected using $\policy_\ttime$, i.e., $\action_\tau^\ttime=\policy_\ttime(\state_\tau^\ttime)$.

The goal of the learner is to minimize the pseudo regret. Let $\Value^*(\state_1)$ be the optimal value function from the initial state $\state_1$. The pseudo regret is define as,
\[
\E[Regret] = \E[\sum_{\ttime\in[K]}\Value^*(\state_1)-\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
\]

We now like to introduce a UCB-like algorithm. We will first assume that the learner knows the dynamics, but does not know the rewards. This will imply that the learner, given a reward function, can compute an optimal policy.

Let $\mu_{\state,\action}=\E[r_{\state,\action}]$ be the expected reward for $(\state,\action)$.
%
As in the case of UCB we will define an Upper Confidence Bound for each reward. Namely, for each state $\state$ and action $\action$ we will maintain an empirical average $\hat{\mu}^\ttime_{\state,\action}$ and a confidence parameter $\lambda^\ttime_{\state,\action}=\sqrt{\frac{2\log KSA}{n_{\state,\action}^\ttime}}$, where $n_{\state,\action}^\ttime$ is the number of times we visited state $\state$ and performed action $\action$.

We define the good event similar to before
\[
G=\{\forall\state,\action, \ttime\; |\hat{\mu}^\ttime_{\state,\action}-\mu_{\state,\action}|\leq \lambda_{\state,\action}^\ttime\}
\]
and similar to before, we show that it holds with high probability, namely $1-\frac{2}{K^2}$.

\begin{lemma}
    We have that  $\Pr[G]\geq 1-\frac{2}{K^2}$.
\end{lemma}

\begin{proof}
    Similar to the UCB analysis using Chernoff bounds.
\end{proof}

We now describe the UCB-RL algorithm. For each episode $\ttime$ we compute a UCB for each state-action, denote the resulting reward function by $\bar{R}^\ttime$. Recall that $\bar{R}^\ttime(\state,\action)=\hat{\mu}_{\state,\action}^\ttime+\lambda_{\state,\action}^\ttime$.  Let $\policy^\ttime$ the optimal policy with respect to the rewards $\bar{R}^\ttime$ (the UCB rewards).

The following lemma shows that we have ``optimism'', namely the expected value of $\policy^\ttime$ w.r.t. the reward function $\bar{R}^\ttime$ upper bounds the optimal reward function $\Value^*$.

In the following we use the notation $\Value(\cdot|R)$ to imply that we are using the reward function $R$. We denote by $R^*$ the true reward function, i.e., $R^*(\state,\action)=\E[r_{\state,\action}]$.

\begin{lemma}
    Assume the good event $G$ holds. Then, for any episode $\ttime$ we have that $\Value^{\policy^\ttime}(\state | \bar{R}^\ttime)\geq \Value^*(s|R^*)$.
    %, where $R^*$ is the true reward function.
    %$\E[\sum_{\tau=1}^\tHorizon R^\ttime$
\end{lemma}

\begin{proof}
    Since $\policy^\ttime$ is optimal for the rewards $\bar{R}^\ttime$, we have that $\Value^{\policy^\ttime}(\state | \bar{R}^\ttime)\geq \Value^{\policy^*}(\state | \bar{R}^\ttime)$.
    
Since $\bar{R}^\ttime\geq R^*$, then we have 
$\Value^{\policy^*}(\state | \bar{R}^\ttime)\geq \Value^{\policy^*}(\state | R^*)$.

Combining the two inequalities, yields the lemma.
\end{proof}

The optimism is very powerful property, as it let's us bound the pseudo regret as a function of quantities we observe, namely $\bar{R}^\ttime$, rather than unknown quantities, such as the true rewards $R^*$ or the unklnown optimal policy $\policy^*$.

\begin{lemma}
\label{MAB:lemma:UCB-RL:optimism}
Assume the good event $G$ holds. Then,
    \[
    \E[Regret] \leq \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
    \E[2\lambda^\ttime_{\state^\ttime_\tau,\action^\ttime_\tau}]
    \]
\end{lemma}

\begin{proof}
    The definition of the pseudo regret is
    \[
    \E[Regret]= \E[\sum_{\ttime\in [K]} \Value^*(\state_1)-\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
    \]

Using Lemma~\ref{MAB:lemma:UCB-RL:optimism}, we have that,
    \[
    \E[Regret]= \E[\sum_{\ttime\in [K]} \Value^*(\state_1)-\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
    \leq 
    \sum_{\ttime\in [K]} 
    \E[\Value^{\policy^\ttime}(\state_1|\bar{R}^\ttime)
    -\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}]
    %\Value^{\policy^\ttime}(\state_1|R^*)]
    \]
Since the good event $G$ holds, we have [[YM: needed??]]
\[
\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} \geq \sum_{\tau=1}^\tHorizon 
 \hat{\mu}_{\state_\tau^\ttime,\action_\tau^\ttime}-\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime}
\]
Note that
\[
\E[\sum_{\tau=1}^\tHorizon \reward^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} ] =\E[\Value^{\policy^\ttime}(\state_1|R^*)]
\]
and we have,
\[
\E[Regret]\leq \E[\Value^{\policy^\ttime}(\state_1|\bar{R}^\ttime)] -
\E[\Value^{\policy^\ttime}(\state_1|R^*)]=
\E[\sum_{\tau=1}^\tHorizon \lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} ]
\]
which compltes the proof of the lemma.
%Combining all the identities yields the lemma.
\end{proof}

We are now left with only upper bounding the sum of the confidence bounds.
We can upper bound this sum regardless of the realization. [[YM: hopefully correct]]

\begin{lemma}
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
   % \sqrt{\frac{2\log KSA}{n_{\state,\action}^\ttime}}
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} 
    \leq \sqrt{KSA\log (KSA)}
    \]
\end{lemma}

\begin{proof}
    We first change the order of summation to be over state-action pairs.
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} =
\sum_{\state,\action} \sum_{\tau=1}^{n_{\state,\action}^K}
\sqrt{\frac{2\log KSA}{\tau}}
    \]
In the above, $\tau$ is the index of the $\tau$-th visit to the state-action pair $(\state,\action)$ at some time $\ttime$. During that visit we have that  $n_{\state,\action}^\ttime=\tau$.
This explain the expression for the confidence intervals.
    
Since $1/\sqrt{x}$ is a convex function, we can upper bound the sum using Jensen inequality, and have $\sum_{\tau=1}^N 1/\sqrt{\tau}\leq \sqrt{2N}$, and have 
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} \leq
\sqrt{2\log KSA}
\sum_{\state,\action}
\sqrt{2 n_{\state,\action}^K}
    \]
Recall that $\sum_{\state,\action} n_{\state,\action}^K =K$. This implies that $\sum_{\state,\action}
\sqrt{2 n_{\state,\action}^K}$ is maximized when all the $n_{\state,\action}^K$ are equal, i.e, $n_{\state,\action}^K=K/(SA)$. Hence,
    \[
    \sum_{\ttime\in [K]} \sum_{\tau=1}^\tHorizon
\lambda^\ttime_{\state_\tau^\ttime,\action_\tau^\ttime} \leq
2
\sqrt{SAK\log KSA}
    \]
\end{proof}

We can now derive the upper bound on the pseudo regret
\begin{theorem}
    \[
    \E[Regret] \leq 2\sqrt{KSA\log (KSA)}
    \]
\end{theorem}

\section{Unknown dynamics}

\subsection{Confidence sets}


\subsection{Bonuses}