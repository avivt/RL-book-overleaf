\section{Temporal Difference algorithms}
\label{sec:TD}


%In this lecture we will continue looking at model-free learning. In
%the previous lecture we presented: (1) $Q$-learning, which is an
%off-policy learning algorithm that learns the optimal $Q^*$
%function, (2) {\tt SARSA} which is an on-policy variant of
%$Q$-learning where we need to select actions, and (3) Monte-Carlo
%which is a model-free algorithm to learn the value function from
%episodes.

In this section, we will look at temporal differences methods, which
work in an online fashion. We will start with $TD(0)$, which uses
only the most recent observations for the updates, and we will
continue with methods that allow for a longer look-ahead, and then
consider $TD(\lambda)$ which averages multiple look-ahead
estimations.

In general, temporal differences (TD) methods learn directly from
experience and therefore are model-free methods. Unlike Monte-Carlo
algorithms, they are not restricted to episodic MDPs, thus,
will use incomplete episodes for the updates.
The TD methods are similar in spirit to $Q$-learning and SARSA since
they update their estimates given the current observation.

\subsection{TD(0)}
Fix a stationary and deterministic policy, $\policy\in \Pi_{SD}$. 
The goal is to learn the value function $\Value^\policy(\state)$ for every
$s\in S$ (the same goal as Monte-Carlo learning). The TD algorithms
maintain an estimate $\widehat{\Value}(\state)$ of the value function
$\policy$, $\Value^\policy(\state)$, and use it for the updates. 
This implies that, unlike Monte-Carlo, there will be an interaction between the estimates of
different states at different times.

From the Bellman equation 
we have the following relation between the value of the current state $\state_{\ttime}$
and the value of the next state $\state_{\ttime+1}$
\[\Value(\state_{\ttime})=
\E^\policy[\reward(\state_{\ttime},\action_{\ttime})+\discount\Value(\state_{\ttime+1})]\]
Given an observed sample $(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$, 
and an estimated value function $\widehat{\Value}$ 
a reasonable estimation for the value of the current state $\state_{\ttime}$ 
based on the next state $\state_{\ttime+1}$(bootstrapping) is
\[
\widehat{\Value}(\state_{\ttime}) \approx \reward_{\ttime}+\discount\widehat{\Value}(\state_{\ttime+1})
\]
The idea in TD(0), is to iteratively update our estimated value function $\widehat{\Value}(\state)$
using the next state observed. The algorithm performs the following update
\[
\widehat{\Value}(\state_\ttime)\leftarrow\widehat{\Value}(\state_\ttime)+\alpha_\ttime(\state_\ttime,\action_\ttime)
\big[\reward_\ttime+\discount
\widehat{\Value}(\state_{\ttime+1})-\widehat{\Value}(\state_\ttime)\big]
\]
where $\alpha_\ttime(\state_\ttime,\action_\ttime)$ is the step size,
which will be discussed later in this chapter.

\paragraph{$TD(0)$ Algorithm}
\begin{itemize}
\item \emph{Initialize:} $\widehat{\Value}(\state)$ arbitrarily.
\item At time $\ttime=0,1,\dots$:
-- Observe: $(\state_\ttime, \action_\ttime, \reward_\ttime,
\state_{\ttime+1})$.
-- Update:
%$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$:
\[
\widehat{\Value}_{\ttime+1}(\state_\ttime)=\widehat{\Value}_{\ttime} (\state_\ttime)+\alpha_\ttime(\state_\ttime,\action_\ttime)
\big[\reward_\ttime+\discount
\widehat{\Value}_{\ttime}(\state_{\ttime+1})-\widehat{\Value}_{\ttime}(\state_\ttime)\big]
\]
where $\alpha_\ttime(\state,\action)$ is the step size for
$(\state,\action)$ at time $\ttime$.
\end{itemize}
We define the {\em temporal difference} to be
\[
\Delta_\ttime =\reward_\ttime+\discount
\widehat{\Value}_{\ttime}(\state_{\ttime+1})-\widehat{\Value}_{\ttime}(\state_\ttime)
\]
And the $TD(0)$ update becomes:
\[
\widehat{\Value}_{\ttime+1}(\state_\ttime)=\widehat{\Value}_{\ttime}(\state_\ttime)+\alpha_\ttime(\state_\ttime,\action_\ttime)\Delta_\ttime
\]


% [[YM: motivating example. Waze?!]]

We would like to compare the $TD(0)$ and the Monte-Carlo (MC)
algorithms. Here is a simple example with four states
$S=\{A,B,C,D\}$ where $\{C,D\}$ are terminal states and in $\{A,B\}$
there is one action (essentially, the policy selects a unique
action). Assume we observe eight episodes. One episode is
$(A,0,B,0,C)$, one episode $(B,0,C)$, and six episodes $(B,1,D)$. We
would like to estimate the value function of the non-terminal
states. For $V(B)$ both $TD(0)$ and $MC$ will give $6/8=0.75$. The
interesting question would be: what is the estimate for $A$? MC will
average only the trajectories that include $A$ and will get $0$
(only one trajectory which gives $0$ reward). The $TD(0)$ will
consider the value from $B$ as well and will give an estimate of
$0.75$. (Assume that the $TD(0)$ continuously updates using the same
episodes until it converges.)

We would like to better understand the above example. For the above
example, the empirical MDP will have a transition from $A$ to $B$,
with probability $1$ and reward $0$, from $B$ we will have a
transition to $C$ with probability $0.25$ and reward $0$ and a
transition to $D$ with probability $0.75$ and reward $1$. (See,
Figure~\ref{fig:L7-ML}.) The value of $A$ in the empirical model is
$0.75$. In this case, the empirical model agrees with the $TD(0)$
estimate, we show that this holds in general.

\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{centering}
  \includegraphics[width=0.5\textwidth]{figures/L7-ML}\\
  \caption{The situated agent}\label{fig:L7-ML}
  \end{centering}
\end{figure}

\paragraph{Maximum Likelihood model}\footnote{YM: repeat} 
Recall that the empirical model of a sample, 
which is also the maximum likelihood model, is defined as follows. Let
$n(\state,\action)$ be the number of times $(\state,\action)$
appears in the sample. Given a sample
$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$
for $1\leq t\leq T$, we define the empirical model as follows. The
rewards $\widehat{\reward}(\state,\action)$ are the average rewards
of $(\state,\action)$, i.e.,
\[\widehat{\reward}(\state,\action)=\frac{1}{n(\state,\action)}\sum_{\ttime:\state_\ttime=\state,\action_\ttime=\action}\reward_\ttime\]
and
\[\widehat{\transitionprob}(\state'|\state,\action)=\frac{n(\state,\action,\state')}{n(\state,\action)}\]
where $n(\state,\action,\state')$ is the number of samples that have
$\state_{\ttime+1}=\state'$ when $\state_\ttime=\state$ and
$\action_\ttime =\action$.

The following theorem states that the value of the policy $\policy$ on 
the empirical model is identical to that of $TD(0)$ (running on the
sample until convergence, namely, continuously sampling uniformly
$t\in[1,T]$ and using $(\state_\ttime,
\action_r,\reward_\ttime,\state_{\ttime+1})$ for the $TD(0)$
update).

\begin{theorem}
Let $\Value^\policy_{TD(0)}$ be the estimated value function of
$\policy$ when we run $TD(0)$ until convergence. Let
$\Value^\policy_{EM}$ be the value function of $\policy$ on the
empirical model. Then, \[\Value^\policy_{TD(0)}=\Value^\policy_{EM}\]
\end{theorem}

\begin{proof}[Proof sketch]
The update of $TD(0)$ is
\[\widehat{\Value}_{\ttime+1}(\state_\ttime)=\widehat{\Value}_{\ttime}(\state_\ttime)+\alpha_\ttime(\state_\ttime,\action_\ttime)\Delta_\ttime\]
where $\Delta_\ttime=\reward_\ttime+\discount
\widehat{\Value}_{\ttime}(\state_{\ttime+1})-\widehat{\Value}_{\ttime}(\state_\ttime)$. At
convergence we have $E[\Delta_\ttime]=0$ and hence,
\[
\widehat{\Value}_{\ttime+1}(\state)=\frac{1}{n(\state,\action)}\sum_{\state_{\ttime+1}:\state_\ttime=\state,\action_\ttime=\action}
\reward_\ttime+\discount
\widehat{\Value}_{\ttime}(\state_{\ttime+1})=\widehat{\reward}(\state,\action)+\discount
E_{\state'\sim
\widehat{\transitionprob}(\cdot|\state,\action)}[\widehat{\Value}_{\ttime}(\state')]
\]
where $\action=\policy(\state)$.
\end{proof}

It is worth comparing the above theorem to the case of Monte Carlo
(Theorem~\ref{thm:MC-ML}). Here we use the entire sample and
have the same ML model for any state $\state$. In the Monte-Carlo
case, we used a reduced sample, which depends on the state $\state$
and therefore, we have a different ML model for each state based on
its reduced sample.

\paragraph{Convergence:} The proof of the convergence of $TD(0)$ is very
similar to that of $Q$-learning. We will show that $TD(0)$ is an
instance of the stochastic approximation algorithm, as presented
previously in Section~\ref{sec:stochastic-approximation}, and the
convergence proof will follow from this.
%We will first state it.

\begin{theorem}[Convergence $TD(0)$]
\label{thm:TD0-conrg} If the step size has the properties that for
every $(\state,\action)$ we have $\sum_\ttime
\alpha_\ttime(\state,\action)=\infty $ and $\sum_\ttime
\alpha_\ttime^2(\state,\action)=O(1)$, then $\widehat{\Value}$ converges
to $\Value^\policy$, with probability $1$.
\end{theorem}

We will show the convergence using the general theorem for
stochastic approximation iterative algorithm
(Theorem~\ref{thm:stoch-approx}).

We first define a linear operator $H$ for the policy $\policy$,
\[
(Hv)(\state)=\reward(\state,\policy(\state))+\discount\sum_{\state'}\transitionprob(\state'|\state,\policy(\state))v(\state')
\]
Note that $H$ is the operator $T^\policy$ we define in
Section~\ref{ss:DP_op}.

As we saw before, the operator $H$ is a $\discount$-contracting, namely,
operator
\begin{align*}
\|Hv_1-Hv_2\|_\infty &= \discount \max_{s} |
\sum_{\state'}\transitionprob(\state'|\state,\policy(\state))(v_1(\state')-v_2(\state'))|\\
&\leq \discount\max_{\state'} |v_1(\state')-v_2(\state')|\\
&\leq \discount \|v_1-v_2\|_\infty
\end{align*}
%Therefore $H$ is $\discount$-contracting.

We now want to rewrite the $TD(0)$ update to be a stochastic
approximation iterative algorithm. The $TD(0)$ update is,
\[
V_{\ttime+1}(\state_\ttime)=(1-\alpha_\ttime)V_\ttime(\state_\ttime)+\alpha_\ttime
\Phi_\ttime
\]
where $\Phi_\ttime=\reward_\ttime+\discount
V_\ttime(\state_{\ttime+1})$. We would like to consider the expected
value of $\Phi_\ttime$. Clearly,
$E[\reward_\ttime]=\reward(\state_\ttime,\policy(\state_\ttime))$
and $\state_{\ttime+1} \sim \transitionprob(\cdot |\state_\ttime,\action_\ttime)$.
This implies that $E[\Phi_\ttime]=(HV_\ttime)(\state_\ttime)$.
Therefore, we can define the noise term $\omega_\ttime$ as follows,
\[
\omega_\ttime(\state_\ttime)=[\reward_\ttime+\discount
V_\ttime(\state_{\ttime+1})]-(HV_\ttime)(\state_\ttime)\;,
\]
and have $E[\omega_\ttime]=0$. We can bound $|\omega_\ttime|\leq
V_{max}=\frac{R_{max}}{1-\discount}$, since the value function is
bounded by $V_{max}$.

Returning to $TD(0)$, we can write
\[
V_{\ttime+1}(\state_\ttime)=(1-\alpha_\ttime)V_\ttime(\state_\ttime)+\alpha_\ttime
[(HV_\ttime)(\state_\ttime)+\omega_\ttime(\state_\ttime)]
\]
The requirement of the step size appears in the theorem statement (and so holds). 
The noise $\omega_\ttime$ has both
$E[\omega_\ttime]=0$ and $|\omega_\ttime|\leq V_{max}$. The
operator $H$ is $\discount$-contracting with a fix-point $\Value^\policy$.
Therefore, using Theorem~\ref{thm:stoch-approx}, we established
Theorem~\ref{thm:TD0-conrg}.

\paragraph{Comparing various algorithms:}\footnote{YM: needed? I think it is from Sutton}
When we compare $TD(0)$, to $MC$ and both to  Dynamic Programming
(DP) we can view it as different ways of computing the value
function.
\begin{itemize}
\item[MC] We have $\Value^\policy(\state)=E[R_\ttime|\state_\ttime=\state]$. In MC we observe the
return, $R_\ttime$, of episodes, and their mean is what we like to
estimate.
\item [$TD(0)$]  We have $\Value^\policy(\state)=E[\reward_\ttime+\discount \Value^\policy(\state_{\ttime+1})|\state_\ttime=\state]$. In $TD(0)$ we observe samples of
$\reward_\ttime$, we use our estimate for
$\Value^\policy(\state_{\ttime+1})$ and the expectation is what we
like to estimate (assuming our estimates converge).
\item
[DP]
 We have $\Value^\policy(\state)=\sum_{\action}\policy(\action|\state)[\reward(\state,\action)+\discount \sum_{\state'}\transitionprob(\state'|\state,\action)\Value^\policy(\state')]$. In DP we
 have the entire model, and use it to compute expectations.
\end{itemize}

We can see the difference between $TD(0)$ and $MC$ in the Markov
Chain in Figure~\ref{fig:L7-TD-MC}. To get an approximation of state
$\state_2$, i.e., $|\widehat{\Value}(\state_2)-\frac{1}{2}|\approx
\varepsilon$. The Monte-Carlo will require $O(1/(\beta
\varepsilon^2))$ episodes (out of which only $O(1/\varepsilon^2)$
start at $\state_2$) and the $TD(0)$ will require only
$O(1/\varepsilon^2+1/\beta)$ since the estimate of $\state_3$ will
converge after $1/\varepsilon^2$ episodes which start from
$\state_1$.

\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{centering}
  \includegraphics[width=0.5\textwidth]{figures/L7-TD-MC}\\
  \caption{Markov Reward Chain}\label{fig:L7-TD-MC}
  \end{centering}
\end{figure}

\subsection{TD: Multiple look-ahead}

We have seen that $TD(0)$ updates the estimated value of state $\state_{\ttime}$ 
using a one-step lookahead, i.e., given
$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$ 
it updates 
\[\Delta_\ttime=R^{(1)}_\ttime(\state_\ttime) -
\widehat{\Value}_{\ttime}(\state_\ttime)\] 
where $R^{(1)}_\ttime(\state_\ttime)=
\reward_\ttime +\discount \widehat{\Value}_{\ttime}(\state_{\ttime+1})$.

\noindent We can also consider a two-step look-ahead as follows, given
$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1},\action_{\ttime+1},\reward_{\ttime+1},\state_{\ttime+2})$
we update using
\[\Delta^{(2)}_\ttime=R^{(2)}_\ttime(\state_\ttime) -
\widehat{\Value}_{\ttime}(\state_\ttime)\] 
where $R^{(2)}_\ttime(\state_\ttime)=
\reward_\ttime+\discount \reward_{\ttime+1} +\discount^2
\widehat{\Value}_{\ttime}(\state_{\ttime+2})$. 

\noindent Using the same logic, we can generalize this to any $n$-step look-ahead and define
\[\widehat{\Value}_{\ttime+1}(\state_\ttime)=\widehat{\Value}_{\ttime}(\state_\ttime)+\alpha_\ttime
\Delta^{(n)}_\ttime\] 
where $\Delta^{(n)}_\ttime=R^{(n)}_\ttime(\state_\ttime)-\widehat{\Value}_{\ttime}(\state_\ttime)$
and $R^{(n)}_\ttime(\state_\ttime)= \sum_{i=0}^{n-1} \discount^i
\reward_{\ttime+i}+\discount^n \widehat{\Value}_{\ttime}(\state_{\ttime+n})$ .

We can use any parameter $n$ for the $n$-step look-ahead. If the
episode ends before step $n$ we can pad it with rewards zero. This
implies that for $n=\infty$ we have that $n$-step look-ahead is
simply the Monte-Carlo estimate. However, we need to select some
parameter $n$. 

\noindent{\bf Remarks:}
\begin{enumerate}
    \item We can view $R^{(n)}_\ttime$ as an operator over $V$. This operator is
$\discount^n$-contracting, namely
\[
\| R^{(n)}_\ttime(V_1)-R^{(n)}_\ttime(V_2)\|_\infty \leq \discount^n
\|V_1-V_2\|_\infty
\]
\item There is a relation between $\Delta^{(n)}_\ttime$ and $\Delta_\ttime$ 
\[
\Delta^{(n)}_\ttime=\sum_{i=0}^{n-1} \discount^i \Delta_{\ttime+i}
\]
This follows since
\begin{align*}
\sum_{i=0}^{n-1} \discount^i \Delta_{\ttime+i}
=&\sum_{i=0}^{n-1}
\discount^i \reward_{\ttime+i}+ \sum_{i=0}^{n-1} \discount^{i+1}
\widehat{\Value}(\state_{\ttime+i+1})- \sum_{i=0}^{n-1} \discount^{i}
\widehat{\Value}(\state_{\ttime+i})\\
=&\sum_{i=0}^{n-1} \discount^i
\reward_{\ttime+i}+\discount^n
\widehat{\Value}(\state_{\ttime+n})-\widehat{\Value}(\state_\ttime)\\
=&R^{(n)}_\ttime(\state_\ttime)-\widehat{\Value}(\state_\ttime)=\Delta^{(n)}_\ttime
\end{align*}
\end{enumerate}

\subsection{TD($\lambda$)}
An alternative idea for selecting $n$, is to simply average 
over the possible values of $n$. One simple way to average is to use
exponential averaging with a parameter $\lambda\in(0,1)$. 
This implies that the weight of each parameter $n$ is
$(1-\lambda)\lambda^{n-1}$.
This leads us to the $TD(\lambda)$ forward view update.\\

\noindent{\bf Forward view update}\\
For each visited state, the update is determined by looking forward in time to all future rewards and states.
\[
\widehat{\Value}_{\ttime+1}(\state_\ttime)=\widehat{\Value}_{\ttime}(\state_\ttime)+\alpha_\ttime\Delta^{\lambda}_\ttime
\]
where $\Delta^{\lambda}_\ttime= (1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}\Delta^{(n)}_\ttime$

\noindent{\bf Remark:} While both $\discount$ and $\lambda$ are used
to generate exponential decaying values, their goals are very
different. The discount parameter $\discount$ defines the objective
of the MDP, the goal we like to maximize. The exponential
averaging parameter $\lambda$ is used by the learning algorithm to average over the different
look-ahead parameters and is selected to optimize the convergence.\\

%In the slides there is an example of a random walk and its
%convergence taken from \cite{SuttonB98}.
\noindent{\bf Backward view}\\
The above is a description of the forward view, in which we average future rewards.
If we attempt to implement it in a strict manner, 
we will have to wait until the end of the episode 
because we must first observe all the rewards. Fortunately,
an equivalent form of the $TD(\lambda)$ uses a {\em
backward view}. The backward view updates at each time step, using
incomplete information. And at the end of the episode, 
both forward and backward updates will be the same.

The basic idea of the backward view is the following. Fix a time
$\ttime$ and a state $\state=\state_\ttime$. We have at time
$\ttime$ a temporal difference $\Delta_\ttime= \reward_\ttime
+\discount V_\ttime(\state_{\ttime+1})-V_\ttime(\state_\ttime)$.
Consider how this $\Delta_\ttime$ affects all the previous times
$\tau<\ttime$ where $\state_\tau=\state=\state_\ttime$. The influence is
exactly $(\discount\lambda)^{\ttime-\tau}\Delta_\ttime$. This implies
that for every such $\tau$ we can do the desired update. However, we
can aggregate all those updates into a single update. Let,
\[
e_\ttime(\state)=\sum_{\tau\leq \ttime: \state_\tau=\state}
(\discount\lambda)^{\ttime-\tau} = \sum_{\tau=1}^\ttime
(\discount\lambda)^{\ttime-\tau}I(\state_\tau=\state)
\]
The above $e_\ttime(\state)$ is defined as the {\em eligibility trace} and
we can compute it online using
\[
e_\ttime(\state)=\discount\lambda
e_{\ttime-1}(\state)+I(\state=\state_\ttime)
\]
Which result in the update
\[
\widehat{\Value}_{\ttime+1}(\state)=\widehat{\Value}_\ttime(\state)+\alpha_\ttime
e_\ttime(\state)\Delta_\ttime
\]
At any given time, these eligibility traces indicate which
states have been visited most recently, 
where "recently" is defined in terms of $\discount\lambda$. 
Intuitively, these traces indicate the extent to which 
each state $\state$ is qualified to undergo learning changes
in the event of a reinforcing event.

Note that for $\lambda=0$ the eligibility
trace becomes $e_\ttime(\state)=I(\state=\state_\ttime)$, and we obtain $TD(0)$. This
implies that we update only $\state_\ttime$ and
$\widehat{\Value}_{\ttime+1}(\state_\ttime)=\widehat{\Value}_\ttime(\state_\ttime)+\alpha_\ttime
\Delta_\ttime$.

\paragraph{Backward view algorithm}\ \\

 -- Initialization: Set $\widehat{\Value}(\state)=0$ (or any other value), and
 $e_0(\state)=0$.

 -- Update: observe $(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$ and set
\begin{align*}
\Delta_\ttime &= \reward_\ttime +\discount \widehat{\Value}_\ttime(\state_{\ttime+1})-\widehat{\Value}_{\ttime}(\state_\ttime)\\
e_\ttime(\state) &= \discount\lambda e_{\ttime-1}(\state)+I(\state=\state_\ttime)\\
\widehat{\Value}_{\ttime+1}(\state) &=
\widehat{\Value}_\ttime(\state)+\alpha_\ttime e_\ttime
(\state)\Delta_\ttime
\end{align*}

 To summarize, the
benefit of $TD(\lambda)$ is that it interpolates between $TD(0)$ and
Monte-Carlo updates and many times achieve superior performance to
both.
%
Similar to $TD(0)$. Also,  $TD(\lambda)$ can be written as a
stochastic approximation iterative algorithm, and one can derive its convergence.\footnote{YM: Do we want to add it? Maybe HW?}
%
In the next section, we show the equivalence of the forward and
backward $TD(\lambda)$ updates.
%

\subsection{The equivalence of the forward and backward view}

We would like to show that the forward and backward views result in the same overall update.\footnote{YM: should we move to the Harm van Seijen, Richard S. Sutton: True Online TD(lambda). ICML 2014: 692-700}

Consider the following intuition. Assume that state $\state$ occurs in time $\tau_1$. This occurrence will contribute to the forward view $\sum_{\ttime=0}^\infty \lambda^\ttime\discount^\ttime\Delta_{\tau_1+\ttime} $.
The same contribution applies to any time $\tau_j$ where state $\state$ occurs. The sum of those contributions would be $\sum_{j=1}^M \lambda^\ttime\discount^\ttime\Delta_{\tau_j+\ttime} $, where $M$ is the number of occurrences of state $\state$. Now we compute the contribution of any update $\Delta_i$. The updates of $\Delta_i$ will  contribute to any update of state $\state$ which occurs at time $\tau_j\leq i$. The total update of $\Delta_i$ would be $\sum_{\tau_j\leq i } (\lambda\discount)^{i-\tau_j}\Delta_i $. Note that $e_i(\state)$ st time $i$ equals to $\sum_{\tau_j\leq i } (\lambda\discount)^{i-\tau_j}$, which implies that the update equals $e_i(\state)\Delta_i$.So the sum of the updates should be equivalent. Figure~\ref{fig-TD-lambda-forward-backward} has an illustrative example.
We will derive it more formally in the proof that follows.

\begin{figure}
    \centering
    %\includegraphics{}
  $$  
  \begin{matrix}
                & \Delta_0 & \Delta_1 & \Delta_2 & \Delta_3 &  \Delta_4 & \Delta_5 & \Delta_6 & \Delta_7 \\
\state_0=\state & 1 & \lambda\discount & (\lambda\discount)^2 & (\lambda\discount)^3& (\lambda\discount)^4 & (\lambda\discount)^5& (\lambda\discount)^6 &(\lambda\discount)^7\\
\state_2=\state &  & & 1 & \lambda\discount & (\lambda\discount)^2 & (\lambda\discount)^3& (\lambda\discount)^4  & (\lambda\discount)^5\\
\state_5=\state &  &  & & & & 1 & \lambda\discount & (\lambda\discount)^2\\
 & e_0(\state) & e_1(\state) & e_2(\state)& e_3(\state) & e_4(\state) & e_5(\state) & e_6(\state) & e_7(\state)
\end{matrix}
$$
    \caption{An example for $TD(\lambda)$ updates of state $\state$ that occurs at times $0$, $2$ and $5$. The forward update appear the rows. Each column is  the coefficients of the update of $\Delta_i$, and their sum equals $e_i(\state)$.} \label{fig-TD-lambda-forward-backward}
\end{figure}


For the forward view we define the updates to be
$\Delta V^F_\ttime(\state)=\alpha(R^\lambda_\ttime-V_\ttime(\state))$,
where $R^\lambda_\ttime=(1-\lambda)\sum_{n=1}^\infty
\lambda^{n-1}R_\ttime^{(n)}$. Equivalently,
$\Delta V^F_\ttime(\state)=\alpha (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}\Delta_\ttime ^{(n)}$.


For the backward view we define the updates to be
$\Delta V^B_\ttime(\state)=\alpha\Delta_\ttime e_\ttime(\state)$,
where the eligibility trace is
$e_\ttime(\state)=\sum_{k=0}^\ttime(\lambda\discount)^{\ttime-k}I(\state=\state_k)$.

\begin{theorem}
For any state $\state$
%For any trajectory of length $T$ we have
\[
\sum_{\ttime=0}^{\infty} \Delta V^B_\ttime
(\state)=\sum_{\ttime=0}^{\infty} \Delta V_\ttime^F(\state)
I(\state_\ttime=\state)
\]
\end{theorem}

\begin{proof}
Consider the sum of the forward updates for state $\state$:
\begin{align}
\sum_{\ttime=0}^{\infty } \Delta V^F_\ttime (\state)&=
\sum_{\ttime=0}^{\infty} \alpha
(1-\lambda) \sum_{n=\ttime}^{\infty} \lambda^{n-\ttime}\Delta_\ttime^{(n)} I(\state=\state_\ttime)\nonumber\\
%
&= \sum_{\ttime=0}^{\infty}  \alpha
(1-\lambda) \sum_{n=\ttime}^{\infty} \lambda^{n-\ttime}\sum_{i=0}^n \discount^i \Delta_{\ttime+i} I(\state=\state_\ttime)\nonumber\\
%
&=\sum_{\ttime=0}^\infty \sum_{n=0}^\infty \sum_{k=\ttime}^n \alpha
(1-\lambda)
\lambda^{n-k}\lambda^{k-\ttime}\discount^{k-\ttime}\Delta_k I(\state=\state_\ttime)\nonumber\\
%
&=\sum_{\ttime=0}^\infty \sum_{k=\ttime}^\infty \alpha
(\discount\lambda)^{k-\ttime}\Delta_k I(\state=\state_\ttime) \sum_{i=0}^\infty (1-\lambda)\lambda^{i}\nonumber\\
%
&= \sum_{\ttime=0}^\infty \sum_{k=\ttime}^\infty \alpha
(\discount\lambda)^{k-\ttime} \Delta_k(\state)
I(\state=\state_\ttime) \label{eq:TD-F}
\end{align}
where the first identity is the definition, the second identity
follows since $\Delta_\ttime^{(n)}=\sum_{i=0}^n\discount^i
\Delta_{\ttime+i}$, in the third identity we substitute $k$ for
$\ttime+i$ and sum over $n$, $k$ and $\ttime$, in the forth identity we
substitute $i$ for $n-k$ and isolate the terms that depend on $i$,
and in the last identity we note that $\sum_{i=0}^\infty
(1-\lambda)\lambda^{i}=1$.

For the backward view for state $\state$ we have
\begin{align}
\sum_{\ttime=0}^{\infty } \Delta V^B_\ttime (\state)&=\sum_{\ttime=0}^{\infty} \alpha
\Delta_\ttime(\state) e_{\ttime}(\state)\nonumber\\
&= \sum_{\ttime=0}^{\infty} \alpha
\Delta_\ttime(\state) \sum_{k=0}^\ttime (\discount\lambda)^{\ttime-k} I(\state=\state_\ttime)\nonumber\\
&= \sum_{k=0}^\infty \sum_{\ttime=k}^\infty \alpha
(\discount\lambda)^{\ttime-k}\Delta_\ttime(\state)I(\state=\state_\ttime)
\label{eq:TD-B}
\end{align}

Note that if we interchange $k$ and $\ttime$ in Eq. (\ref{eq:TD-F})
and in Eq. (\ref{eq:TD-B}), then we have identical expressions.
\end{proof}

\subsection{$SARSA(\lambda)$}

We can use the idea of eligibility traces also in other algorithms,
such as SARSA. Recall that given
$(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1},\action_{\ttime+1})$
the update of SARSA is
\[Q_{\ttime+1}(\state_\ttime,\action_\ttime) = Q_\ttime (\state_\ttime,\action_\ttime) + \alpha_\ttime(\state_\ttime,\action_\ttime) \Gamma_{\ttime} \]
where $\Gamma_{\ttime} = \reward_\ttime +
\discount  Q_\ttime (\state_{\ttime+1}, a_{\ttime+1}) -Q_\ttime
(\state_\ttime,\action_\ttime)$.

\noindent{}Similarly, we can define an $n$-step look-ahead, and set
\[\Gamma_{\ttime}^{(n)} = R_\ttime^{(n)}(\state)-Q_\ttime
(\state_\ttime,\action_\ttime)\]
where $R_\ttime^{(n)}(\state) = \sum_{i=0}^{n-1}\discount^i \reward_{\ttime+i}
+\discount^n Q_\ttime(\state_{\ttime+n},\action_{\ttime+n})$

\noindent{}We can now define $SARSA(\lambda)$ using exponential averaging with
parameter $\lambda$. Namely, we define
\[\Gamma_{\ttime}^{\lambda}=(1-\lambda)\sum_{n=1}^\infty
\lambda^{n-1}\Gamma_{\ttime}^{(n)}\]
This makes the forward view of
$SARSA(\lambda)$ to be
\[Q_{\ttime+1}(\state_\ttime,\action_\ttime) = Q_\ttime (\state_\ttime,\action_\ttime) + \alpha_\ttime(\state_\ttime,\action_\ttime)\Gamma_{\ttime}^{\lambda} \]
Similar to $TD(\lambda)$, we can also define a backward view using
eligibility traces:
\[e_\ttime(\state,\action)&=\discount\lambda
e_{\ttime-1}(\state,\action)+ I(\state=\state_\ttime,
a=\action_\ttime)\]
\paragraph{Backward view algorithm}\ \\

 -- Initialization: Set $\widehat{Q}_{0}(\state)=0$ (or any other value), and
 $e_0(\state)=0$.

 -- Update: observe $(\state_\ttime,\action_\ttime,\reward_\ttime,\state_{\ttime+1})$ and set
\begin{align*}
\Delta_\ttime&= \reward_\ttime +\discount Q_\ttime(\state_{\ttime+1},\action_{\ttime+1})-Q_\ttime(\state_\ttime,\action_\ttime)\\
e_\ttime(\state,\action)&=\discount\lambda
e_{\ttime-1}(\state,\action)+ I(\state=\state_\ttime,
a=\action_\ttime)\\
Q_{\ttime+1}(\state,\action)&=Q_{\ttime}(\state,\action)+\alpha_\ttime
e_\ttime(\state,\action)\Delta_\ttime
\end{align*}

\section{Miscellaneous}

\subsection{Importance Sampling}

Importance sampling is a simple general technique to estimate the
mean with respect to a given distribution, while sampling from a
different distribution. To be specific, let $Q$ be the sampling
distribution and $P$ the evaluation distribution. The basic idea is
the following
\[
E_{x\sim P}[f(x)]=\sum_x P(x)f(x)=\sum_x Q(x)\frac{P(x)}{Q(x)}
f(x)=E_{x\sim Q} [\frac{P(x)}{Q(x)}f(x)]
\]
This implies that given a sample $\{x_1, \ldots , x_m\}$ from $Q$,
we can estimate $E_{x\sim P}[f(x)]$ using $\sum_{i=1}^m
\frac{P(x_i)}{Q(x_i)}f(x_i)$.
%
The importance sampling gives an unbiased estimator, but the
variance of the estimator might be huge, since it depends on
$P(x)/Q(x)$.

We would like to apply the idea of importance sampling to learning
in MDPs. Assume that there is a policy $\policy$ that selects the
actions, and there is a policy $\rho$ that we would like to
evaluate. For the importance sampling, given a trajectory, we need
to take the ratio of probabilities under $\rho$ and $\policy$.
\[
\frac{\rho(\state_1,\action_1,\reward_1,
\ldots,\state_{T},\action_{T},\reward_T,\state_{T+1})}{\policy(\state_1,\action_1,\reward_1,
\ldots,\state_{T},\action_{T},\reward_T,\state_{T+1})}=\prod_{\ttime=1}^T
\frac{\rho(a_\ttime|\state_\ttime)}{\policy(a_\ttime|\state_\ttime)}
\]
where the equality follows since the reward and transition
probabilities are identical, and cancel.

For Monte-Carlo, the estimates would be
\[
G^{\rho/\policy}=\prod_{\ttime=1}^T
\frac{\rho(\action_\ttime|\state_\ttime)}{\policy(\action_\ttime|\state_\ttime)}
(\sum_{\ttime=1}^T \reward_\ttime)
\]
and we have
\[
\widehat{\Value}^\rho (\state_1) = \widehat{\Value}^\rho(\state_1) +\alpha(
G^{\rho/\policy}- \widehat{\Value}^\rho(\state_1))
\]
This updates might be huge, since we are multiplying the ratios of
many small numbers.

For the $TD(0)$ the updates will be
\[
\Delta^{\rho/\policy}_\ttime
=\frac{\rho(\action_\ttime|\state_\ttime)}{\policy(\action_\ttime|\state_\ttime)}\reward_\ttime
+\discount \widehat{\Value}(\state_{\ttime+1})-\widehat{\Value}(\state_\ttime)
\]
and we have
\[
\widehat{\Value}^\rho (\state_1) = \widehat{\Value}^\rho(\state_1) +\alpha(
\Delta^{\rho/\policy}_\ttime- \widehat{\Value}^\rho(\state_1))
\]
This update is much more stable, since we have only one factor
multiplying the observed reward.

\begin{example}
Consider an MDP with a single state and two actions (also called multi-arm bandit, which we will cover in Chapter~\ref{chapter:MAB}). We consider a finite horizon return with parameter $\tHorizon$. Policy $\pi$ at each time selects one of the two actions uniformly at random. The policy $\rho$ selects action one always.

Using the Monte Carlo approach, when considering complete trajectories, only after expected  $ 2^\tHorizon$ trajectories we have a trajectory in which for $\tHorizon$ times action one was selected. (Note that the update will have weight $2^\tHorizon$.)

Using the $TD(0)$ updates, each time action one is selected by $\pi$ we can do an update the estimates of $\rho$ (with a factor of $2$). 
%This implies that in order to have 

To compare the two approaches, consider the number of trajectories required to get an $\epsilon$ approximation for the return of $\rho$. Using Monte-Carlo, we need $O(\tHorizon 2^\tHorizon/\epsilon^2)$ trajectories, in expectation. In contrast, for $TD(0)$ we need only $O(\tHorizon/\epsilon^2)$ trajectories. The huge gap is due to the fact that $TD(0)$ utilizes partial trajectories while Monte-Carlo requires the entire trajectory to agree with $\rho$.
\end{example}

\subsection{Actor-critic methodology}
\label{section:actor-critic}

Actor-critic gives a general methodology of building reinforcement
learning algorithms. It is composed from an actor, that selects the
actions, and a critic, that learns the value function. The actor
observes the current state, and the value function, and selects an
action. The critic, observes the current state (and action) and
reward and outputs a value function. See
Figure~\ref{fig:L7-Actor-Critic}.

The Critic has as input the current state and reward. The goal of the critic is to evaluate current policy. This can be done in many ways, for example:
Monte Carlo, TD, Q-learning, etc.

The Actor has as input the value and state. The goal of the actor is to improve the current policy. This can be done in many ways, for example, $\epsilon$-greedy, soft-max, SARSA, etc.



\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{centering}
  \includegraphics[width=0.5\textwidth]{figures/L7-Actor-Critic}
  \caption{The situated agent}\label{fig:L7-Actor-Critic}
  \end{centering}
\end{figure}





\section{Bibliography Remarks}

The $Q$-learning outline of the asymptotic convergence and the step
size analysis follows \cite{Even-DarM03}.

Expected SARSA was presented in \cite{SeijenHWW09}

The comparison of the {\tt First Visit} and {\tt Every Visit} is
based on \cite{SinghS96}.

Part of the outline borrows from David Silver class notes and the
the book of Sutton and Barto \cite{SuttonB98}.


The presentation of the TD algorithms follows the book of Sutton and
Barto \cite{SuttonB98}.
