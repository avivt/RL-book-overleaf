Up until now, we have discussed \textit{planning} under a known model, such as the MDP. Indeed, the algorithms we discussed made extensive use of the model, such as iterating over all the states, actions, and transitions.
In the remainder of this book, we shall tackle the \textit{learning} setting -- how to make decisions when the model is not known in advance, or too large for iterating over it, precluding the use of the planning methods described earlier. Before diving in, however, we shall spend some time on defining the various approaches to modeling a learning problem. In the next chapters, we will rigorously cover some of these approaches. This chapter, similarly to Chapter \ref{chapter-planning-preface}, is quite different than the rest of the book, as it discusses epistemological issues more than anything else. 

In the machine learning literature, perhaps the most iconic learning problem is \textit{supervised learning}, where we are given a training dataset of $N$ samples, $X_1,X_2,\dots, X_N$, sampled i.i.d.~from some distribution, and corresponding labels $Y_1,\dots,Y_N$, generated by some procedure. We can think of $Y_i$ as the supervisor's answer to the question ``what to do when the input is $X_i$?''. The learning problem, then, is to use this data to find some function $Y = f(X)$, such that when given a new sample $X'$ from the data distribution (not necessarily in the dataset), the output of $f(X')$ will be similar to the corresponding label $Y'$ (which is not known to us). A successful machine learning algorithm therefore exhibits \textit{generalization} to samples outside its training set.

Measuring the success of a supervised learning algorithm in practice is straightforward -- by measuring the average error it makes on a test set sampled from the data distribution. The Probably Approximately Correct (PAC) framework is a common framework for providing theoretical guarantees for a learning algorithm. For a given algorithm, a PAC sample complexity result, also known as an ``upper bound", would compute the expected error ($\epsilon$) that can be guaranteed with a given error probability ($\delta$) for a given sample size ($N$). A PAC lower bound is an impossibility result arguing that no algorithm can learn with an expected error less than $\epsilon$ with success probability more than $1-\delta$ with fewer than $N$ samples.
PAC results are therefore important for understanding how efficient a learning algorithm is (e.g., how the expected error and failure probability reduce with $N$).

In reinforcement learning, we are interested in learning how to solve sequential decision problems. We shall now discuss the main learning model, why it is useful, how to measure success and provide guarantees, and also briefly mention some alternative learning models that are outside the scope of this book.

\section{Interacting Online with an Unknown MDP}

The common RL model is inspired by models of behavioral psychology, where an agent (e.g., a rat) needs to learn some desired behavior (e.g., navigate a maze), by reinforcing the desired behavior with some reward (e.g., giving the rat food upon exiting the maze). The key distinction with supervised learning, is that the agent is not given direct supervision about its actions (i.e., how to navigate the maze), but must understand what actions are good only from the reward signal.

To a great extent, much of the RL literature implements this model as interacting with an MDP whose parameters are unknown. As depicted in Figure \ref{fig:RL_model_fig}, at each time step $\ttime = 1,2,\dots, N$, the agent can observe the current state $\state_\ttime$, take an action $\action_\ttime$, and subsequently obtain an observation from the environment (MDP) about the current reward $\reward(\state_\ttime,\action_\ttime)$ and next state $\state_{\ttime+1}\sim \transitionprob(\cdot | \state_\ttime, \action_\ttime)$.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=4cm, auto, >=Stealth]

    % Nodes
    \node [rectangle, draw, rounded corners, minimum width=2cm, minimum height=1cm] (agent) {Agent};
    \node [rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=1cm, right=of agent] (environment) {Environment};

    % Arrows
    % \draw [->] (agent.east) -- node [midway, above] {$s_t$} (environment.west);
    \draw [->] (agent.east) to[bend left=20] node [midway, below] {$\action_\ttime | \state_\ttime$} (environment.west);
    \draw [->] (environment.south) -- ++(0,-1.5) -- node [midway, below] {$\reward(\state_\ttime,\action_\ttime), \state_{\ttime+1}\sim \transitionprob(\cdot | \state_\ttime, \action_\ttime)$} ++(-4,0) -- (agent.south);
\end{tikzpicture}
    \caption{Interaction of an agent with the environment}
    \label{fig:RL_model_fig}
\end{figure}

We can think of the $N$ training samples in RL as tuples $(\state_\ttime,\action_\ttime,\reward(\state_\ttime,\action_\ttime),\state_{\ttime+1})_{\ttime=0}^{N}$, and the goal of the learning agent is to eventually (for $N$ large enough) perform well in the environment, that is, learn a policy for the MDP that is near optimal. Note that in this learning model, the agent cannot make any explicit use of the MDP model (rewards and transitions), but only obtain samples of them in the states it visited.

The reader may wonder -- why is such a learning model useful at all? After all, it's quite hard to imagine real world problems as in Figure \ref{fig:RL_model_fig}, where the agent starts out without \textit{any} knowledge about the world and must learning everything only from the reinforcement signal. As it turns out, RL algorithms essentially learn to solve MDPs without requiring an explicit MDP model, and can therefore be applied even to very large MDPs, for which the planning methods in the previous chapters do not apply. The important insight is that if we have an RL algorithm, and a \textit{simulator} of the MDP, capable of generating $\reward(\state_\ttime,\action_\ttime)$ and $\state_{\ttime+1}\sim \transitionprob(\cdot | \state_\ttime, \action_\ttime)$, then we can run the RL algorithm with the simulator replacing the real environment. To date, almost all RL successes in game playing, control, and decision making have been obtained under this setting.

Another motivation for this learning model comes from the field of adaptive control~\cite{astrom2008adaptive}. If the agent has an imperfect model of the MDP (what we called epistemic uncertainty in Chapter \ref{chapter-planning-preface}), any policy it computes using it may be suboptimal. To overcome this error, the agent can try and correct its model of the MDP or adapt its policy during interaction with the real environment. Indeed, RL is very much related to adaptive optimal control~\cite{sutton1992reinforcement}, which studies a similar problem.

% When faced with such a situation, the question is what should the decision maker (or agent) do? 

%What are we trying to achieve: Regret VS PAC guarantees.
% One approach is to try and learn the parameters of the MDP. That is, the decision maker may want to estimate from data the parameters of the unknown MDP, similarly to proper learning in standard machine learning. While learning exact parameters may require infinite time a more relaxed goal is 


In contrast with the supervised learning model, where measuring success was straightforward, we shall see that defining a goodness measure for an RL agent is more involved, and we shall discuss some prominent ideas in the literature.

\paragraph{Regret, PAC, and asymptotic guarantees.} Consider that we evaluate the agent based the cumulative reward it can obtain in the MDP. Naturally, we should expect that with enough interactions with the environment, any reasonable RL algorithm should converge to obtaining as much reward as an optimal policy would. That is, as the number of training samples $N$ goes to infinity, the value of the agent's policy should converge to the optimal value function $\Value^*$. Such an asymptotic result will guarantee that the algorithm is \textit{fundamentally sound}, and does not make any systematic errors.

To compare the learning efficiency of different RL algorithms, it is more informative to look at finite-sample guarantees. A direct extension of the PAC framework to the RL setting could be: bound the sub-optimaly $\epsilon$ of the value of the learned policy with respect to an optimal policy, after taking $N$ samples from the environment, with probability $1 - \delta$ (the probability is with respect to the stochasticity of the MDP transitions). A corresponding practical evaluation is to first train the agent for $N$ time steps, and then evaluate the learned policy. 

The problem with the PAC approach is that we only care about the reward collected \textit{after} learning, but not the reward obtained \textit{during} learning. For some problems, such as online marketing or finance, we may want to maximize revenue \textit{all throughout} learning. A useful measure for this is the \textit{regret}, 
\begin{equation*}
    Regret(N) = \sum_{\ttime = 0}^{N} \reward^*_\ttime - \sum_{\ttime = 0}^{N} \reward(\state_\ttime,\action_\ttime),
\end{equation*}
which measures the difference between the cumulative reward the agent obtained on the $N$ samples and the sum of rewards that an optimal policy would have obtained (with the same amount of time steps $N$), denoted here as $\reward^*_\ttime$. Any algorithm that converges to an optimal policy would have $\frac{1}{N} Regret(N) \to 0$, but we can also compare algorithms by the \textit{rate} that the average regret decreases.

Interestingly, for an algorithm to be optimal in terms of regret, it must balance between \textit{exploration} -- taking actions that yield information about the MDP, and \textit{exploitation} -- taking actions that simply yield high reward. This is different from PAC, where the agent should in principle devote all the $N$ samples for exploration. 
% \AT{do we want to say something deeper about PAC and regret here? Does regret $\to$ PAC? }


% \AT{Are there other important performance measures that we want to mention (even though we do not cover them)?}

\subsection{Alternative Learning Models}

Humans are perhaps the best example we have for agents learning general, well performing decision making. Even though the common RL model was inspired from behavioral psychology, its specific mathematical formulation is much more limited than the general decision making we may imagine as humans. In the following, we discuss some limitations of the RL model, and alternative decision making formulations that address them. These models are outside the scope of this book.

\paragraph{The challenges of learning from rewards (revisited)}
We have already discussed the difficulty of specifying decision making problems using reward functions in the preface to the planning, Chapter \ref{chapter-planning-preface}. In the RL model, we assume that we can evaluate the observed interaction of the agent with environment by scalar rewards. This is easy if we have an MDP model or simulator, but often difficult otherwise. For example, if we want to use RL to automatically train a robot to perform some task (e.g., fold a piece of cloth), we need to write a reward function that can evaluate whether the cloth was folded or not -- a difficult task in itself. We can also directly query a human expert for evaluating the agent. However, it turns out that humans find it easier to rank different interactions than to associate their performance with a scalar reward. The field of RL from Human Feedback (RLHF) studies such evaluation models, and has been instrumental for tuning chatbots using RL~\cite{ouyang2022training}. It is also important to emphasize that in the RL model defined above, the agent \textit{is only concerned with maximizing reward}, leading to behavior that can be very different from human decision making. As argued by Lake et al.~\cite{lake2017building} in the context of video games, humans can easily imagine how to play the game differently, e.g., how to lose the game as quickly as possible, or how to achieve certain goals, but such behaviors are outside the desiderata of the standard RL problem; extensions of the RL problem include more general reward evaluations such as `obtain a reward higher than $x$'~\cite{srivastava2019training,chen2021decision}, or goal-based formulations~\cite{kaelbling1993learning}, and a key question is how to train agents that \textit{generalize} to new goals.

\paragraph{Bayesian vs Frequentist}

The RL model described above is \textit{frequentist}\footnote{The Bayesian and Frequentist approaches are two fundamental schools of thought in statistics that differ in how they interpret probability and approach inference. In frequentist inference, parameters are considered fixed but unknown quantities, and inference is made by examining how an estimator would perform in repeated sampling. Bayesian inference treats parameters as sampled from a prior distribution, and calculates the posterior parameter probability after observing data, using Bayes rule.} in nature -- the agent interacts with a fixed, but unknown, MDP. An alternative paradigm is \textit{Bayesian RL}~\cite{ghavamzadeh2015bayesian}, where we assume some prior distribution over possible MDPs that the agent can interact with, and update the agent's belief about the ``real'' (but unknown) MDP using the data samples. The Bayesian prior is a convenient method to specify prior knowledge that the agent may have before learning, and the Bayesian formulation offers a principled solution to the exploration-exploitation tradeoff -- the agent can calculate in advance how much information any action would yield (i.e., how it would affect the belief). 

\paragraph{Offline reinforcement learning}
A common situation that is often encountered in practice is that of offline RL. In its purest form, in offline learning, the agent is given recorded trajectories from some historical policy and needs to learn an optimal policy based on them. When the historical data is created by a strong agent, perhaps even optimal, one often tries to use {\em behavioral cloning} to learn a policy that will be as similar as possible to the one that created the data.  However, the data itself may be sampled from a suboptimal policy and in which case, since the agent cannot explore further, the variance of the learned policy may be large even when the offline data is huge. We note that the pure offline and pure online RL settings are two extremes. Often, one has both: plenty of offline data and the opportunity to interact with an environment in an online manner. 


\paragraph{Generalization to changes in the MDP}
A stark difference between RL and supervised learning is what we mean by generalization. While in supervised learning we evaluate the agent's decision making on test problems unseen during training, in the RL problem described above the agent is trained and tested \textit{on the same MDP}. At test time, the agent may encounter states that it has not visited during training and in this sense must generalize, but the main focus of the learning problem is how to take actions in the MDP that eventually lead to learning a good policy.

Several alternative learning paradigms explored generalization in sequential decision making. In Meta RL~\cite{beck2023survey}, the agent can interact with several \textit{training MDPs} during learning, but is then tested on a similar, yet unseen, \textit{test MDP}. If the training and test MDPs are sampled from some distribution, meta RL relates to Bayesian RL, where the prior is the training MDP distribution, and PAC-style guarantees can be provided on how many training MDPs are required to obtain near Bayes-optimal performance~\cite{tamar2022regularization}. A related paradigm is contextual MDPs, where repeated interactions with several MDPs are considered at test time, and regret bounds can capture the tradeoff between identifying the MDPs and maximizing rewards~\cite{hallak2015contextual}.
More generally, \textit{transfer learning} in RL concerns how to transfer knowledge between different decision making problems~\cite{taylor2009transfer,kirk2023survey}.
It is also possible to search for policies that work well across many different MDPs, and are therefore robust enough to generalize to changes in the MDP. One approach, commonly termed \textit{domain randomization}, trains a single policy on an ensemble of different MDPs~\cite{tobin2017domain}. Another approach optimizes a policy for the worst case MDP in some set, based on the robust MDP formulation~\cite{nilim2005robust}. Yet another learning setting is lifelong RL, where an agent interacts with an MDP that gradually changes over time~\cite{khetarpal2022towards}.


\subsection{What to Learn in RL?}
In the next chapters we shall explore several approaches to the RL problem. Relating to the underlying MDP model, we shall apply a learning-based approach to different MDP-related quantities. 

A straightforward approach is \textit{model-based}. In this approach, the agent learns the rewards and transitions of the MDP, and uses them to compute a policy using planning algorithms. A key question here is how to take actions that would guarantee that the agent sufficiently explores all states of the MDP.

An alternative approach is \textit{model-free}. Interestingly, the agent can learn optimal behavior without explicitly estimating the MDP parameters. This can be done by directly estimating either the value function, or the optimal policy. In particular, this approach will allow us to use function approximation to generalize the learned value or policy to states that the agent has not seen during training, potentially allowing us to handle MDPs with large state spaces.

