Up until now, we have discussed \textit{planning} under a known model, such as the MDP. Indeed, the algorithms we discussed made extensive use of the model, such as iterating over all the states, actions, and transitions.
In the remainder of this book, we shall tackle the \textit{learning} setting -- how to make decisions when the model is not known in advance, or too large for iterating over it, precluding the use of the planning methods described earlier. Before diving in, however, we shall spend some time on defining the various approaches to modeling a learning problem. In the next chapters, we will rigorously cover some of these approaches. This chapter, similarly to Chapter \ref{chapter-planning-preface}, is quite different than the rest of the book, as it discusses epistemological issues more than anything else. 

In the machine learning literature, perhaps the most iconic learning problem is \textit{supervised learning}, where we are given a training dataset of $N$ samples, $X_1,X_2,\dots, X_N$, sampled i.i.d.~from some distribution, and corresponding labels $Y_1,\dots,Y_N$, generated by some procedure. We can think of $Y_i$ as the supervisor's answer to the question ``what to do when the input is $X_i$?''. The learning problem, then, is to use this data to find some function $Y = f(X)$, such that when given a new sample $X'$ from the data distribution (not necessarily in the dataset), the output of $f(X')$ will be similar to the corresponding label $Y'$ (which is not known to us). A successful machine learning algorithm therefore exhibits \textit{generalization} to samples outside its training set.

Measuring the success of a supervised learning algorithm in practice is straightforward -- by measuring the average error it makes on a test set sampled from the data distribution. The Probably Approximately Correct (PAC) framework is a common framework for providing theoretical guarantees for a learning algorithm. A standard PAC result gives a bound $\epsilon$ on the average error for a randomly sampled test data, given a randomly sampled training set of size $N$, that holds with probability $1 - \delta$. PAC results are therefore important to understand how efficient a learning algorithm is (e.g., how the error reduces with $N$).

In reinforcement learning, we are interested in learning how to solve sequential decision problems. We shall now discuss the main learning model, why it is useful, how to measure success and provide guarantees, and also briefly mention some alternative learning models that are outside the scope of this book.

\section{Interacting with an Unknown MDP}

The common reinforcement learning model is inspired by models of behavioral psychology, where an agent (e.g., a rat) needs to learn some desired behavior (e.g., navigate a maze), by reinforcing the desired behavior with some reward (e.g., giving the rat food upon exiting the maze). The key distinction with supervised learning, is that the agent is not given direct supervision about its actions (i.e., how to navigate the maze), but must understand what actions are good only from the reward signal.

To a great extent, much of the RL literature implements this model as interacting with an MDP whose parameters are unknown. As depicted in Figure \ref{fig:RL_model_fig}, at each time step $\ttime = 1,2,\dots, N$, the agent can observe the current state $\state_\ttime$, take an action $\action_\ttime$, and subsequently obtain an observation from the environment (MDP) about the current reward $\reward(\state_\ttime,\action_\ttime)$ and next state $\state_{\ttime+1}\sim \transitionprob(\cdot | \state_\ttime, \action_\ttime)$.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=4cm, auto, >=Stealth]

    % Nodes
    \node [rectangle, draw, rounded corners, minimum width=2cm, minimum height=1cm] (agent) {Agent};
    \node [rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=1cm, right=of agent] (environment) {Environment};

    % Arrows
    % \draw [->] (agent.east) -- node [midway, above] {$s_t$} (environment.west);
    \draw [->] (agent.east) to[bend left=20] node [midway, below] {$\action_\ttime | \state_\ttime$} (environment.west);
    \draw [->] (environment.south) -- ++(0,-1.5) -- node [midway, below] {$\reward(\state_\ttime,\action_\ttime), \state_{\ttime+1}\sim \transitionprob(\cdot | \state_\ttime, \action_\ttime)$} ++(-4,0) -- (agent.south);
\end{tikzpicture}
    \caption{Interaction of an agent with the environment}
    \label{fig:RL_model_fig}
\end{figure}

We can think of the $N$ training samples in RL as tuples $(\state_\ttime,\action_\ttime,\reward(\state_\ttime,\action_\ttime),\state_{\ttime+1})_{\ttime=0}^{N}$, and the goal of the learning agent is to eventually ($N$ large enough) perform well in the environment, that is, learn a policy for the MDP that is near optimal. Note that in this learning model, the agent cannot make any explicit use of the MDP model (rewards and transitions), but only obtain samples of them in the states it visited.

The reader may wonder -- why is such a learning model useful at all? After all, it's quite hard to imagine real world problems as in Figure \ref{fig:RL_model_fig}, where the agent starts out without \textit{any} knowledge about the world and must learning everything only from the reinforcement signal. As it turns out, RL algorithms essentially learn to solve MDPs without requiring an explicit MDP model, and can therefore be applied even to very large MDPs, for which the planning methods in the previous chapters do not apply. The important insight is that if we have an RL algorithm, and a \textit{simulator} of the MDP, capable of generating $\reward(\state_\ttime,\action_\ttime)$ and $\state_{\ttime+1}\sim \transitionprob(\cdot | \state_\ttime, \action_\ttime)$, then we can run the RL algorithm with the simulator replacing the real environment. To date, almost all RL successes in game playing, control, and decision making have been obtained under this setting.

Another motivation for this learning model comes from the field of adaptive control~\cite{astrom2008adaptive}. If the agent has an imperfect model of the MDP (what we called epistemic uncertainty in Chapter \ref{chapter-planning-preface}), any policy it computes using it may be suboptimal. To overcome this error, the agent can try and correct its model of the MDP or adapt its policy during interaction with the real environment. Indeed, RL is very much related to adaptive optimal control~\cite{sutton1992reinforcement}, which studies a similar problem.

% When faced with such a situation, the question is what should the decision maker (or agent) do? 

%What are we trying to achieve: Regret VS PAC guarantees.
% One approach is to try and learn the parameters of the MDP. That is, the decision maker may want to estimate from data the parameters of the unknown MDP, similarly to proper learning in standard machine learning. While learning exact parameters may require infinite time a more relaxed goal is 


In contrast with the supervised learning model, where measuring success was straightforward, we shall see that defining a good RL agent is more involved, and we shall discuss some dominant ideas in the literature.

\paragraph{Regret VS PAC VS asymptotic guarantees.} Consider that we evaluate the agent based the cumulative reward it can obtain in the MDP. Naturally, we should expect that with enough interactions with the environment, any reasonable RL algorithm should converge to obtaining as much reward as an optimal policy would. That is, as the number of training samples $N$ goes to infinity, the value of the agent's policy should converge to the optimal value function $\Value^*$. Such an asymptotic result will guarantee that the algorithm is \textit{fundamentally sound}, and does not make any systematic errors.

To compare the learning efficiency of different RL algorithms, it is more informative to look at finite-sample guarantees. A direct extension of the PAC framework to the RL setting could be: bound the sub-optimaly $\epsilon$ of the value of the learned policy with respect to an optimal policy, after taking $N$ samples from the environment, with probability $1 - \delta$ (the probability is with respect to the stochasticity of the MDP transitions). A corresponding practical evaluation is to first train the agent for $N$ time steps, and then evaluate the learned policy. 

The problem with the PAC approach is that we only care about the reward collected \textit{after} learning, but not the reward obtained \textit{during} learning. For some problems, such as online marketing or finance, we may want to maximize revenue \textit{all throughout} learning. A useful measure for this is the \textit{regret}, 
\begin{equation*}
    Regret(N) = \sum_{\ttime = 0}^{N} \reward^*_\ttime - \sum_{\ttime = 0}^{N} \reward(\state_\ttime,\action_\ttime),
\end{equation*}
which measures the difference between the cumulative reward the agent obtained on the $N$ samples and the sum of rewards that an optimal policy would have obtained (with the same amount of time steps $N$), denoted here as $\reward^*_\ttime$. Any algorithm that converges to an optimal policy would have $\frac{1}{N} Regret(N) \to 0$, but we can also compare algorithms by the \textit{rate} that the average regret decreases.

Interestingly, for an algorithm to be optimal in terms of regret, it must balance between \textit{exploration} -- taking actions that yield information about the MDP, and \textit{exploitation} -- taking actions that simply yield high reward. This is different from PAC, where the agent should in principle devote all the $N$ samples for exploration. \AT{do we want to say something deeper about PAC and regret here? Does regret $\to$ PAC? }


\AT{Are there other important performance measures that we want to mention (even though we do not cover them)?}

\subsection{Alternative Learning Models}

Humans are perhaps the best example we have for agents learning general, well performing decision making. Even though the common RL model was inspired from behavioral psychology, its specific mathematical formulation is much more limited than the general decision making we may imagine as humans. In the following, we discuss some limitations of the RL model, and alternative decision making formulations that address them. These models are outside the scope of this book.

\paragraph{The challenges of learning from rewards (revisited)}
We have already discussed the difficulty of specifying decision making problems using a reward in the preface to the planning, Chapter \ref{chapter-planning-preface}. In the RL model, we assume that we can evaluate the observed interaction of the agent with environment by scalar rewards. This is easy if we have an MDP model or simulator, but often difficult otherwise. For example, if we want to use RL to automatically train a robot to perform some task (e.g., fold a piece of cloth), we need to write a reward function that can evaluate whether the cloth was folded or not -- a difficult task in itself. We can also directly query a human expert for evaluating the agent. However, it turns out that humans find it easier to rank different interactions than to associate their performance with a scalar reward. The field of RL from Human Feedback (RLHF) studies such evaluation models, and has been instrumental for tuning chatbots using RL~\cite{ouyang2022training}. It is also important to emphasize that in the RL model defined above, the agent \textit{is only concerned with maximizing reward}, leading to behavior that can be very different from human decision making. As argued by Lake et al.~\cite{lake2017building} in the context of video games, humans can easily imagine how to play the game differently, e.g., how to lose the game as quickly as possible, or how to achieve certain goals, but such behaviors are outside the desiderata of the standard RL problem; extensions of the RL problem include more general reward evaluations such as `obtain a reward higher than $x$'~\cite{srivastava2019training,chen2021decision}, or goal-based formulations~\cite{kaelbling1993learning}, and a key question is how to train agents that \textit{generalize} to new goals.

\paragraph{Bayesian vs Frequentist}

The RL model described above is \textit{frequentist}\footnote{The Bayesian and Frequentist approaches are two fundamental schools of thought in statistics that differ in how they interpret probability and approach inference. In frequentist inference, parameters are considered fixed but unknown quantities, and inference is made by examining how an estimator would perform in repeated sampling. Bayesian inference treats parameters as sampled from a prior distribution, and calculates the posterior parameter probability after observing data, using Bayes rule.} in nature -- the agent interacts with a fixed, but unknown, MDP. An alternative paradigm is \textit{Bayesian RL}~\cite{ghavamzadeh2015bayesian}, where we assume some prior distribution over possible MDPs that the agent can interact with, and update the agent's belief about the ``real'' (but unknown) MDP using the data samples. The Bayesian prior is a convenient method to specify prior knowledge that the agent may have before learning, and the Bayesian formulation offers a principled solution to the exploration-exploitation tradeoff -- the agent can calculate in advance how much information any action would yield (i.e., how it would affect the belief). 

\paragraph{Generalization to changes in the MDP}
A stark difference between RL and supervised learning is what we mean by generalization. While in supervised learning we evaluate the agent's decision making on test problems unseen during training, in the RL problem described above the agent is trained and tested \textit{on the same MDP}. At test time, the agent may encounter states that it has not visited during training and in this sense must generalize, but the main focus of the learning problem is how to take actions in the MDP that eventually lead to learning a good policy.

Several alternative learning paradigms explored generalization in sequential decision making. In Meta RL~\cite{beck2023survey}, the agent can interact with several \textit{training MDPs} during learning, but is then tested on a similar, yet unseen, \textit{test MDP}. If the training and test MDPs are sampled from some distribution, meta RL relates to Bayesian RL, where the prior is the training MDP distribution, and PAC-style guarantees can be provided on how many training MDPs are required to obtain near Bayes-optimal performance~\cite{tamar2022regularization}. A related paradigm is contextual MDPs, where repeated interactions with several MDPs are considered at test time, and regret bounds can capture the tradeoff between identifying the MDPs and maximizing rewards~\cite{hallak2015contextual}.
More generally, \textit{transfer learning} in RL concerns how to transfer knowledge between different decision making problems~\cite{taylor2009transfer,kirk2023survey}.
It is also possible to search for policies that work well across many different MDPs, and are therefore robust enough to generalize to changes in the MDP. One approach, commonly termed \textit{domain randomization}, trains a single policy on an ensemble of different MDPs~\cite{tobin2017domain}. Another approach optimizes a policy for the worst case MDP in some set, based on the robust MDP formulation~\cite{nilim2005robust}. Yet another learning setting is lifelong RL, where an agent interacts with an MDP that gradually changes over time~\cite{khetarpal2022towards}.


\subsection{What to Learn in RL?}
In the next chapters we shall explore several approaches to the RL problem. Relating to the underlying MDP model, we shall apply a learning-based approach to different MDP-related quantities. 

A straightforward approach is \textit{model-based} -- learn the rewards and transitions of the MDP, and use them to compute a policy using planning algorithms. A key question here is how to take actions that would guarantee that the agent sufficiently explores all states of the MDP.

An alternative approach is \textit{model-free}. Interestingly, the agent can learn optimal behavior without ever explicitly estimating the MDP parameters. This can be done by directly estimating either the value function, or the optimal policy. In particular, this approach will allow us to use function approximation to generalize the learned value or policy to states that the agent has not seen during training, potentially allowing us to handle MDPs with large state spaces.


\begin{enumerate}
    \item 
What are we trying to achieve: Regret VS PAC guarantees.\YM{will you give the definitions, otherwise it is tricky.}
\item A Parameter Identification View:
Bayesian VS Frequentist \YM{Note that we never define Bayesian, so this and other Bayesian approaches will be tricky to address, definitely in detail.}
\item A Model Free RL
\item Asymptotics and Finite Time Results
\end{enumerate}


\section{Foundations of Knowledge Acquisition in RL}
\YM{I would add the lst two to the unknown MDP model}
\begin{itemize}
    \item \sout{Experience-driven learning: Knowledge gained through interaction}
    \item \sout{Inductive reasoning: Generalizing from specific experiences}
    \item Delayed feedback: Temporal credit assignment problem
    \item Exploration vs. exploitation: Balancing known and unknown
\end{itemize}

\section{Representation of Knowledge}
\YM{Not sure where state representation goes. Note that up to here they have only tabular setting, so no representation.}

\begin{itemize}
    \item State representation: Abstracting relevant features from environment
    \item Policy-based knowledge: Mapping states to actions
    \item Value-based knowledge: Estimating long-term rewards
    \item Model-based knowledge: Explicit representation of environment dynamics
\end{itemize}

\section{Reasoning Under Uncertainty}

\YM{The last three are not address in any way in the book. This is like "future". The first should fit with unknown MDP and delayed feedback}
\begin{itemize}
    \item Aleatoric uncertainty: Handling inherent randomness
    \item Epistemic uncertainty: Dealing with lack of knowledge
    \item Model misspecification: Addressing incorrect assumptions
    \item Partial observability: Reasoning with incomplete information
\end{itemize}

\section{Generalization and Transfer}
\YM{This is really not address in the book, future research?
Are we sending the reader to find other books?!}

\begin{itemize}
    \item Abstraction: Forming general concepts from specific experiences
    \item Transfer learning: Applying knowledge across different tasks
    \item Meta-learning: Learning to learn, adapting learning processes
    \item Domain randomization: Seeking invariant knowledge across variations
\end{itemize}

\section{Robustness and Adaptability}
\YM{Again, it is out of scope for the book.} 

\begin{itemize}
    \item Distributional shift: Handling differences between training and deployment
    \item Adversarial robustness: Maintaining performance under perturbations
    \item Continual learning: Adapting to changing environments over time
    \item Safe exploration: Balancing knowledge acquisition with constraints
\end{itemize}

\section{Causal Reasoning}
\YM{Again, it is out of scope for the book.} 

\begin{itemize}
    \item Correlation vs. causation: Distinguishing predictive from causal relationships
    \item Counterfactual reasoning: Imagining alternative scenarios
    \item Interventions: Understanding the effects of actions on the environment
\end{itemize}

\section{Interpretability and Explainability}
\YM{Again, it is out of scope for the book.} 

\begin{itemize}
    \item Transparency: Understanding the basis of agent decisions
    \item Abstraction hierarchies: Representing knowledge at multiple levels
    \item Symbolic vs. subsymbolic representations: Balancing interpretability and performance
\end{itemize}

\section{Epistemological Limitations and Challenges}
\YM{I think we should have a discussion of rewards, and who sets them. This is not related to learning, even in planning it is the issue. The reality gap is more like a warning, we do not address it is any way. The first two are vague enough so that I am not sure about them. }

\begin{itemize}
    \item Limits of induction: Fundamental constraints on generalization
    \item Exploration in large state spaces: Scalability of knowledge acquisition
    \item Reality gap: Transferring knowledge from simulations to real-world
    \item Reward specification: Challenges in defining and pursuing goals
\end{itemize}

\section{Multi-Agent and Social Epistemology}
\YM{We never define multi-agent}
\begin{itemize}
    \item Emergent behaviors: Complex strategies arising from simple rules
    \item Theory of mind: Reasoning about other agents' knowledge and intentions
    \item Cooperative vs. competitive learning: Knowledge acquisition in multi-agent settings
\end{itemize}

\section{Ethical and Philosophical Implications}
\YM{We never address those issue}
\begin{itemize}
    \item Value alignment: Ensuring AI systems acquire knowledge aligned with human values
    \item Epistemological pluralism: Recognizing multiple valid approaches to knowledge
    \item AI consciousness: Philosophical questions about machine knowledge and awareness
\end{itemize}

\section{Objective Optimization}
\YM{The first three are important, but unclear why learning and not also planning. The last two are future book.}
\begin{itemize}
    \item Reward design: Epistemological challenges in defining "right" objectives
    \item Proxy rewards: Implications of optimizing indirect measures
    \item Long-term vs. short-term objectives: Balancing immediate and future knowledge, the dubious role of the discount factor
    \item Multi-objective reinforcement learning: Handling competing epistemological goals
    \item Inverse reinforcement learning: Inferring objectives from observed behavior
\end{itemize}

\section{Bayesian vs. Non-Bayesian Approaches}
\YM{We never define a Bayesian approach. We do not present Thompson sampling. This would be more confusing then helpful}
\begin{itemize}
    \item Bayesian RL: Probabilistic reasoning about knowledge and uncertainty
    \item Posterior updating (Thompson Sampling): Formal framework for belief revision in light of evidence
    \item Non-Bayesian approaches: Alternative frameworks for handling uncertainty
    \item Model-free vs. model-based from a Bayesian perspective
    \item Computational tractability: Balancing theoretical ideals with practical constraints
\end{itemize}

\section{Importance of Small (Finite) Models}
\YM{Very important!}
\begin{itemize}
    \item Occam's Razor in RL: Balancing model complexity with explanatory power
    \item Finite MDPs: Epistemological implications of working with limited state spaces
    \item Sample efficiency: Learning effectively from limited experiences
    \item Interpretability of small models: Enhanced understanding and explainability
    \item Generalization in finite models: Extracting broad knowledge from limited representations
    \item Computational benefits: Tractability and scalability considerations
    \item Latent models
\end{itemize}


\chapter{Alternative}


\section{Interacting with an Unknown MDP}

\begin{enumerate}
    \item 
What are we trying to achieve: Regret VS PAC guarantees.
\item A Parameter Identification View:
Bayesian VS Frequentist 
\item A Model Free RL
\item Asymptotics and Finite Time Results
\end{enumerate}
More topics:

\begin{itemize}
    \item Exploration vs. exploitation: Balancing known and unknown
        \item Aleatoric uncertainty: Handling inherent randomness

\end{itemize}



\section{Large state space MDPs}

\begin{itemize}
    \item State representation: Abstracting relevant features from environment
    \item Limits of induction: Fundamental constraints on generalization
    \item Exploration in large state spaces: Scalability of knowledge acquisition
\end{itemize}


\section{What are rewards}
\begin{itemize}
    \item Reward specification: Challenges in defining and pursuing goals
    \item Reward design: Epistemological challenges in defining "right" objectives
    \item Proxy rewards: Implications of optimizing indirect measures
    \item Long-term vs. short-term objectives: Balancing immediate and future knowledge, the dubious role of the discount factor
\end{itemize}


\section{Goal of leaning}

\begin{itemize}
    \item Policy-based knowledge: Mapping states to actions
    \item Value-based knowledge: Estimating long-term rewards
    \item Model-based knowledge: Explicit representation of environment dynamics
\end{itemize}


\section{Importance of Small (Finite) Models}
\begin{itemize}
    \item Occam's Razor in RL: Balancing model complexity with explanatory power
    \item Finite MDPs: Epistemological implications of working with limited state spaces
    \item Sample efficiency: Learning effectively from limited experiences
    \item Interpretability of small models: Enhanced understanding and explainability
    \item Generalization in finite models: Extracting broad knowledge from limited representations
    \item Computational benefits: Tractability and scalability considerations
    \item Latent models
\end{itemize}


